[
  {
    "objectID": "weekly-notes/week-05-notes.html",
    "href": "weekly-notes/week-05-notes.html",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "Setting up repositories on"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-05-notes.html#key-concepts-learned",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "Setting up repositories on"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#coding-techniques",
    "href": "weekly-notes/week-05-notes.html#coding-techniques",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nBasics of tidyverse and its accompanying commands of filter(), select(), mutate(), and summarize()\nQuarto functions on how to bold, italics, both bold and italics, code list, and strikethrough"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#questions-challenges",
    "href": "weekly-notes/week-05-notes.html#questions-challenges",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\n\n\nEverything was clear. I would still like to mess around more with Quarto."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#connections-to-policy",
    "href": "weekly-notes/week-05-notes.html#connections-to-policy",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\n\nThis was a building block week, so not much of direct application rather tracking and documentation baseline for setup."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#reflection",
    "href": "weekly-notes/week-05-notes.html#reflection",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Reflection",
    "text": "Reflection\n\nLearning how to create a custom repository was both enjoyable and insightful.\nIt could also serve as a way to share supplementary analyses and to present code in a more public, graphical, and accessible format for non-coding audiences."
  },
  {
    "objectID": "weekly-notes/week-03-notes.html",
    "href": "weekly-notes/week-03-notes.html",
    "title": "Week 3 Notes - Data Visualization & Exploratory Analysis",
    "section": "",
    "text": "Setting up repositories on"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-03-notes.html#key-concepts-learned",
    "title": "Week 3 Notes - Data Visualization & Exploratory Analysis",
    "section": "",
    "text": "Setting up repositories on"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#coding-techniques",
    "href": "weekly-notes/week-03-notes.html#coding-techniques",
    "title": "Week 3 Notes - Data Visualization & Exploratory Analysis",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nBasics of tidyverse and its accompanying commands of filter(), select(), mutate(), and summarize()\nQuarto functions on how to bold, italics, both bold and italics, code list, and strikethrough"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#questions-challenges",
    "href": "weekly-notes/week-03-notes.html#questions-challenges",
    "title": "Week 3 Notes - Data Visualization & Exploratory Analysis",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\n\n\nEverything was clear. I would still like to mess around more with Quarto."
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#connections-to-policy",
    "href": "weekly-notes/week-03-notes.html#connections-to-policy",
    "title": "Week 3 Notes - Data Visualization & Exploratory Analysis",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\n\nThis was a building block week, so not much of direct application rather tracking and documentation baseline for setup."
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#reflection",
    "href": "weekly-notes/week-03-notes.html#reflection",
    "title": "Week 3 Notes - Data Visualization & Exploratory Analysis",
    "section": "Reflection",
    "text": "Reflection\n\nLearning how to create a custom repository was both enjoyable and insightful.\nIt could also serve as a way to share supplementary analyses and to present code in a more public, graphical, and accessible format for non-coding audiences."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html",
    "href": "weekly-notes/week-01-notes.html",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "Public Sector Data scinece has several considerations different from private sector\n\nPublic good, and equity consideration\nNeeds to be interpretable and not have algorithmic bias\n\nGit is for version control system (VCS)\nGithub is the cloud computing backend of Git\nGithub Lingo\n\nFolder -&gt; Repo\nCommit -&gt; Post a snapshot\nPush means to send files to cloud\nPull downloads the files to local machine\n\nSetting up repositories on\n“YAML Ain’t Markup Language”"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "Public Sector Data scinece has several considerations different from private sector\n\nPublic good, and equity consideration\nNeeds to be interpretable and not have algorithmic bias\n\nGit is for version control system (VCS)\nGithub is the cloud computing backend of Git\nGithub Lingo\n\nFolder -&gt; Repo\nCommit -&gt; Post a snapshot\nPush means to send files to cloud\nPull downloads the files to local machine\n\nSetting up repositories on\n“YAML Ain’t Markup Language”"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#coding-techniques",
    "href": "weekly-notes/week-01-notes.html#coding-techniques",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nBasics of tidyverse and its accompanying commands of filter(), select(), mutate(), and summarize()\nQuarto functions on how to bold, italics, both bold and italics, code list, and strikethrough\nQuarto uses — as the header call\n“#” are used as the header calls\nTibbles\n\nselect() choose columns\nfilter() choose rows\nmutate() create new variables\nsummarize() calculate statistics\ngroup_by() operate on sub-groups\n\nSummarize and group_by tend ot happen together and typically using a pipeline %&gt;%\n%&gt;% pipelines are used to connect multiple codes in one command line rather than a sequence."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#questions-challenges",
    "href": "weekly-notes/week-01-notes.html#questions-challenges",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nEverything was clear. I would still like to mess around more with Quarto.\nI messed around with Quarto again @ 9/14/2025"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#connections-to-policy",
    "href": "weekly-notes/week-01-notes.html#connections-to-policy",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nThis was a building block week, so not much of direct application rather tracking and documentation baseline for setup."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#reflection",
    "href": "weekly-notes/week-01-notes.html#reflection",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Reflection",
    "text": "Reflection\n\nLearning how to create a custom repository was both enjoyable and insightful.\nIt could also serve as a way to share supplementary analyses and to present code in a more public, graphical, and accessible format for non-coding audiences."
  },
  {
    "objectID": "labs/week-03/scrips/week3_lab_exercise.html",
    "href": "labs/week-03/scrips/week3_lab_exercise.html",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "",
    "text": "# Load required packages\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(scales)\nlibrary(RColorBrewer)\n# Set your Census API key if you haven't already\ncensus_api_key(Sys.getenv(\"CENSUS_API_KEY\"))\n\n# We'll use Pennsylvania data for consistency with previous weeks\nstate_choice &lt;- \"PA\""
  },
  {
    "objectID": "labs/week-03/scrips/week3_lab_exercise.html#setup-and-data-loading",
    "href": "labs/week-03/scrips/week3_lab_exercise.html#setup-and-data-loading",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "",
    "text": "# Load required packages\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(scales)\nlibrary(RColorBrewer)\n# Set your Census API key if you haven't already\ncensus_api_key(Sys.getenv(\"CENSUS_API_KEY\"))\n\n# We'll use Pennsylvania data for consistency with previous weeks\nstate_choice &lt;- \"PA\""
  },
  {
    "objectID": "labs/week-03/scrips/week3_lab_exercise.html#exercise-0-finding-census-variable-codes",
    "href": "labs/week-03/scrips/week3_lab_exercise.html#exercise-0-finding-census-variable-codes",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 0: Finding Census Variable Codes",
    "text": "Exercise 0: Finding Census Variable Codes\nThe Challenge: You know you want data on total population, median income, and median age, but you don’t know the specific Census variable codes. How do you find them?\n\n0.1 Load the Variable Dictionary\n\n# Load all available variables for ACS 5-year 2022\nacs_vars_2022 &lt;- load_variables(2022, \"acs5\", cache = TRUE)\n\n# Look at the structure\nglimpse(acs_vars_2022)\n\nRows: 28,152\nColumns: 4\n$ name      &lt;chr&gt; \"B01001A_001\", \"B01001A_002\", \"B01001A_003\", \"B01001A_004\", …\n$ label     &lt;chr&gt; \"Estimate!!Total:\", \"Estimate!!Total:!!Male:\", \"Estimate!!To…\n$ concept   &lt;chr&gt; \"Sex by Age (White Alone)\", \"Sex by Age (White Alone)\", \"Sex…\n$ geography &lt;chr&gt; \"tract\", \"tract\", \"tract\", \"tract\", \"tract\", \"tract\", \"tract…\n\nhead(acs_vars_2022)\n\n# A tibble: 6 × 4\n  name        label                                   concept          geography\n  &lt;chr&gt;       &lt;chr&gt;                                   &lt;chr&gt;            &lt;chr&gt;    \n1 B01001A_001 Estimate!!Total:                        Sex by Age (Whi… tract    \n2 B01001A_002 Estimate!!Total:!!Male:                 Sex by Age (Whi… tract    \n3 B01001A_003 Estimate!!Total:!!Male:!!Under 5 years  Sex by Age (Whi… tract    \n4 B01001A_004 Estimate!!Total:!!Male:!!5 to 9 years   Sex by Age (Whi… tract    \n5 B01001A_005 Estimate!!Total:!!Male:!!10 to 14 years Sex by Age (Whi… tract    \n6 B01001A_006 Estimate!!Total:!!Male:!!15 to 17 years Sex by Age (Whi… tract    \n\n\nWhat you see:\n\nname: The variable code (e.g., “B01003_001”)\nlabel: Human-readable description\nconcept: The broader table this variable belongs to\n\n\n\n0.2 Search for Population Variables\nYour Task: Find the variable code for total population.\n\n# Search for population-related variables\npopulation_vars &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(label, \"Total.*population\"))\n\n# Look at the results\nhead(population_vars, 10)\n\n# A tibble: 10 × 4\n   name       label                                            concept geography\n   &lt;chr&gt;      &lt;chr&gt;                                            &lt;chr&gt;   &lt;chr&gt;    \n 1 B16008_002 \"Estimate!!Total:!!Native population:\"           Citize… tract    \n 2 B16008_003 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 3 B16008_004 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 4 B16008_005 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 5 B16008_006 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 6 B16008_007 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 7 B16008_008 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 8 B16008_009 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 9 B16008_010 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n10 B16008_011 \"Estimate!!Total:!!Native population:!!18 years… Citize… tract    \n\n# Or search in the concept field\npop_concept &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(concept, \"Total Population\"))\n\nhead(pop_concept)\n\n# A tibble: 6 × 4\n  name        label                             concept                geography\n  &lt;chr&gt;       &lt;chr&gt;                             &lt;chr&gt;                  &lt;chr&gt;    \n1 B01003_001  Estimate!!Total                   Total Population       block gr…\n2 B25008A_001 Estimate!!Total:                  Total Population in O… block gr…\n3 B25008A_002 Estimate!!Total:!!Owner occupied  Total Population in O… block gr…\n4 B25008A_003 Estimate!!Total:!!Renter occupied Total Population in O… block gr…\n5 B25008B_001 Estimate!!Total:                  Total Population in O… block gr…\n6 B25008B_002 Estimate!!Total:!!Owner occupied  Total Population in O… block gr…\n\n\nTip: Look for “Total” followed by “population” - usually B01003_001\n\n\n0.3 Search for Income Variables\nYour Task: Find median household income variables.\n\n# Search for median income\nincome_vars &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(label, \"[Mm]edian.*income\"))\n\n# Look specifically for household income\nhousehold_income &lt;- income_vars %&gt;%\n  filter(str_detect(label, \"household\"))\n\nprint(\"Household income variables:\")\n\n[1] \"Household income variables:\"\n\nhead(household_income)\n\n# A tibble: 6 × 4\n  name        label                                            concept geography\n  &lt;chr&gt;       &lt;chr&gt;                                            &lt;chr&gt;   &lt;chr&gt;    \n1 B10010_002  Estimate!!Median family income in the past 12 m… Median… tract    \n2 B10010_003  Estimate!!Median family income in the past 12 m… Median… tract    \n3 B19013A_001 Estimate!!Median household income in the past 1… Median… tract    \n4 B19013B_001 Estimate!!Median household income in the past 1… Median… tract    \n5 B19013C_001 Estimate!!Median household income in the past 1… Median… tract    \n6 B19013D_001 Estimate!!Median household income in the past 1… Median… tract    \n\n# Alternative: search by concept\nincome_concept &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(concept, \"Median Household Income\"))\n\nhead(income_concept)\n\n# A tibble: 6 × 4\n  name        label                                            concept geography\n  &lt;chr&gt;       &lt;chr&gt;                                            &lt;chr&gt;   &lt;chr&gt;    \n1 B19013A_001 Estimate!!Median household income in the past 1… Median… tract    \n2 B19013B_001 Estimate!!Median household income in the past 1… Median… tract    \n3 B19013C_001 Estimate!!Median household income in the past 1… Median… tract    \n4 B19013D_001 Estimate!!Median household income in the past 1… Median… tract    \n5 B19013E_001 Estimate!!Median household income in the past 1… Median… county   \n6 B19013F_001 Estimate!!Median household income in the past 1… Median… tract    \n\n\nPattern Recognition: Median household income is typically B19013_001\n\n\n0.4 Search for Age Variables\nYour Task: Find median age variables.\n[write the code below - first add a code chunk]\n\n\n0.5 Advanced Search Techniques\nYour Task: Learn more sophisticated search methods.\n\n# Search for multiple terms at once\nhousing_vars &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(label, \"[Mm]edian.*(rent|value)\"))\n\nprint(\"Housing cost variables:\")\n\n[1] \"Housing cost variables:\"\n\nhead(housing_vars, 10)\n\n# A tibble: 10 × 4\n   name         label                                          concept geography\n   &lt;chr&gt;        &lt;chr&gt;                                          &lt;chr&gt;   &lt;chr&gt;    \n 1 B07002PR_004 Estimate!!Median age --!!Total:!!Moved from d… Median… &lt;NA&gt;     \n 2 B07002_004   Estimate!!Median age --!!Total:!!Moved from d… Median… tract    \n 3 B07002_005   Estimate!!Median age --!!Total:!!Moved from d… Median… tract    \n 4 B07011PR_004 Estimate!!Median income in the past 12 months… Median… &lt;NA&gt;     \n 5 B07011_004   Estimate!!Median income in the past 12 months… Median… tract    \n 6 B07011_005   Estimate!!Median income in the past 12 months… Median… tract    \n 7 B07402PR_004 Estimate!!Median age --!!Total living in area… Median… &lt;NA&gt;     \n 8 B07402_004   Estimate!!Median age --!!Total living in area… Median… county   \n 9 B07402_005   Estimate!!Median age --!!Total living in area… Median… county   \n10 B07411PR_004 Estimate!!Median income in the past 12 months… Median… &lt;NA&gt;     \n\n# Search excluding certain terms\nincome_not_family &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(label, \"[Mm]edian.*income\") & \n         !str_detect(label, \"family\"))\n\nprint(\"Income variables (not family income):\")\n\n[1] \"Income variables (not family income):\"\n\nhead(income_not_family)\n\n# A tibble: 6 × 4\n  name         label                                           concept geography\n  &lt;chr&gt;        &lt;chr&gt;                                           &lt;chr&gt;   &lt;chr&gt;    \n1 B06011PR_001 Estimate!!Median income in the past 12 months … Median… &lt;NA&gt;     \n2 B06011PR_002 Estimate!!Median income in the past 12 months … Median… &lt;NA&gt;     \n3 B06011PR_003 Estimate!!Median income in the past 12 months … Median… &lt;NA&gt;     \n4 B06011PR_004 Estimate!!Median income in the past 12 months … Median… &lt;NA&gt;     \n5 B06011PR_005 Estimate!!Median income in the past 12 months … Median… &lt;NA&gt;     \n6 B06011_001   Estimate!!Median income in the past 12 months … Median… tract    \n\n# Case-insensitive search using regex\neducation_vars &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(label, regex(\"bachelor\", ignore_case = TRUE)))\n\nprint(\"Education variables:\")\n\n[1] \"Education variables:\"\n\nhead(education_vars, 5)\n\n# A tibble: 5 × 4\n  name         label                                           concept geography\n  &lt;chr&gt;        &lt;chr&gt;                                           &lt;chr&gt;   &lt;chr&gt;    \n1 B06009PR_005 Estimate!!Total:!!Bachelor's degree             Place … &lt;NA&gt;     \n2 B06009PR_011 Estimate!!Total:!!Born in Puerto Rico:!!Bachel… Place … &lt;NA&gt;     \n3 B06009PR_017 Estimate!!Total:!!Born in the United States:!!… Place … &lt;NA&gt;     \n4 B06009PR_023 Estimate!!Total:!!Native; born elsewhere:!!Bac… Place … &lt;NA&gt;     \n5 B06009PR_029 Estimate!!Total:!!Foreign born:!!Bachelor's de… Place … &lt;NA&gt;     \n\n\n\n\n0.6 Interactive Exploration\nYour Task: Use RStudio’s viewer for easier searching.\n\n# Open the full variable list in RStudio viewer\n# This opens a searchable data table\nView(acs_vars_2022)\n\n# Pro tip: You can also search specific table groups\n# B01 = Age and Sex\n# B19 = Income  \n# B25 = Housing\ntable_b19 &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(name, \"^B19\"))  # ^ means \"starts with\"\n\nprint(\"All B19 (Income) table variables:\")\n\n[1] \"All B19 (Income) table variables:\"\n\nhead(table_b19, 10)\n\n# A tibble: 10 × 4\n   name        label                                concept            geography\n   &lt;chr&gt;       &lt;chr&gt;                                &lt;chr&gt;              &lt;chr&gt;    \n 1 B19001A_001 Estimate!!Total:                     Household Income … tract    \n 2 B19001A_002 Estimate!!Total:!!Less than $10,000  Household Income … tract    \n 3 B19001A_003 Estimate!!Total:!!$10,000 to $14,999 Household Income … tract    \n 4 B19001A_004 Estimate!!Total:!!$15,000 to $19,999 Household Income … tract    \n 5 B19001A_005 Estimate!!Total:!!$20,000 to $24,999 Household Income … tract    \n 6 B19001A_006 Estimate!!Total:!!$25,000 to $29,999 Household Income … tract    \n 7 B19001A_007 Estimate!!Total:!!$30,000 to $34,999 Household Income … tract    \n 8 B19001A_008 Estimate!!Total:!!$35,000 to $39,999 Household Income … tract    \n 9 B19001A_009 Estimate!!Total:!!$40,000 to $44,999 Household Income … tract    \n10 B19001A_010 Estimate!!Total:!!$45,000 to $49,999 Household Income … tract    \n\n\n\n\n0.7 Verify Your Variable Choices\nYour Task: Test your variables by getting a small sample of data.\n\n# Test the variables you found\ntest_vars &lt;- c(\n  total_pop = \"B01003_001\",      # Total population\n  median_income = \"B19013_001\",  # Median household income\n  median_age = \"B01002_001\"      # Median age\n)\n\n# Get data for just one state to test\ntest_data &lt;- get_acs(\n  geography = \"state\",\n  variables = test_vars,\n  state = \"PA\",\n  year = 2022\n)\n\n# Check that you got what you expected\ntest_data\n\n# A tibble: 3 × 5\n  GEOID NAME         variable        estimate   moe\n  &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt; &lt;dbl&gt;\n1 42    Pennsylvania median_age          40.8   0.1\n2 42    Pennsylvania total_pop     12989208    NA  \n3 42    Pennsylvania median_income    73170   347  \n\n\n\n\n0.8 Common Variable Patterns\nReference guide for future use:\n\n# Common patterns to remember:\ncommon_variables &lt;- tribble(\n  ~concept, ~typical_code, ~description,\n  \"Total Population\", \"B01003_001\", \"Total population\",\n  \"Median Age\", \"B01002_001\", \"Median age of population\", \n  \"Median HH Income\", \"B19013_001\", \"Median household income\",\n  \"White Population\", \"B03002_003\", \"White alone population\",\n  \"Black Population\", \"B03002_004\", \"Black/African American alone\",\n  \"Hispanic Population\", \"B03002_012\", \"Hispanic or Latino population\",\n  \"Bachelor's Degree\", \"B15003_022\", \"Bachelor's degree or higher\",\n  \"Median Rent\", \"B25058_001\", \"Median contract rent\",\n  \"Median Home Value\", \"B25077_001\", \"Median value owner-occupied\"\n)\n\nprint(\"Common Census Variables:\")\n\n[1] \"Common Census Variables:\"\n\ncommon_variables\n\n# A tibble: 9 × 3\n  concept             typical_code description                  \n  &lt;chr&gt;               &lt;chr&gt;        &lt;chr&gt;                        \n1 Total Population    B01003_001   Total population             \n2 Median Age          B01002_001   Median age of population     \n3 Median HH Income    B19013_001   Median household income      \n4 White Population    B03002_003   White alone population       \n5 Black Population    B03002_004   Black/African American alone \n6 Hispanic Population B03002_012   Hispanic or Latino population\n7 Bachelor's Degree   B15003_022   Bachelor's degree or higher  \n8 Median Rent         B25058_001   Median contract rent         \n9 Median Home Value   B25077_001   Median value owner-occupied  \n\n\nKey Tips for Variable Hunting:\n\nStart with concepts - search for the topic you want (income, age, housing)\nLook for “Median” vs “Mean” - median is usually more policy-relevant\nCheck the universe - some variables are for “households,” others for “population”\nTest with small data before running large queries\nBookmark useful variables for future projects (type them in your weekly notes!)"
  },
  {
    "objectID": "labs/week-03/scrips/week3_lab_exercise.html#exercise-1-single-variable-eda",
    "href": "labs/week-03/scrips/week3_lab_exercise.html#exercise-1-single-variable-eda",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 1: Single Variable EDA",
    "text": "Exercise 1: Single Variable EDA\n\n1.1 Load and Inspect Data\n\n# Get county-level data for your state\ncounty_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    total_pop = \"B01003_001\",       # Total population\n    median_income = \"B19013_001\",   # Median household income\n    median_age = \"B01002_001\"       # Median age\n  ),\n  state = state_choice,\n  year = 2022,\n  output = \"wide\"\n)\n\n# Clean county names\ncounty_data &lt;- county_data %&gt;%\n  mutate(county_name = str_remove(NAME, paste0(\", \", state_choice)))\n\n# Basic inspection\nglimpse(county_data)\n\nRows: 67\nColumns: 9\n$ GEOID          &lt;chr&gt; \"42001\", \"42003\", \"42005\", \"42007\", \"42009\", \"42011\", \"…\n$ NAME           &lt;chr&gt; \"Adams County, Pennsylvania\", \"Allegheny County, Pennsy…\n$ total_popE     &lt;dbl&gt; 104604, 1245310, 65538, 167629, 47613, 428483, 122640, …\n$ total_popM     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ median_incomeE &lt;dbl&gt; 78975, 72537, 61011, 67194, 58337, 74617, 59386, 60650,…\n$ median_incomeM &lt;dbl&gt; 3334, 869, 2202, 1531, 2606, 1191, 2058, 2167, 1516, 21…\n$ median_ageE    &lt;dbl&gt; 43.8, 40.6, 47.0, 44.9, 47.3, 39.9, 42.9, 43.9, 44.0, 4…\n$ median_ageM    &lt;dbl&gt; 0.2, 0.1, 0.2, 0.1, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, …\n$ county_name    &lt;chr&gt; \"Adams County, Pennsylvania\", \"Allegheny County, Pennsy…\n\n\n\n\n1.2 Explore Income Distribution\nYour Task: Create a histogram of median household income and describe what you see.\n\n# Create histogram of median income\nggplot(county_data) +\n  aes(x = median_incomeE) +\n  geom_histogram(bins = 15, fill = \"steelblue\", alpha = 0.7) +\n  labs(\n    title = \"Distribution of Median Household Income\",\n    x = \"Median Household Income ($)\",\n    y = \"Number of Counties\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = dollar)\n\n\n\n\n\n\n\n\n\n\n1.3 Box Plot for Outlier Detection\nYour Task: Create a boxplot to identify specific outlier counties.\n\n# Box plot to see outliers clearly\nggplot(county_data) +\n  aes(y = median_incomeE) +\n  geom_boxplot(fill = \"lightblue\", width = 0.5) +\n  labs(\n    title = \"Median Income Distribution with Outliers\",\n    y = \"Median Household Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n# Identify the outlier counties\nincome_outliers &lt;- county_data %&gt;%\n  mutate(\n    Q1 = quantile(median_incomeE, 0.25, na.rm = TRUE),\n    Q3 = quantile(median_incomeE, 0.75, na.rm = TRUE),\n    IQR = Q3 - Q1,\n    outlier = median_incomeE &lt; (Q1 - 1.5 * IQR) | median_incomeE &gt; (Q3 + 1.5 * IQR)\n  ) %&gt;%\n  filter(outlier) %&gt;%\n  select(county_name, median_incomeE)\n\nprint(\"Outlier counties:\")\n\n[1] \"Outlier counties:\"\n\nincome_outliers\n\n# A tibble: 3 × 2\n  county_name                     median_incomeE\n  &lt;chr&gt;                                    &lt;dbl&gt;\n1 Bucks County, Pennsylvania              107826\n2 Chester County, Pennsylvania            118574\n3 Montgomery County, Pennsylvania         107441\n\n\n\n\n1.4 Challenge Exercise: Population Distribution\nYour Task: Create your own visualization of population distribution and identify outliers.\nRequirements:\n\nCreate a histogram of total population (total_popE)\nUse a different color than the income example (try “darkgreen” or “purple”)\nAdd appropriate labels and title\nCreate a boxplot to identify population outliers\nFind and list the 3 most populous and 3 least populous counties"
  },
  {
    "objectID": "labs/week-03/scrips/week3_lab_exercise.html#exercise-2-two-variable-relationships",
    "href": "labs/week-03/scrips/week3_lab_exercise.html#exercise-2-two-variable-relationships",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 2: Two Variable Relationships",
    "text": "Exercise 2: Two Variable Relationships\n\n2.1 Population vs Income Scatter Plot\nYour Task: Explore the relationship between population size and median income.\n\n# Basic scatter plot\nggplot(county_data) +\n  aes(x = total_popE, y = median_incomeE) +\n  geom_point() +\n  labs(\n    title = \"Population vs Median Income\",\n    x = \"Total Population\",\n    y = \"Median Household Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n\n\n\n2.2 Add Trend Line and Labels\nYour Task: Improve the plot by adding a trend line and labeling interesting points.\n\n# Enhanced scatter plot with trend line\nggplot(county_data) +\n  aes(x = total_popE, y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(\n    title = \"Population vs Median Income in Pennsylvania Counties\",\n    subtitle = \"2018-2022 ACS 5-Year Estimates\",\n    x = \"Total Population\",\n    y = \"Median Household Income ($)\",\n    caption = \"Source: U.S. Census Bureau\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n# Calculate correlation\ncorrelation &lt;- cor(county_data$total_popE, county_data$median_incomeE, use = \"complete.obs\")\nprint(paste(\"Correlation coefficient:\", round(correlation, 3)))\n\n[1] \"Correlation coefficient: 0.457\"\n\n\n\n\n2.3 Deal with Skewed Data\nYour Task: The population data is highly skewed. Try a log transformation.\n\n# Log-transformed scatter plot\nggplot(county_data) +\n  aes(x = log(total_popE), y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  labs(\n    title = \"Log(Population) vs Median Income\",\n    x = \"Log(Total Population)\",\n    y = \"Median Household Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n\nQuestion: Does the log transformation reveal a clearer relationship?\n\n\n2.4 Challenge Exercise: Age vs Income Relationship\nYour Task: Explore the relationship between median age and median income using different visualization techniques.\nRequirements:\n\nCreate a scatter plot with median age on x-axis and median income on y-axis\nUse red points (color = \"red\") with 50% transparency (alpha = 0.5)\nAdd a smooth trend line using method = \"loess\" instead of “lm”\nUse the “dark” theme (theme_dark())\nFormat the y-axis with dollar signs\nAdd a title that mentions both variables"
  },
  {
    "objectID": "labs/week-03/scrips/week3_lab_exercise.html#exercise-3-data-quality-visualization",
    "href": "labs/week-03/scrips/week3_lab_exercise.html#exercise-3-data-quality-visualization",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 3: Data Quality Visualization",
    "text": "Exercise 3: Data Quality Visualization\n\n3.1 Visualize Margins of Error\nYour Task: Create a visualization showing how data reliability varies across counties.\n\n# Calculate MOE percentages\ncounty_reliability &lt;- county_data %&gt;%\n  mutate(\n    income_moe_pct = (median_incomeM / median_incomeE) * 100,\n    pop_category = case_when(\n      total_popE &lt; 50000 ~ \"Small (&lt;50K)\",\n      total_popE &lt; 200000 ~ \"Medium (50K-200K)\",\n      TRUE ~ \"Large (200K+)\"\n    )\n  )\n\n# MOE by population size\nggplot(county_reliability) +\n  aes(x = total_popE, y = income_moe_pct) +\n  geom_point(alpha = 0.7) +\n  geom_hline(yintercept = 10, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Data Reliability Decreases with Population Size\",\n    x = \"Total Population\",\n    y = \"Margin of Error (%)\",\n    caption = \"Red line = 10% reliability threshold\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma)\n\n\n\n\n\n\n\n\n\n\n3.2 Compare Reliability by County Size\nYour Task: Use box plots to compare MOE across county size categories.\n\n# Box plots by population category\nggplot(county_reliability) +\n  aes(x = pop_category, y = income_moe_pct, fill = pop_category) +\n  geom_boxplot() +\n  labs(\n    title = \"Data Reliability by County Size Category\",\n    x = \"Population Category\",\n    y = \"Margin of Error (%)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")  # Remove legend since x-axis is clear\n\n\n\n\n\n\n\n\n\n\n3.3 Challenge Exercise: Age Data Reliability\nYour Task: Analyze the reliability of median age data across counties.\nRequirements:\n\nCalculate MOE percentage for median age (median_ageM / median_ageE * 100)\nCreate a scatter plot showing population vs age MOE percentage\nUse purple points (color = \"purple\") with size = 2\nAdd a horizontal line at 5% MOE using geom_hline() with a blue dashed line\nUse theme_classic()instead of theme_minimal()\nCreate a boxplot comparing age MOE across the three population categories"
  },
  {
    "objectID": "labs/week-03/scrips/week3_lab_exercise.html#exercise-4-multiple-variables-with-color-and-faceting",
    "href": "labs/week-03/scrips/week3_lab_exercise.html#exercise-4-multiple-variables-with-color-and-faceting",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 4: Multiple Variables with Color and Faceting",
    "text": "Exercise 4: Multiple Variables with Color and Faceting\n\n4.1 Three-Variable Scatter Plot\nYour Task: Add median age as a color dimension to the population-income relationship.\n\n# Three-variable scatter plot\nggplot(county_data) +\n  aes(x = total_popE, y = median_incomeE, color = median_ageE) +\n  geom_point(size = 2, alpha = 0.7) +\n  scale_color_viridis_c(name = \"Median\\nAge\") +\n  labs(\n    title = \"Population, Income, and Age Patterns\",\n    x = \"Total Population\",\n    y = \"Median Household Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n\n\n\n4.2 Create Categories for Faceting\nYour Task: Create age categories and use faceting to compare patterns.\n\n# Create age categories and faceted plot\ncounty_faceted &lt;- county_data %&gt;%\n  mutate(\n    age_category = case_when(\n      median_ageE &lt; 40 ~ \"Young (&lt; 40)\",\n      median_ageE &lt; 45 ~ \"Middle-aged (40-45)\",\n      TRUE ~ \"Older (45+)\"\n    )\n  )\n\nggplot(county_faceted) +\n  aes(x = total_popE, y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~age_category) +\n  labs(\n    title = \"Population-Income Relationship by Age Profile\",\n    x = \"Total Population\",\n    y = \"Median Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n\nQuestion: Do the relationships between population and income differ by age profile?\nYour Task: Create a visualization using income categories and multiple aesthetic mappings.\nRequirements:\n\nCreate income categories: “Low” (&lt;$50k), “Middle” ($50k-$80k), “High” (&gt;$80k)\nMake a scatter plot with population (x) vs median age (y) - Color points by income category\nSize points by the margin of error for income (median_incomeM)\nUse the “Set2” color palette: scale_color_brewer(palette = \"Set2\") **note: you’ll need to load the RColorBrewer package for this`\nFacet by income category using facet_wrap()\nUse theme_bw() theme"
  },
  {
    "objectID": "labs/week-03/scrips/week3_lab_exercise.html#exercise-5-data-joins-and-integration",
    "href": "labs/week-03/scrips/week3_lab_exercise.html#exercise-5-data-joins-and-integration",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 5: Data Joins and Integration",
    "text": "Exercise 5: Data Joins and Integration\n\n5.1 Get Additional Census Data\nYour Task: Load educational attainment data and join it with our existing data.\n\n# Get educational attainment data\neducation_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    total_25plus = \"B15003_001\",    # Total population 25 years and over\n    bachelor_plus = \"B15003_022\"    # Bachelor's degree or higher\n  ),\n  state = state_choice,\n  year = 2022,\n  output = \"wide\"\n) %&gt;%\n  mutate(\n    pct_college = (bachelor_plusE / total_25plusE) * 100,\n    county_name = str_remove(NAME, paste0(\", \", state_choice))\n  ) %&gt;%\n  select(GEOID, county_name, pct_college)\n\n# Check the data\nhead(education_data)\n\n# A tibble: 6 × 3\n  GEOID county_name                    pct_college\n  &lt;chr&gt; &lt;chr&gt;                                &lt;dbl&gt;\n1 42001 Adams County, Pennsylvania           13.9 \n2 42003 Allegheny County, Pennsylvania       25.4 \n3 42005 Armstrong County, Pennsylvania       12.7 \n4 42007 Beaver County, Pennsylvania          18.3 \n5 42009 Bedford County, Pennsylvania          9.73\n6 42011 Berks County, Pennsylvania           17.2 \n\n\n\n\n5.2 Join the Datasets\nYour Task: Join the education data with our main county dataset.\n\n# Perform the join\ncombined_data &lt;- county_data %&gt;%\n  left_join(education_data, by = \"GEOID\")\n\n# Check the join worked\ncat(\"Original data rows:\", nrow(county_data), \"\\n\")\n\nOriginal data rows: 67 \n\ncat(\"Combined data rows:\", nrow(combined_data), \"\\n\")\n\nCombined data rows: 67 \n\ncat(\"Missing education data:\", sum(is.na(combined_data$pct_college)), \"\\n\")\n\nMissing education data: 0 \n\n# View the combined data\nhead(combined_data)\n\n# A tibble: 6 × 11\n  GEOID NAME     total_popE total_popM median_incomeE median_incomeM median_ageE\n  &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;\n1 42001 Adams C…     104604         NA          78975           3334        43.8\n2 42003 Alleghe…    1245310         NA          72537            869        40.6\n3 42005 Armstro…      65538         NA          61011           2202        47  \n4 42007 Beaver …     167629         NA          67194           1531        44.9\n5 42009 Bedford…      47613         NA          58337           2606        47.3\n6 42011 Berks C…     428483         NA          74617           1191        39.9\n# ℹ 4 more variables: median_ageM &lt;dbl&gt;, county_name.x &lt;chr&gt;,\n#   county_name.y &lt;chr&gt;, pct_college &lt;dbl&gt;\n\n\n\n\n5.3 Analyze the New Relationship\nYour Task: Explore the relationship between education and income.\n\n# Education vs Income scatter plot\nggplot(combined_data) +\n  aes(x = pct_college, y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  labs(\n    title = \"Education vs Income Across Counties\",\n    x = \"Percent with Bachelor's Degree or Higher\",\n    y = \"Median Household Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n# Calculate correlation\nedu_income_cor &lt;- cor(combined_data$pct_college, combined_data$median_incomeE, use = \"complete.obs\")\nprint(paste(\"Education-Income Correlation:\", round(edu_income_cor, 3)))\n\n[1] \"Education-Income Correlation: 0.811\"\n\n\n\n\n5.4 Get Housing Data and Triple Join\nYour Task: Add housing cost data to create a three-way analysis.\n\n# Get housing cost data\nhousing_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    median_rent = \"B25058_001\",     # Median contract rent\n    median_home_value = \"B25077_001\" # Median value of owner-occupied units\n  ),\n  state = state_choice,\n  year = 2022,\n  output = \"wide\"\n) %&gt;%\n  select(GEOID, median_rent = median_rentE, median_home_value = median_home_valueE)\n\n# Join all three datasets\nfull_data &lt;- combined_data %&gt;%\n  left_join(housing_data, by = \"GEOID\")\n\n# Create a housing affordability measure\nfull_data &lt;- full_data %&gt;%\n  mutate(\n    rent_to_income = (median_rent * 12) / median_incomeE * 100,\n    income_category = case_when(\n      median_incomeE &lt; 50000 ~ \"Low Income\",\n      median_incomeE &lt; 80000 ~ \"Middle Income\",\n      TRUE ~ \"High Income\"\n    )\n  )\n\nhead(full_data)\n\n# A tibble: 6 × 15\n  GEOID NAME     total_popE total_popM median_incomeE median_incomeM median_ageE\n  &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;\n1 42001 Adams C…     104604         NA          78975           3334        43.8\n2 42003 Alleghe…    1245310         NA          72537            869        40.6\n3 42005 Armstro…      65538         NA          61011           2202        47  \n4 42007 Beaver …     167629         NA          67194           1531        44.9\n5 42009 Bedford…      47613         NA          58337           2606        47.3\n6 42011 Berks C…     428483         NA          74617           1191        39.9\n# ℹ 8 more variables: median_ageM &lt;dbl&gt;, county_name.x &lt;chr&gt;,\n#   county_name.y &lt;chr&gt;, pct_college &lt;dbl&gt;, median_rent &lt;dbl&gt;,\n#   median_home_value &lt;dbl&gt;, rent_to_income &lt;dbl&gt;, income_category &lt;chr&gt;\n\n\n\n\n5.5 Advanced Multi-Variable Analysis\nYour Task: Create a comprehensive visualization showing multiple relationships.\n\n# Complex multi-variable plot\nggplot(full_data) +\n  aes(x = pct_college, y = rent_to_income, \n      color = income_category, size = total_popE) +\n  geom_point(alpha = 0.7) +\n  labs(\n    title = \"Education, Housing Affordability, and Income Patterns\",\n    subtitle = \"Larger points = larger population\",\n    x = \"Percent with Bachelor's Degree or Higher\",\n    y = \"Annual Rent as % of Median Income\",\n    color = \"Income Category\",\n    size = \"Population\"\n  ) +\n  theme_minimal() +\n  guides(size = guide_legend(override.aes = list(alpha = 1)))"
  },
  {
    "objectID": "labs/week-03/scrips/week3_lab_exercise.html#exercise-6-publication-ready-visualization",
    "href": "labs/week-03/scrips/week3_lab_exercise.html#exercise-6-publication-ready-visualization",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 6: Publication-Ready Visualization",
    "text": "Exercise 6: Publication-Ready Visualization\n\n6.1 Create a Policy-Focused Visualization\nYour Task: Combine multiple visualizations to tell a more complete story about county characteristics.\n\n# Create a multi-panel figure\nlibrary(patchwork)  # For combining plots\n\n# Plot 1: Income distribution\np1 &lt;- ggplot(full_data) +\n  aes(x = median_incomeE) +\n  geom_histogram(bins = 15, fill = \"steelblue\", alpha = 0.7) +\n  labs(title = \"A) Income Distribution\", \n       x = \"Median Income ($)\", y = \"Counties\") +\n  scale_x_continuous(labels = dollar) +\n  theme_minimal()\n\n# Plot 2: Education vs Income\np2 &lt;- ggplot(full_data) +\n  aes(x = pct_college, y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"B) Education vs Income\",\n       x = \"% College Educated\", y = \"Median Income ($)\") +\n  scale_y_continuous(labels = dollar) +\n  theme_minimal()\n\n# Plot 3: Housing affordability by income category\np3 &lt;- ggplot(full_data) +\n  aes(x = income_category, y = rent_to_income, fill = income_category) +\n  geom_boxplot() +\n  labs(title = \"C) Housing Affordability by Income\",\n       x = \"Income Category\", y = \"Rent as % of Income\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Plot 4: Data reliability by population\np4 &lt;- ggplot(county_reliability) +\n  aes(x = total_popE, y = income_moe_pct) +\n  geom_point(alpha = 0.7) +\n  geom_hline(yintercept = 10, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"D) Data Reliability\",\n       x = \"Population\", y = \"MOE (%)\") +\n  scale_x_continuous(labels = comma) +\n  theme_minimal()\n\n# Combine all plots\ncombined_plot &lt;- (p1 | p2) / (p3 | p4)\ncombined_plot + plot_annotation(\n  title = \"Pennsylvania County Analysis: Income, Education, and Housing Patterns\",\n  caption = \"Source: American Community Survey 2018-2022\"\n)"
  },
  {
    "objectID": "labs/week-03/scrips/week3_lab_exercise.html#exercise-7-ethical-data-communication---implementing-research-recommendations",
    "href": "labs/week-03/scrips/week3_lab_exercise.html#exercise-7-ethical-data-communication---implementing-research-recommendations",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 7: Ethical Data Communication - Implementing Research Recommendations",
    "text": "Exercise 7: Ethical Data Communication - Implementing Research Recommendations\nBackground: Research by Jurjevich et al. (2018) found that only 27% of planners warn users about unreliable ACS data, violating AICP ethical standards. In this exercise, you’ll practice the five research-based guidelines for ethical ACS data communication.\n\n7.1 Create Professional Data Tables with Uncertainty\nYour Task: Follow the Jurjevich et al. guidelines to create an ethical data presentation.\n\n# Get comprehensive data for ethical analysis\nethical_demo_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    median_income = \"B19013_001\",   # Median household income\n    total_25plus = \"B15003_001\",    # Total population 25 years and over\n    bachelor_plus = \"B15003_022\",   # Bachelor's degree or higher\n    total_pop = \"B01003_001\"        # Total population\n  ),\n  state = state_choice,\n  year = 2022,\n  output = \"wide\"\n) %&gt;%\n  mutate(\n    # Calculate derived statistics\n    pct_college = (bachelor_plusE / total_25plusE) * 100,\n    \n    # Calculate MOE for percentage using error propagation\n    pct_college_moe = pct_college * sqrt((bachelor_plusM/bachelor_plusE)^2 + (total_25plusM/total_25plusE)^2),\n    \n    # Calculate coefficient of variation for all key variables\n    income_cv = (median_incomeM / median_incomeE) * 100,\n    education_cv = (pct_college_moe / pct_college) * 100,\n    \n    # Create reliability categories based on CV\n    income_reliability = case_when(\n      income_cv &lt; 12 ~ \"High\",\n      income_cv &lt;= 40 ~ \"Moderate\", \n      TRUE ~ \"Low\"\n    ),\n    \n    education_reliability = case_when(\n      education_cv &lt; 12 ~ \"High\",\n      education_cv &lt;= 40 ~ \"Moderate\",\n      TRUE ~ \"Low\"\n    ),\n    \n    # Create color coding for reliability\n    income_color = case_when(\n      income_reliability == \"High\" ~ \"🟢\",\n      income_reliability == \"Moderate\" ~ \"🟡\",\n      TRUE ~ \"🔴\"\n    ),\n    \n    education_color = case_when(\n      education_reliability == \"High\" ~ \"🟢\",\n      education_reliability == \"Moderate\" ~ \"🟡\", \n      TRUE ~ \"🔴\"\n    ),\n    \n    # Clean county names\n    county_name = str_remove(NAME, paste0(\", \", state_choice))\n  )\n\n# Create ethical data table focusing on least reliable estimates\nethical_data_table &lt;- ethical_demo_data %&gt;%\n  select(county_name, median_incomeE, median_incomeM, income_cv, income_color,\n         pct_college, pct_college_moe, education_cv, education_color) %&gt;%\n  arrange(desc(income_cv)) %&gt;%  # Show least reliable first\n  slice_head(n = 10)\n\n# Create professional table following guidelines\nlibrary(knitr)\nlibrary(kableExtra)\n\nethical_data_table %&gt;%\n  select(county_name, median_incomeE, median_incomeM, income_cv, income_color) %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"Margin of Error\", \n                  \"CV (%)\", \"Reliability\"),\n    caption = \"Pennsylvania Counties: Median Household Income with Statistical Uncertainty\",\n    format.args = list(big.mark = \",\")\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\")) %&gt;%\n  footnote(\n    general = c(\"Coefficient of Variation (CV) indicates reliability:\",\n                \"🟢 High reliability (CV &lt; 12%)\",\n                \"🟡 Moderate reliability (CV 12-40%)\", \n                \"🔴 Low reliability (CV &gt; 40%)\",\n                \"Following Jurjevich et al. (2018) research recommendations\",\n                \"Source: American Community Survey 2018-2022 5-Year Estimates\"),\n    general_title = \"Notes:\"\n  )\n\n\nPennsylvania Counties: Median Household Income with Statistical Uncertainty\n\n\nCounty\nMedian Income\nMargin of Error\nCV (%)\nReliability\n\n\n\n\nForest County, Pennsylvania\n46,188\n4,612\n9.985278\n🟢 |\n\n\nSullivan County, Pennsylvania\n62,910\n5,821\n9.252901\n🟢 |\n\n\nUnion County, Pennsylvania\n64,914\n4,753\n7.321995\n🟢 |\n\n\nMontour County, Pennsylvania\n72,626\n5,146\n7.085617\n🟢 |\n\n\nElk County, Pennsylvania\n61,672\n4,091\n6.633480\n🟢 |\n\n\nGreene County, Pennsylvania\n66,283\n4,247\n6.407374\n🟢 |\n\n\nCameron County, Pennsylvania\n46,186\n2,605\n5.640237\n🟢 |\n\n\nSnyder County, Pennsylvania\n65,914\n3,666\n5.561793\n🟢 |\n\n\nCarbon County, Pennsylvania\n64,538\n3,424\n5.305402\n🟢 |\n\n\nWarren County, Pennsylvania\n57,925\n3,005\n5.187743\n🟢 |\n\n\n\nNotes:\n\n\n\n\n\n\n Coefficient of Variation (CV) indicates reliability:\n\n\n\n\n\n\n 🟢 High reliability (CV &lt; 12%)\n\n\n\n\n\n\n 🟡 Moderate reliability (CV 12-40%)\n\n\n\n\n\n\n 🔴 Low reliability (CV &gt; 40%)\n\n\n\n\n\n\n Following Jurjevich et al. (2018) research recommendations\n\n\n\n\n\n\n Source: American Community Survey 2018-2022 5-Year Estimates\n\n\n\n\n\n\n\n\n\n\n\n\n7.3 Now try Census Tracts\n\n# Get census tract poverty data for Philadelphia\nphilly_poverty &lt;- get_acs(\n    geography = \"tract\",\n    variables = c(\n      poverty_pop = \"B17001_001\",     \n      poverty_below = \"B17001_002\"    \n    ),\n    state = \"PA\",\n    county = \"101\",\n    year = 2022,\n    output = \"wide\"\n  ) %&gt;%\n  filter(poverty_popE &gt; 0) %&gt;%  # Remove tracts with no poverty data\n  mutate(\n    # Calculate poverty rate and its MOE\n    poverty_rate = (poverty_belowE / poverty_popE) * 100,\n    \n    # MOE for derived percentage using error propagation\n    poverty_rate_moe = poverty_rate * sqrt((poverty_belowM/poverty_belowE)^2 + (poverty_popM/poverty_popE)^2),\n    \n    # Coefficient of variation\n    poverty_cv = (poverty_rate_moe / poverty_rate) * 100,\n    \n    # Reliability assessment\n    reliability = case_when(\n      poverty_cv &lt; 12 ~ \"High\",\n      poverty_cv &lt;= 40 ~ \"Moderate\",\n      poverty_cv &lt;= 75 ~ \"Low\",\n      TRUE ~ \"Very Low\"\n    ),\n    \n    # Color coding\n    reliability_color = case_when(\n      reliability == \"High\" ~ \"🟢\",\n      reliability == \"Moderate\" ~ \"🟡\",\n      reliability == \"Low\" ~ \"🟠\",\n      TRUE ~ \"🔴\"\n    ),\n    \n    # Population size categories\n    pop_category = case_when(\n      poverty_popE &lt; 500 ~ \"Very Small (&lt;500)\",\n      poverty_popE &lt; 1000 ~ \"Small (500-1000)\",\n      poverty_popE &lt; 1500 ~ \"Medium (1000-1500)\",\n      TRUE ~ \"Large (1500+)\"\n    )\n  )\n\n# Check the data quality crisis at tracts\nreliability_summary &lt;- philly_poverty %&gt;%\n  count(reliability) %&gt;%\n  mutate(\n    percentage = round(n / sum(n) * 100, 1),\n    total_bg = sum(n)\n  )\n\nprint(\"Philadelphia Census Tract Poverty Data Reliability:\")\n\n[1] \"Philadelphia Census Tract Poverty Data Reliability:\"\n\nreliability_summary %&gt;%\n  kable(\n    col.names = c(\"Data Quality\", \"Number of Tracts\", \"Percentage\", \"Total\"),\n    caption = \"The Data Quality Crisis: Philadelphia Census Tract Poverty Estimates\"\n  ) %&gt;%\n  kable_styling()\n\n\nThe Data Quality Crisis: Philadelphia Census Tract Poverty Estimates\n\n\nData Quality\nNumber of Tracts\nPercentage\nTotal\n\n\n\n\nLow\n295\n75.8\n389\n\n\nModerate\n53\n13.6\n389\n\n\nVery Low\n41\n10.5\n389\n\n\n\n\n\n\n# Show the most problematic estimates (following Guideline 3: provide context)\nworst_estimates &lt;- philly_poverty %&gt;%\n  filter(reliability %in% c(\"Low\", \"Very Low\")) %&gt;%\n  arrange(desc(poverty_cv)) %&gt;%\n  slice_head(n = 10)\n\nworst_estimates %&gt;%\n  select(GEOID, poverty_rate, poverty_rate_moe, poverty_cv, reliability_color, poverty_popE) %&gt;%\n  kable(\n    col.names = c(\"Tract\", \"Poverty Rate (%)\", \"MOE\", \"CV (%)\", \"Quality\", \"Pop Size\"),\n    caption = \"Guideline 3: Tracts with Least Reliable Poverty Estimates\",\n    digits = c(0, 1, 1, 1, 0, 0)\n  ) %&gt;%\n  kable_styling() %&gt;%\n  footnote(\n    general = c(\"These estimates should NOT be used for policy decisions\",\n                \"CV &gt; 75% indicates very low reliability\",\n                \"Recommend aggregation or alternative data sources\")\n  )\n\n\nGuideline 3: Tracts with Least Reliable Poverty Estimates\n\n\nTract\nPoverty Rate (%)\nMOE\nCV (%)\nQuality\nPop Size\n\n\n\n\n42101989100\n15.8\n45.2\n286.1\n🔴 |\n38|\n\n\n42101000101\n0.7\n1.1\n157.9\n🔴 |\n1947|\n\n\n42101980200\n37.9\n45.2\n119.4\n🔴 |\n66|\n\n\n42101023100\n3.8\n4.5\n119.4\n🔴 |\n1573|\n\n\n42101025600\n1.7\n2.0\n114.2\n🔴 |\n2642|\n\n\n42101014202\n1.7\n1.8\n107.0\n🔴 |\n2273|\n\n\n42101000403\n6.6\n6.7\n101.8\n🔴 |\n1047|\n\n\n42101026100\n4.7\n4.4\n95.0\n🔴 |\n2842|\n\n\n42101036502\n4.9\n4.7\n94.9\n🔴 |\n4284|\n\n\n42101032000\n21.8\n20.6\n94.8\n🔴 |\n7873|\n\n\n\nNote: \n\n\n\n\n\n\n\n These estimates should NOT be used for policy decisions\n\n\n\n\n\n\n\n CV &gt; 75% indicates very low reliability\n\n\n\n\n\n\n\n Recommend aggregation or alternative data sources"
  },
  {
    "objectID": "labs/week-03/scrips/week3_lab_exercise.html#key-references-and-acknowledgments",
    "href": "labs/week-03/scrips/week3_lab_exercise.html#key-references-and-acknowledgments",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Key References and Acknowledgments",
    "text": "Key References and Acknowledgments\nJurjevich, J. R., Griffin, A. L., Spielman, S. E., Folch, D. C., Merrick, M., & Nagle, N. N. (2018). Navigating statistical uncertainty: How urban and regional planners understand and work with American community survey (ACS) data for guiding policy. Journal of the American Planning Association, 84(2), 112-126.\nWalker, K. (2023). Analyzing US Census Data: Methods, Maps, and Models in R. Available at: https://walker-data.com/census-r/\nAI Acknowledgments: This lab was developed with coding assistance from Claude AI. I have run, reviewed, and edited the final version. Any remaining errors are my own."
  },
  {
    "objectID": "labs/lab_setup_instructions.html",
    "href": "labs/lab_setup_instructions.html",
    "title": "Lab Setup Instructions",
    "section": "",
    "text": "Each week, you’ll complete lab assignments and add them to your portfolio website. This document explains the one-time setup process and the weekly workflow you’ll follow all semester.\n\n\n\n\n\nFirst, clone the course repository (you only need to do this once):\n\nOpen Terminal/Command Prompt\nNavigate to where you want to store course materials:\ncd ~/Desktop  # or wherever you keep school folders\nClone the course repository:\ngit clone https://github.com/MUSA-5080-Fall-2025/MUSA-5080-Fall-2025.git\n\nImportant: This creates a separate folder from your portfolio repository.\n\n\n\nIn your portfolio repository, create this folder structure:\nyour-portfolio/\n├── _quarto.yml           # You'll edit this\n├── index.qmd            # Your home page  \n├── weekly-notes/        # Already exists\n├── labs/                # CREATE THIS FOLDER\n│   └── lab_0/           # CREATE THIS FOLDER\n│       ├── lab_0_template.qmd    # You'll copy this file here\n│       └── data/        # CREATE THIS FOLDER (copy data folder)\n└── ... (other files)\nIn RStudio: 1. In the Files pane, click “New Folder” 2. Name it labs 3. Inside labs, create another folder called lab_0 4. Inside lab_0, create a folder called data\n\n\n\nOpen your _quarto.yml file and modify the navbar section to add a Labs menu:\nReplace this section:\nnavbar:\n  left:\n    - href: index.qmd\n      text: Home\n    - href: weekly-notes/\n      text: Weekly Notes\nWith this:\nnavbar:\n  left:\n    - href: index.qmd\n      text: Home\n    - href: weekly-notes/\n      text: Weekly Notes\n    - text: Labs\n      menu:\n        - href: labs/lab_0/\n          text: Lab 0: dplyr Basics\nImportant: Make sure the indentation matches exactly! YAML is sensitive to spacing.\n\n\n\n\nSave your _quarto.yml file\nIn RStudio, go to Build → Render Website\nCheck that you now see a “Labs” dropdown in your navigation menu\n\n\n\n\n\n\n\n\nGo to the course repository\nNavigate to labs/lab_X/ (where X is the lab number)\nDownload or copy the lab template files\n\n\n\n\n\nIn your portfolio, create a new folder: labs/lab_X/\nCopy the template .qmd file into this folder\nCopy any data files into labs/lab_X/data/\n\n\n\n\n\nOpen the lab .qmd file\nUpdate the author field with your name\nComplete all the exercises\nFill in all the “[YOUR ANSWER]” sections\n\n\n\n\nAdd the new lab to your _quarto.yml file:\nnavbar:\n  left:\n    - href: index.qmd\n      text: Home\n    - href: weekly-notes/\n      text: Weekly Notes\n    - text: Labs\n      menu:\n        - href: labs/lab_0/\n          text: Lab 0: dplyr Basics\n        - href: labs/lab_1/           # ADD NEW LABS HERE\n          text: Lab 1: [Lab Title]\n\n\n\n\nBuild your site: Build → Render Website\nCheck locally: Make sure everything looks good\nCommit your changes:\ngit add .\ngit commit -m \"Complete Lab X: [brief description]\"\nPush to GitHub:\ngit push origin main\nCheck your live site: Your portfolio should update automatically\n\n\n\n\n\n\n\n\nCheck your _quarto.yml indentation\nMake sure all file paths are correct\nLook for typos in file names\n\n\n\n\n\nVerify you added it to _quarto.yml\nCheck that the href: path matches your folder structure\nMake sure you rendered the website after making changes\n\n\n\n\n\nCheck that data files are in the right location\nVerify you’re using relative paths: \"data/filename.csv\"\nMake sure you’re working within your R project\n\n\n\n\n\nConfirm you committed and pushed all changes\nCheck that GitHub Actions completed successfully (green checkmark)\nWait a few minutes - updates can take time\n\n\n\n\n\n\nWork incrementally: Complete one exercise at a time, render frequently\nUse descriptive commit messages: “Complete filtering exercises for Lab 0”\nKeep files organized: Don’t put data files in wrong folders\nTest locally first: Always render and check before pushing\nAsk for help: Use office hours or discussion board if stuck\n\n\n\n\nyour-portfolio/\n├── _quarto.yml\n├── index.qmd\n├── weekly-notes/\n│   ├── index.qmd\n│   ├── week_01.qmd\n│   └── week_02.qmd\n├── labs/\n│   ├── lab_0/\n│   │   ├── index.qmd\n│   │   └── data/\n│   │       └── car_sales_data.csv\n│   ├── lab_1/\n│   │   ├── index.qmd\n│   │   └── data/\n│   │       └── census_data.csv\n│   └── lab_2/\n│       ├── index.qmd\n│       └── data/\n│           └── housing_data.csv\n└── docs/                    # Generated by Quarto\n    ├── index.html\n    ├── weekly-notes/\n    ├── labs/\n    └── ...\nBy the end of the semester, you’ll have a professional portfolio showcasing all your data analysis work!\n\n\n\n\nTechnical issues: Post on Canvas discussion board\nConceptual questions: Come to office hours\nGitHub problems: Include error messages in your questions"
  },
  {
    "objectID": "labs/lab_setup_instructions.html#overview",
    "href": "labs/lab_setup_instructions.html#overview",
    "title": "Lab Setup Instructions",
    "section": "",
    "text": "Each week, you’ll complete lab assignments and add them to your portfolio website. This document explains the one-time setup process and the weekly workflow you’ll follow all semester."
  },
  {
    "objectID": "labs/lab_setup_instructions.html#one-time-setup-week-1-only",
    "href": "labs/lab_setup_instructions.html#one-time-setup-week-1-only",
    "title": "Lab Setup Instructions",
    "section": "",
    "text": "First, clone the course repository (you only need to do this once):\n\nOpen Terminal/Command Prompt\nNavigate to where you want to store course materials:\ncd ~/Desktop  # or wherever you keep school folders\nClone the course repository:\ngit clone https://github.com/MUSA-5080-Fall-2025/MUSA-5080-Fall-2025.git\n\nImportant: This creates a separate folder from your portfolio repository.\n\n\n\nIn your portfolio repository, create this folder structure:\nyour-portfolio/\n├── _quarto.yml           # You'll edit this\n├── index.qmd            # Your home page  \n├── weekly-notes/        # Already exists\n├── labs/                # CREATE THIS FOLDER\n│   └── lab_0/           # CREATE THIS FOLDER\n│       ├── lab_0_template.qmd    # You'll copy this file here\n│       └── data/        # CREATE THIS FOLDER (copy data folder)\n└── ... (other files)\nIn RStudio: 1. In the Files pane, click “New Folder” 2. Name it labs 3. Inside labs, create another folder called lab_0 4. Inside lab_0, create a folder called data\n\n\n\nOpen your _quarto.yml file and modify the navbar section to add a Labs menu:\nReplace this section:\nnavbar:\n  left:\n    - href: index.qmd\n      text: Home\n    - href: weekly-notes/\n      text: Weekly Notes\nWith this:\nnavbar:\n  left:\n    - href: index.qmd\n      text: Home\n    - href: weekly-notes/\n      text: Weekly Notes\n    - text: Labs\n      menu:\n        - href: labs/lab_0/\n          text: Lab 0: dplyr Basics\nImportant: Make sure the indentation matches exactly! YAML is sensitive to spacing.\n\n\n\n\nSave your _quarto.yml file\nIn RStudio, go to Build → Render Website\nCheck that you now see a “Labs” dropdown in your navigation menu"
  },
  {
    "objectID": "labs/lab_setup_instructions.html#weekly-workflow-every-lab",
    "href": "labs/lab_setup_instructions.html#weekly-workflow-every-lab",
    "title": "Lab Setup Instructions",
    "section": "",
    "text": "Go to the course repository\nNavigate to labs/lab_X/ (where X is the lab number)\nDownload or copy the lab template files\n\n\n\n\n\nIn your portfolio, create a new folder: labs/lab_X/\nCopy the template .qmd file into this folder\nCopy any data files into labs/lab_X/data/\n\n\n\n\n\nOpen the lab .qmd file\nUpdate the author field with your name\nComplete all the exercises\nFill in all the “[YOUR ANSWER]” sections\n\n\n\n\nAdd the new lab to your _quarto.yml file:\nnavbar:\n  left:\n    - href: index.qmd\n      text: Home\n    - href: weekly-notes/\n      text: Weekly Notes\n    - text: Labs\n      menu:\n        - href: labs/lab_0/\n          text: Lab 0: dplyr Basics\n        - href: labs/lab_1/           # ADD NEW LABS HERE\n          text: Lab 1: [Lab Title]\n\n\n\n\nBuild your site: Build → Render Website\nCheck locally: Make sure everything looks good\nCommit your changes:\ngit add .\ngit commit -m \"Complete Lab X: [brief description]\"\nPush to GitHub:\ngit push origin main\nCheck your live site: Your portfolio should update automatically"
  },
  {
    "objectID": "labs/lab_setup_instructions.html#troubleshooting",
    "href": "labs/lab_setup_instructions.html#troubleshooting",
    "title": "Lab Setup Instructions",
    "section": "",
    "text": "Check your _quarto.yml indentation\nMake sure all file paths are correct\nLook for typos in file names\n\n\n\n\n\nVerify you added it to _quarto.yml\nCheck that the href: path matches your folder structure\nMake sure you rendered the website after making changes\n\n\n\n\n\nCheck that data files are in the right location\nVerify you’re using relative paths: \"data/filename.csv\"\nMake sure you’re working within your R project\n\n\n\n\n\nConfirm you committed and pushed all changes\nCheck that GitHub Actions completed successfully (green checkmark)\nWait a few minutes - updates can take time"
  },
  {
    "objectID": "labs/lab_setup_instructions.html#tips-for-success",
    "href": "labs/lab_setup_instructions.html#tips-for-success",
    "title": "Lab Setup Instructions",
    "section": "",
    "text": "Work incrementally: Complete one exercise at a time, render frequently\nUse descriptive commit messages: “Complete filtering exercises for Lab 0”\nKeep files organized: Don’t put data files in wrong folders\nTest locally first: Always render and check before pushing\nAsk for help: Use office hours or discussion board if stuck"
  },
  {
    "objectID": "labs/lab_setup_instructions.html#example-portfolio-structure-after-several-labs",
    "href": "labs/lab_setup_instructions.html#example-portfolio-structure-after-several-labs",
    "title": "Lab Setup Instructions",
    "section": "",
    "text": "your-portfolio/\n├── _quarto.yml\n├── index.qmd\n├── weekly-notes/\n│   ├── index.qmd\n│   ├── week_01.qmd\n│   └── week_02.qmd\n├── labs/\n│   ├── lab_0/\n│   │   ├── index.qmd\n│   │   └── data/\n│   │       └── car_sales_data.csv\n│   ├── lab_1/\n│   │   ├── index.qmd\n│   │   └── data/\n│   │       └── census_data.csv\n│   └── lab_2/\n│       ├── index.qmd\n│       └── data/\n│           └── housing_data.csv\n└── docs/                    # Generated by Quarto\n    ├── index.html\n    ├── weekly-notes/\n    ├── labs/\n    └── ...\nBy the end of the semester, you’ll have a professional portfolio showcasing all your data analysis work!"
  },
  {
    "objectID": "labs/lab_setup_instructions.html#questions",
    "href": "labs/lab_setup_instructions.html#questions",
    "title": "Lab Setup Instructions",
    "section": "",
    "text": "Technical issues: Post on Canvas discussion board\nConceptual questions: Come to office hours\nGitHub problems: Include error messages in your questions"
  },
  {
    "objectID": "instructions_week1.html",
    "href": "instructions_week1.html",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Welcome to MUSA 5080! This guide will help you set up your personal portfolio repository for the semester.\n\n\nBy the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey\n\n\n\n\nThis is what you are building: Dr. Delmelle’s sample portfolio\n\n\n\n\nBefore starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed\n\n\n\n\n\nYou should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL\n\n\n\n\n\nIf you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!\n\n\n\n\nEach week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes\n\n\n\n\n\n\nWait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version\n\n\n\n\n\n\nCommit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis\n\n\n\n\n\nQuarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial\n\n\n\n\nDuring Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too\n\n\n\nBefore submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "instructions_week1.html#what-youre-building",
    "href": "instructions_week1.html#what-youre-building",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "By the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey"
  },
  {
    "objectID": "instructions_week1.html#example",
    "href": "instructions_week1.html#example",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "This is what you are building: Dr. Delmelle’s sample portfolio"
  },
  {
    "objectID": "instructions_week1.html#prerequisites",
    "href": "instructions_week1.html#prerequisites",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed"
  },
  {
    "objectID": "instructions_week1.html#step-by-step-setup",
    "href": "instructions_week1.html#step-by-step-setup",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "You should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL"
  },
  {
    "objectID": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "href": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "If you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!"
  },
  {
    "objectID": "instructions_week1.html#weekly-workflow",
    "href": "instructions_week1.html#weekly-workflow",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Each week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes"
  },
  {
    "objectID": "instructions_week1.html#troubleshooting",
    "href": "instructions_week1.html#troubleshooting",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Wait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version"
  },
  {
    "objectID": "instructions_week1.html#pro-tips",
    "href": "instructions_week1.html#pro-tips",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Commit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis"
  },
  {
    "objectID": "instructions_week1.html#additional-resources",
    "href": "instructions_week1.html#additional-resources",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Quarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial"
  },
  {
    "objectID": "instructions_week1.html#getting-help",
    "href": "instructions_week1.html#getting-help",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "During Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too"
  },
  {
    "objectID": "instructions_week1.html#checklist",
    "href": "instructions_week1.html#checklist",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae: Mohamad Al Abbas",
    "section": "",
    "text": "PhD in Demography, University of Pennsylvania, Expected May 2028\n\nMA in International Affairs, Lebanese American University, May 2023\nBE in Electrical Engineering, Lebanese American University, August 2019\n\n\n\n\n\nTeaching Assistant, University of Pennsylvania – Graduate School of Education (2025–present)\n\nLead lab sessions on STATA for Principles of Monitoring and Evaluation, a graduate-level course\n\nResearch Assistant, University of Pennsylvania – PIRE Project (2023–present)\n\nNSF-funded project on environmental degradation and child development\n\nPaper: Born in a Haze (forest fires & health outcomes in Indonesia)\n\n\nStatistical Consultant, Department of Sociology, University of Pennsylvania (2023–present)\n\nSupport PhD students in Sociology and Demography\n\nConduct workshops on machine learning techniques\n\n\nResearch Intern, UNDP Office of Audit & Investigations (2022–2023)\n\nDrafted investigation plans, reports, and conducted interviews\n\n\nResearch Intern, Center for International Policy – Technology Policy Program (2022–2023)\n\nDesigned Social Media Harms tracker\n\nAuthored briefs and attended U.S. legislative hearings\n\n\nResearch Assistant – Individual Contractor, UN ESCWA (2022)\n\nBuilt/improved social demographic database (29% → 83% completion)\n\n\nMonitoring & Evaluation Officer, LAU – Graduate Studies & Research (2019–2021)\n\nGraduate Teaching Assistant, LAU – Electrical & Computer Engineering (2019–2021)\n\n\n\n\n\nBenjamin Franklin Fellowship, University of Pennsylvania (2024–2028)\n\nDean’s Fellowship, University of Pennsylvania (2023–2024)\n\nMEPI – Tomorrow’s Leaders Graduate Fellowship, LAU (2021–2023)\n\nOutstanding Researcher Award, LAU (2021)\n\nBest Presentation Award, ICIET Japan (2022)\n\n\n\n\n\nProgramming: R, Python, Stata, Java, Assembly\n\nMachine Learning: NLP, RL, Neural Networks (CNN & RNN), Decision Trees\n\nSoftware: Tableau, MATLAB, SPSS, LaTeX\n\nLanguages: English (Bilingual), Arabic (Native)\n\n\n\n\n\nEmail: ma96@upenn.edu\n\nLinkedIn: linkedin.com/in/mohammadalabbas96\n\nORCID: 0000-0002-2084-8856"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Curriculum Vitae: Mohamad Al Abbas",
    "section": "",
    "text": "PhD in Demography, University of Pennsylvania, Expected May 2028\n\nMA in International Affairs, Lebanese American University, May 2023\nBE in Electrical Engineering, Lebanese American University, August 2019"
  },
  {
    "objectID": "cv.html#experience",
    "href": "cv.html#experience",
    "title": "Curriculum Vitae: Mohamad Al Abbas",
    "section": "",
    "text": "Teaching Assistant, University of Pennsylvania – Graduate School of Education (2025–present)\n\nLead lab sessions on STATA for Principles of Monitoring and Evaluation, a graduate-level course\n\nResearch Assistant, University of Pennsylvania – PIRE Project (2023–present)\n\nNSF-funded project on environmental degradation and child development\n\nPaper: Born in a Haze (forest fires & health outcomes in Indonesia)\n\n\nStatistical Consultant, Department of Sociology, University of Pennsylvania (2023–present)\n\nSupport PhD students in Sociology and Demography\n\nConduct workshops on machine learning techniques\n\n\nResearch Intern, UNDP Office of Audit & Investigations (2022–2023)\n\nDrafted investigation plans, reports, and conducted interviews\n\n\nResearch Intern, Center for International Policy – Technology Policy Program (2022–2023)\n\nDesigned Social Media Harms tracker\n\nAuthored briefs and attended U.S. legislative hearings\n\n\nResearch Assistant – Individual Contractor, UN ESCWA (2022)\n\nBuilt/improved social demographic database (29% → 83% completion)\n\n\nMonitoring & Evaluation Officer, LAU – Graduate Studies & Research (2019–2021)\n\nGraduate Teaching Assistant, LAU – Electrical & Computer Engineering (2019–2021)"
  },
  {
    "objectID": "cv.html#fellowships-awards",
    "href": "cv.html#fellowships-awards",
    "title": "Curriculum Vitae: Mohamad Al Abbas",
    "section": "",
    "text": "Benjamin Franklin Fellowship, University of Pennsylvania (2024–2028)\n\nDean’s Fellowship, University of Pennsylvania (2023–2024)\n\nMEPI – Tomorrow’s Leaders Graduate Fellowship, LAU (2021–2023)\n\nOutstanding Researcher Award, LAU (2021)\n\nBest Presentation Award, ICIET Japan (2022)"
  },
  {
    "objectID": "cv.html#skills",
    "href": "cv.html#skills",
    "title": "Curriculum Vitae: Mohamad Al Abbas",
    "section": "",
    "text": "Programming: R, Python, Stata, Java, Assembly\n\nMachine Learning: NLP, RL, Neural Networks (CNN & RNN), Decision Trees\n\nSoftware: Tableau, MATLAB, SPSS, LaTeX\n\nLanguages: English (Bilingual), Arabic (Native)"
  },
  {
    "objectID": "cv.html#contact",
    "href": "cv.html#contact",
    "title": "Curriculum Vitae: Mohamad Al Abbas",
    "section": "",
    "text": "Email: ma96@upenn.edu\n\nLinkedIn: linkedin.com/in/mohammadalabbas96\n\nORCID: 0000-0002-2084-8856"
  },
  {
    "objectID": "Assignments/Assignment_1/assignment1_template.html",
    "href": "Assignments/Assignment_1/assignment1_template.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the California Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "Assignments/Assignment_1/assignment1_template.html#scenario",
    "href": "Assignments/Assignment_1/assignment1_template.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the California Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "Assignments/Assignment_1/assignment1_template.html#learning-objectives",
    "href": "Assignments/Assignment_1/assignment1_template.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "Assignments/Assignment_1/assignment1_template.html#data-retrieval",
    "href": "Assignments/Assignment_1/assignment1_template.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\nYour Task: Use get_acs() to retrieve county-level data for your chosen state.\nRequirements: - Geography: county level - Variables: median household income (B19013_001) and total population (B01003_001)\n- Year: 2022 - Survey: acs5 - Output format: wide\nHint: Remember to give your variables descriptive names using the variables = c(name = \"code\") syntax.\n\n# Write your get_acs() code here\ncounty_vars &lt;- c( med_hh_income = \"B19013_001\", total_pop     = \"B01003_001\")\n\ncounty_raw &lt;- get_acs(geography = \"county\", state =  my_state, survey = \"acs5\", year = 2022, variables = county_vars, output = \"wide\")\n\n# Clean the county names to remove state name and \"County\" \n# Hint: use mutate() with str_remove()\n\ncounty &lt;- county_raw %&gt;%\n  mutate(\n    county_name = str_remove(NAME, paste0(\", \", my_state)),\n    county_name = str_remove(county_name, \" County$\")\n  ) %&gt;%\n  select(GEOID, county_name, med_hh_incomeE, med_hh_incomeM, total_popE, total_popM)\n\n# Display the first few rows\nhead(county)\n\n# A tibble: 6 × 6\n  GEOID county_name med_hh_incomeE med_hh_incomeM total_popE total_popM\n  &lt;chr&gt; &lt;chr&gt;                &lt;dbl&gt;          &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 06001 Alameda             122488           1231    1663823         NA\n2 06003 Alpine              101125          17442       1515        206\n3 06005 Amador               74853           6048      40577         NA\n4 06007 Butte                66085           2261     213605         NA\n5 06009 Calaveras            77526           3875      45674         NA\n6 06011 Colusa               69619           5745      21811         NA"
  },
  {
    "objectID": "Assignments/Assignment_1/assignment1_template.html#data-quality-assessment",
    "href": "Assignments/Assignment_1/assignment1_template.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\nYour Task: Calculate margin of error percentages and create reliability categories.\nRequirements: - Calculate MOE percentage: (margin of error / estimate) * 100 - Create reliability categories: - High Confidence: MOE &lt; 5% - Moderate Confidence: MOE 5-10%\n- Low Confidence: MOE &gt; 10% - Create a flag for unreliable estimates (MOE &gt; 10%)\nHint: Use mutate() with case_when() for the categories.\n\n# Calculate MOE percentage and reliability categories using mutate()\ncounty_reliability &lt;- county %&gt;%\n  mutate(\n    moe_percentage = round((med_hh_incomeM/med_hh_incomeE) * 100, 2),\n    Reliability = case_when(\n      moe_percentage &lt; 5 ~ \"High Confidence\",\n      moe_percentage &gt;= 5 & moe_percentage &lt;= 10 ~ \"Moderate Confidence\",\n      moe_percentage &gt; 10 ~ \"Low Confidence\"\n    )\n  )\n\n# Create a summary showing count of counties in each reliability category\n# Hint: use count() and mutate() to add percentages\n\nreliability_summary &lt;- county_reliability %&gt;%\n  count(Reliability, name = \"Count\") %&gt;%\n  mutate(Proportion = round(100 * Count / sum(Count), 1))\n\nkable(reliability_summary, caption = \"County Income Reliability Categories\")\n\n\nCounty Income Reliability Categories\n\n\nReliability\nCount\nProportion\n\n\n\n\nHigh Confidence\n41\n70.7\n\n\nLow Confidence\n5\n8.6\n\n\nModerate Confidence\n12\n20.7"
  },
  {
    "objectID": "Assignments/Assignment_1/assignment1_template.html#high-uncertainty-counties",
    "href": "Assignments/Assignment_1/assignment1_template.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\nYour Task: Identify the 5 counties with the highest MOE percentages.\nRequirements: - Sort by MOE percentage (highest first) - Select the top 5 counties - Display: county name, median income, margin of error, MOE percentage, reliability category - Format as a professional table using kable()\nHint: Use arrange(), slice(), and select() functions.\n\n# Create table of top 5 counties by MOE percentage\n\ntop5 &lt;- county_reliability %&gt;%\n  arrange(desc(moe_percentage)) %&gt;%\n  slice(1:5) %&gt;%\n  select(\n    county_name,\n    med_hh_incomeE,\n    med_hh_incomeM,\n    moe_percentage,\n    Reliability\n  )\n  \n# Format as table with kable() - include appropriate column names and caption\n\nkable(\n  top5,\n  caption = \"Top 5 Counties by Median Household Income MOE Percentage\",\n  col.names = c(\"County\", \"Median Income\", \"Margin of Error\", \"MOE %\", \"Reliability Category\"),\n  digits = 2,\n  align = c(\"l\", \"c\", \"c\", \"c\", \"c\"),\n  format.args = list(big.mark = \",\")\n)\n\n\nTop 5 Counties by Median Household Income MOE Percentage\n\n\n\n\n\n\n\n\n\nCounty\nMedian Income\nMargin of Error\nMOE %\nReliability Category\n\n\n\n\nMono\n82,038\n15,388\n18.76\nLow Confidence\n\n\nAlpine\n101,125\n17,442\n17.25\nLow Confidence\n\n\nSierra\n61,108\n9,237\n15.12\nLow Confidence\n\n\nTrinity\n47,317\n5,890\n12.45\nLow Confidence\n\n\nPlumas\n67,885\n7,772\n11.45\nLow Confidence\n\n\n\n\n\nData Quality Commentary:\nAll five of these counties are among the lowest-density areas in California. Because their populations are so small, the ACS relies on limited samples to generate median income estimates, which introduces greater variability. This explains the large disparities and the relatively high margins of error (11–19%). As a result, algorithms that classify or rank counties using these figures could produce erroneous outcomes if they neglect the margins of error. For example, Alpine County appears to have a median income exceeding $100,000, but its margin of error is more than $17,000 an uncertainty that is enormous relative to its ~1,000 residents. This is both a sampling size and representativeness issue, highlighting how misleading the raw point estimate can be without MOE context."
  },
  {
    "objectID": "Assignments/Assignment_1/assignment1_template.html#focus-area-selection",
    "href": "Assignments/Assignment_1/assignment1_template.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\nYour Task: Select 2-3 counties from your reliability analysis for detailed tract-level study.\nStrategy: Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\n\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\n\nselected_counties &lt;- bind_rows(\n  county_reliability %&gt;%\n    filter(Reliability == \"High Confidence\") %&gt;%\n    slice(1),\n  county_reliability %&gt;%\n    filter(Reliability == \"Moderate Confidence\") %&gt;%\n    slice(1),\n  county_reliability %&gt;%\n    filter(Reliability == \"Low Confidence\") %&gt;%\n    slice(1),\n) %&gt;%\n  select(\n    County = county_name,\n    `Median Income` = med_hh_incomeE,\n    `Margin of Error` = med_hh_incomeM,\n    `MOE %` = moe_percentage,\n    Reliability = Reliability,\n    Population = total_popE\n  )\n\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\nkable(\n  selected_counties,\n  caption = \"Largest County by Population in Each Reliability Category\",\n  align = c(\"l\", \"r\", \"r\", \"r\", \"l\", \"r\"),\n  format.args = list(big.mark = \",\")\n)\n\n\nLargest County by Population in Each Reliability Category\n\n\n\n\n\n\n\n\n\n\nCounty\nMedian Income\nMargin of Error\nMOE %\nReliability\nPopulation\n\n\n\n\nAlameda\n122,488\n1,231\n1.00\nHigh Confidence\n1,663,823\n\n\nAmador\n74,853\n6,048\n8.08\nModerate Confidence\n40,577\n\n\nAlpine\n101,125\n17,442\n17.25\nLow Confidence\n1,515\n\n\n\n\n\nComment on the output: Because I specified no randomness in how the slice is sampling the data across the reliability categories, it quite literally picked the first match it had. Which is why all three are arranged alphabetically. On the positive side we still have atleast 1 sample from each category and Alpine is still with us :)! The lowest county by density in California."
  },
  {
    "objectID": "Assignments/Assignment_1/assignment1_template.html#tract-level-demographics",
    "href": "Assignments/Assignment_1/assignment1_template.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\nYour Task: Get demographic data for census tracts in your selected counties.\nRequirements: - Geography: tract level - Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001) - Use the same state and year as before - Output format: wide - Challenge: You’ll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n# Define your race/ethnicity variables with descriptive names\n\nrace_vars &lt;- c(\n  total    = \"B03002_001\",\n  white    = \"B03002_003\",\n  black    = \"B03002_004\",\n  hispanic = \"B03002_012\"\n)\n\n\n# Use get_acs() to retrieve tract-level data\n# Hint: You may need to specify county codes in the county parameter\n\ncounty_codes &lt;- unique(str_sub(county$GEOID, 3, 5))\n\ntract_raw &lt;- get_acs(\n  geography = \"tract\",\n  state     = my_state,\n  county    = county_codes,\n  survey    = \"acs5\",\n  year      = 2022,\n  variables = race_vars,\n  output    = \"wide\"\n)\n\n\n# Calculate percentage of each group using mutate()\n# Create percentages for white, Black, and Hispanic populations\n\ntract_percent &lt;- tract_raw %&gt;%\n  mutate(\n    county_code  = substr(GEOID, 3, 5),\n    pct_white    = 100 * (whiteE    / totalE),\n    pct_black    = 100 * (blackE    / totalE),\n    pct_hispanic = 100 * (hispanicE / totalE),\n    tract_label  = str_remove(NAME, paste0(\", \", my_state)),\n    tract_label  = str_remove(tract_label, \", United States$\")\n  ) %&gt;%\n  left_join(\n    county %&gt;%\n      mutate(county_code = substr(GEOID, 3, 5)) %&gt;% #I doubled checked the counts with the census website.\n      select(county_code, county_name),\n    by = \"county_code\"\n  ) %&gt;%\n  select(\n    GEOID, county_name, tract_label,\n    totalE, whiteE, blackE, hispanicE,\n    totalM, whiteM, blackM, hispanicM,\n    pct_white, pct_black, pct_hispanic\n  )\n\n\n# Add readable tract and county name columns using str_extract() or similar\n\nkable(\n  tract_percent %&gt;% count(county_name, name = \"Number of Tracts\"),\n  caption = \"Tracts Retrieved per Selected County\",\n  align = c(\"l\", \"c\"),\n  col.names = c(\"County Names\", \"Number of Tracts\")\n)\n\n\nTracts Retrieved per Selected County\n\n\nCounty Names\nNumber of Tracts\n\n\n\n\nAlameda\n379\n\n\nAlpine\n1\n\n\nAmador\n10\n\n\nButte\n54\n\n\nCalaveras\n14\n\n\nColusa\n6\n\n\nContra Costa\n242\n\n\nDel Norte\n9\n\n\nEl Dorado\n55\n\n\nFresno\n225\n\n\nGlenn\n8\n\n\nHumboldt\n36\n\n\nImperial\n40\n\n\nInyo\n6\n\n\nKern\n236\n\n\nKings\n31\n\n\nLake\n21\n\n\nLassen\n9\n\n\nLos Angeles\n2498\n\n\nMadera\n34\n\n\nMarin\n63\n\n\nMariposa\n6\n\n\nMendocino\n24\n\n\nMerced\n63\n\n\nModoc\n4\n\n\nMono\n4\n\n\nMonterey\n104\n\n\nNapa\n40\n\n\nNevada\n26\n\n\nOrange\n614\n\n\nPlacer\n92\n\n\nPlumas\n7\n\n\nRiverside\n518\n\n\nSacramento\n363\n\n\nSan Benito\n12\n\n\nSan Bernardino\n466\n\n\nSan Diego\n737\n\n\nSan Francisco\n244\n\n\nSan Joaquin\n174\n\n\nSan Luis Obispo\n70\n\n\nSan Mateo\n174\n\n\nSanta Barbara\n109\n\n\nSanta Clara\n408\n\n\nSanta Cruz\n70\n\n\nShasta\n50\n\n\nSierra\n1\n\n\nSiskiyou\n16\n\n\nSolano\n100\n\n\nSonoma\n122\n\n\nStanislaus\n112\n\n\nSutter\n21\n\n\nTehama\n14\n\n\nTrinity\n4\n\n\nTulare\n103\n\n\nTuolumne\n18\n\n\nVentura\n190\n\n\nYolo\n53\n\n\nYuba\n19"
  },
  {
    "objectID": "Assignments/Assignment_1/assignment1_template.html#demographic-analysis",
    "href": "Assignments/Assignment_1/assignment1_template.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\nYour Task: Analyze the demographic patterns in your selected areas.\n\n# Find the tract with the highest percentage of Hispanic/Latino residents\n# Hint: use arrange() and slice() to get the top tract\n\ntop_hispanic_tract &lt;- tract_percent %&gt;%\n  arrange(desc(pct_hispanic)) %&gt;%\n  slice(1) %&gt;%\n  select(GEOID, tract_label, county_name, pct_hispanic)\n\nkable(top_hispanic_tract, caption = \"Tract with Highest % Hispanic/Latino\")\n\n\nTract with Highest % Hispanic/Latino\n\n\n\n\n\n\n\n\nGEOID\ntract_label\ncounty_name\npct_hispanic\n\n\n\n\n06065045611\nCensus Tract 456.11; Riverside County; California\nRiverside\n100\n\n\n\n\n# Calculate average demographics by county using group_by() and summarize()\n# Show: number of tracts, average percentage for each racial/ethnic group\n\ncounty_demo_avgs &lt;- tract_percent %&gt;%\n  group_by(county_name) %&gt;%\n  summarise(\n    \"Number of Tracts\" = n(),\n    \"Average White Percentage\" = mean(pct_white, na.rm = TRUE),\n    \"Average Black Percentage\" = mean(pct_black, na.rm = TRUE),\n    \"Average Hispanic Percentage\"  = mean(pct_hispanic, na.rm = TRUE)\n  )\n\n# Create a nicely formatted table of your results using kable()\n\nkable(\n  county_demo_avgs,\n  caption = \"Average Tract Demographics by County\",\n  digits = 1,\n  align = c(\"l\",\"c\",\"c\",\"c\",\"c\"),\n  col.names = c(\"County Names\", \"Number of Tracts\", \"Average White Percentage\",\"Average Black Percentage\",\"Average Hispanic Percentage\")\n)\n\n\nAverage Tract Demographics by County\n\n\n\n\n\n\n\n\n\nCounty Names\nNumber of Tracts\nAverage White Percentage\nAverage Black Percentage\nAverage Hispanic Percentage\n\n\n\n\nAlameda\n379\n31.0\n10.7\n21.4\n\n\nAlpine\n1\n58.1\n0.0\n14.1\n\n\nAmador\n10\n75.7\n1.6\n14.9\n\n\nButte\n54\n69.3\n1.5\n17.4\n\n\nCalaveras\n14\n81.0\n0.9\n11.6\n\n\nColusa\n6\n34.0\n1.6\n60.5\n\n\nContra Costa\n242\n42.6\n8.0\n25.1\n\n\nDel Norte\n9\n59.5\n2.2\n19.6\n\n\nEl Dorado\n55\n76.0\n0.6\n13.8\n\n\nFresno\n225\n28.4\n4.1\n54.1\n\n\nGlenn\n8\n54.0\n0.3\n39.2\n\n\nHumboldt\n36\n72.1\n1.3\n11.6\n\n\nImperial\n40\n11.5\n2.6\n82.2\n\n\nInyo\n6\n62.1\n0.8\n23.2\n\n\nKern\n236\n33.1\n4.7\n54.0\n\n\nKings\n31\n29.6\n5.7\n57.3\n\n\nLake\n21\n69.3\n2.5\n20.0\n\n\nLassen\n9\n70.0\n5.0\n15.4\n\n\nLos Angeles\n2498\n26.3\n7.6\n47.6\n\n\nMadera\n34\n33.7\n2.4\n58.0\n\n\nMarin\n63\n69.2\n2.5\n16.5\n\n\nMariposa\n6\n76.8\n0.9\n13.4\n\n\nMendocino\n24\n64.6\n0.5\n24.9\n\n\nMerced\n63\n25.4\n2.8\n62.1\n\n\nModoc\n4\n76.6\n1.4\n15.1\n\n\nMono\n4\n64.1\n0.2\n27.8\n\n\nMonterey\n104\n35.2\n2.0\n52.6\n\n\nNapa\n40\n54.6\n2.1\n31.7\n\n\nNevada\n26\n83.4\n0.3\n9.6\n\n\nOrange\n614\n41.3\n1.5\n32.4\n\n\nPlacer\n92\n70.9\n1.4\n14.5\n\n\nPlumas\n7\n85.2\n0.6\n8.6\n\n\nRiverside\n518\n34.6\n5.7\n49.9\n\n\nSacramento\n363\n43.2\n9.1\n23.8\n\n\nSan Benito\n12\n32.8\n0.8\n59.5\n\n\nSan Bernardino\n466\n28.5\n7.1\n53.3\n\n\nSan Diego\n737\n45.5\n4.4\n33.3\n\n\nSan Francisco\n244\n39.5\n5.1\n15.1\n\n\nSan Joaquin\n174\n29.8\n6.7\n43.6\n\n\nSan Luis Obispo\n70\n67.1\n1.3\n23.2\n\n\nSan Mateo\n174\n37.9\n2.1\n23.5\n\n\nSanta Barbara\n109\n46.0\n1.8\n43.6\n\n\nSanta Clara\n408\n29.6\n2.3\n25.3\n\n\nSanta Cruz\n70\n56.7\n0.8\n34.2\n\n\nShasta\n50\n77.7\n0.9\n10.6\n\n\nSierra\n1\n86.6\n0.2\n11.4\n\n\nSiskiyou\n16\n73.9\n1.2\n14.5\n\n\nSolano\n100\n36.1\n13.0\n28.2\n\n\nSonoma\n122\n63.6\n1.4\n25.6\n\n\nStanislaus\n112\n38.7\n2.7\n48.9\n\n\nSutter\n21\n45.8\n1.8\n32.5\n\n\nTehama\n14\n65.9\n0.9\n26.0\n\n\nTrinity\n4\n79.2\n1.7\n7.0\n\n\nTulare\n103\n27.4\n1.2\n65.3\n\n\nTuolumne\n18\n78.1\n1.8\n13.5\n\n\nVentura\n190\n45.0\n1.7\n42.1\n\n\nYolo\n53\n46.5\n2.7\n31.1\n\n\nYuba\n19\n56.0\n3.2\n26.7"
  },
  {
    "objectID": "Assignments/Assignment_1/assignment1_template.html#moe-analysis-for-demographic-variables",
    "href": "Assignments/Assignment_1/assignment1_template.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\nYour Task: Examine margins of error for demographic variables to see if some communities have less reliable data.\nRequirements: - Calculate MOE percentages for each demographic variable - Flag tracts where any demographic variable has MOE &gt; 15% - Create summary statistics\n\n# Calculate MOE percentages for white, Black, and Hispanic variables\n# Hint: use the same formula as before (margin/estimate * 100)\n\n# Create a flag for tracts with high MOE on any demographic variable\n# Use logical operators (| for OR) in an ifelse() statement\n\ntract_quality &lt;- tract_percent %&gt;%\n  mutate(\n    moe_total_pct    = 100 * (totalM    / totalE),\n    moe_white_pct    = 100 * (whiteM    / whiteE),\n    moe_black_pct    = 100 * (blackM    / blackE),\n    moe_hispanic_pct = 100 * (hispanicM / hispanicE),\n    high_moe_flag = (moe_white_pct &gt; 15) | (moe_black_pct &gt; 15) | (moe_hispanic_pct &gt; 15)\n  )\n\n# Create summary statistics showing how many tracts have data quality issues\n\ntract_quality_summary &lt;- tract_quality %&gt;%\n  summarise(\n    tracts_total = n(),\n    tracts_high_moe = sum(high_moe_flag, na.rm = TRUE),\n    percent_high_moe = round(100 * tracts_high_moe / tracts_total, 1)\n  )\n\nkable(\n  tract_quality_summary,\n  caption = \"Tract-Level High-MOE Summary (&gt;15% on any demographic variable)\",\n  col.names = c(\"Total Tracts\", \"High-MOE Tracts\", \"Percent High-MOE (%)\"),\n  align = \"c\"\n)\n\n\nTract-Level High-MOE Summary (&gt;15% on any demographic variable)\n\n\nTotal Tracts\nHigh-MOE Tracts\nPercent High-MOE (%)\n\n\n\n\n9129\n9123\n99.9"
  },
  {
    "objectID": "Assignments/Assignment_1/assignment1_template.html#pattern-analysis",
    "href": "Assignments/Assignment_1/assignment1_template.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\nYour Task: Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\n\npattern_table &lt;- tract_quality %&gt;%\n  group_by(high_moe_flag, county_name) %&gt;%\n  summarise(\n    tracts = n(),\n    avg_pop = mean(totalE, na.rm = TRUE),\n    avg_pct_white = mean(pct_white, na.rm = TRUE),\n    avg_pct_black = mean(pct_black, na.rm = TRUE),\n    avg_pct_hispanic = mean(pct_hispanic, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(desc(high_moe_flag), county_name)\n\n# Use group_by() and summarize() to create this comparison\n# Create a professional table showing the patterns\n\nkable(\n  pattern_table,\n  caption = \"Characteristics by High-MOE Status (Any Demographic Variable &gt; 15% MOE)\",\n  digits = 1,\n  format.args = list(big.mark = \",\"),\n  col.names = c(\"High MOE Flag\", \"County name\", \"Total Tracts\", \"Average Population\", \"Percentage White\", \"Percentage Black\", \"Percentage Hispanic\")\n)\n\n\nCharacteristics by High-MOE Status (Any Demographic Variable &gt; 15% MOE)\n\n\n\n\n\n\n\n\n\n\n\nHigh MOE Flag\nCounty name\nTotal Tracts\nAverage Population\nPercentage White\nPercentage Black\nPercentage Hispanic\n\n\n\n\nTRUE\nAlameda\n379\n4,390.0\n31.0\n10.7\n21.4\n\n\nTRUE\nAlpine\n1\n1,515.0\n58.1\n0.0\n14.1\n\n\nTRUE\nAmador\n10\n4,057.7\n75.7\n1.6\n14.9\n\n\nTRUE\nButte\n54\n3,955.6\n69.3\n1.5\n17.4\n\n\nTRUE\nCalaveras\n14\n3,262.4\n81.0\n0.9\n11.6\n\n\nTRUE\nColusa\n6\n3,635.2\n34.0\n1.6\n60.5\n\n\nTRUE\nContra Costa\n242\n4,804.3\n42.6\n8.0\n25.1\n\n\nTRUE\nDel Norte\n9\n3,051.3\n59.5\n2.2\n19.6\n\n\nTRUE\nEl Dorado\n55\n3,485.7\n76.0\n0.6\n13.8\n\n\nTRUE\nFresno\n225\n4,481.2\n28.4\n4.1\n54.1\n\n\nTRUE\nGlenn\n8\n3,582.1\n54.0\n0.3\n39.2\n\n\nTRUE\nHumboldt\n36\n3,781.4\n72.1\n1.3\n11.6\n\n\nTRUE\nImperial\n40\n4,489.4\n11.5\n2.6\n82.2\n\n\nTRUE\nInyo\n6\n3,138.2\n62.1\n0.8\n23.2\n\n\nTRUE\nKern\n236\n3,842.7\n33.1\n4.7\n54.0\n\n\nTRUE\nKings\n30\n4,830.1\n30.0\n5.0\n57.6\n\n\nTRUE\nLake\n21\n3,239.2\n69.3\n2.5\n20.0\n\n\nTRUE\nLassen\n8\n3,019.5\n75.0\n2.8\n12.0\n\n\nTRUE\nLos Angeles\n2,497\n3,975.8\n26.3\n7.6\n47.6\n\n\nTRUE\nMadera\n33\n4,551.5\n33.9\n2.0\n58.3\n\n\nTRUE\nMarin\n63\n4,134.7\n69.2\n2.5\n16.5\n\n\nTRUE\nMariposa\n6\n2,855.0\n76.8\n0.9\n13.4\n\n\nTRUE\nMendocino\n24\n3,797.7\n64.6\n0.5\n24.9\n\n\nTRUE\nMerced\n63\n4,480.8\n25.4\n2.8\n62.1\n\n\nTRUE\nModoc\n4\n2,162.8\n76.6\n1.4\n15.1\n\n\nTRUE\nMono\n4\n3,304.8\n64.1\n0.2\n27.8\n\n\nTRUE\nMonterey\n104\n4,207.8\n35.2\n2.0\n52.6\n\n\nTRUE\nNapa\n40\n3,434.6\n54.6\n2.1\n31.7\n\n\nTRUE\nNevada\n26\n3,935.5\n83.4\n0.3\n9.6\n\n\nTRUE\nOrange\n614\n5,171.4\n41.3\n1.5\n32.4\n\n\nTRUE\nPlacer\n92\n4,419.7\n70.9\n1.4\n14.5\n\n\nTRUE\nPlumas\n7\n2,807.1\n85.2\n0.6\n8.6\n\n\nTRUE\nRiverside\n518\n4,690.1\n34.6\n5.7\n49.9\n\n\nTRUE\nSacramento\n363\n4,350.4\n43.2\n9.1\n23.8\n\n\nTRUE\nSan Benito\n12\n5,396.1\n32.8\n0.8\n59.5\n\n\nTRUE\nSan Bernardino\n465\n4,681.6\n28.5\n7.1\n53.3\n\n\nTRUE\nSan Diego\n737\n4,463.6\n45.5\n4.4\n33.3\n\n\nTRUE\nSan Francisco\n244\n3,487.9\n39.5\n5.1\n15.1\n\n\nTRUE\nSan Joaquin\n174\n4,479.6\n29.8\n6.7\n43.6\n\n\nTRUE\nSan Luis Obispo\n70\n4,024.5\n67.1\n1.3\n23.2\n\n\nTRUE\nSan Mateo\n174\n4,334.8\n37.9\n2.1\n23.5\n\n\nTRUE\nSanta Barbara\n109\n4,084.5\n46.0\n1.8\n43.6\n\n\nTRUE\nSanta Clara\n408\n4,698.1\n29.6\n2.3\n25.3\n\n\nTRUE\nSanta Cruz\n70\n3,836.7\n56.7\n0.8\n34.2\n\n\nTRUE\nShasta\n50\n3,637.0\n77.7\n0.9\n10.6\n\n\nTRUE\nSierra\n1\n2,916.0\n86.6\n0.2\n11.4\n\n\nTRUE\nSiskiyou\n16\n2,753.1\n73.9\n1.2\n14.5\n\n\nTRUE\nSolano\n99\n4,497.2\n36.3\n12.7\n28.2\n\n\nTRUE\nSonoma\n122\n4,003.6\n63.6\n1.4\n25.6\n\n\nTRUE\nStanislaus\n112\n4,929.1\n38.7\n2.7\n48.9\n\n\nTRUE\nSutter\n21\n4,719.1\n45.8\n1.8\n32.5\n\n\nTRUE\nTehama\n14\n4,677.4\n65.9\n0.9\n26.0\n\n\nTRUE\nTrinity\n4\n3,972.2\n79.2\n1.7\n7.0\n\n\nTRUE\nTulare\n103\n4,596.6\n27.4\n1.2\n65.3\n\n\nTRUE\nTuolumne\n18\n3,055.2\n78.1\n1.8\n13.5\n\n\nTRUE\nVentura\n190\n4,431.6\n45.0\n1.7\n42.1\n\n\nTRUE\nYolo\n53\n4,097.0\n46.5\n2.7\n31.1\n\n\nTRUE\nYuba\n19\n4,300.3\n56.0\n3.2\n26.7\n\n\nFALSE\nKings\n1\n7,612.0\n18.3\n25.1\n49.9\n\n\nFALSE\nLassen\n1\n7,717.0\n30.4\n22.1\n42.4\n\n\nFALSE\nLos Angeles\n1\n8,994.0\n16.8\n33.6\n41.4\n\n\nFALSE\nMadera\n1\n7,043.0\n25.2\n14.9\n49.2\n\n\nFALSE\nSan Bernardino\n1\n3,618.0\n15.5\n25.5\n50.4\n\n\nFALSE\nSolano\n1\n5,774.0\n18.5\n43.6\n29.0\n\n\n\n\n\nPattern Analysis: Detecting patterns in MOE is not straightforward, especially given well-known challenges of minority representation in surveys like the ACS. Still, two clear dynamics emerge. First, tracts with a high-MOE flag tend to have smaller average populations than those without flags, which reflects how limited sample sizes inflate sampling error. Second, the racial/ethnic composition of flagged tracts highlights underrepresentation of minority groups. Many high-MOE tracts are White-dominant, but when examining subgroup estimates within these tracts, Black and Hispanic populations frequently have margins of error above 15%—and in some cases no sampled observations at all, producing infinite MOE values (I did not address this in the code as no instructions indicated I do so). This indicates that ACS sampling struggles to reliably capture smaller minority populations, which causes margins of error to balloon and undermines the reliability of these tracts as decision-making units."
  },
  {
    "objectID": "Assignments/Assignment_1/assignment1_template.html#analysis-integration-and-professional-summary",
    "href": "Assignments/Assignment_1/assignment1_template.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nYour Task: Write an executive summary that integrates findings from all four analyses.\nExecutive Summary Requirements: 1. Overall Pattern Identification: What are the systematic patterns across all your analyses? 2. Equity Assessment: Which communities face the greatest risk of algorithmic bias based on your findings? 3. Root Cause Analysis: What underlying factors drive both data quality issues and bias risk? 4. Strategic Recommendations: What should the Department implement to address these systematic issues?\nExecutive Summary:\nAcross county- and tract-level analyses, two systematic patterns consistently appear. First, tracts and counties with smaller populations tend to have disproportionately high margins of error, making their estimates far less stable than those from larger areas. Second, the reliability of racial and ethnic subgroup estimates varies sharply: Black and Hispanic populations are much more likely to have margins of error above 15%, and in some cases, the ACS does not capture enough observations to produce valid estimates. Together, these patterns show that measurement error is pervasive but not random it reflects structural features of both tract size and demographic composition.\nCommunities facing the greatest risk of algorithmic bias are those that are either very small and rural or racially/ethnically diverse. Rural tracts, because of small sample sizes, may be flagged as unreliable and thus deprioritized in automated systems, despite having genuine needs. At the same time, urban minority communities, particularly those with large Hispanic or Black populations, often show the highest subgroup MOEs, meaning their conditions could be systematically misclassified or underestimated. In both cases, the communities already at risk of marginalization are the same ones where the data is least reliable.\nThe drivers of these problems are structural. In rural areas, small sample sizes inflate margins of error, while in diverse urban tracts, underrepresentation of minority subgroups disrupts the accuracy of need assessments. This underrepresentation is tied to long-standing stratification in data collection, where certain groups are less visible in surveys, and to socio-spatial self-selection, where minorities concentrate in particular neighborhoods that are often harder to measure with precision. These processes produce systematic biases: the very communities whose needs are greatest — low-income, minority, and geographically marginalized — are those most likely to be misrepresented in the data.\nThe Department should treat reliability as central to its algorithmic framework. Specifically, it should (a) adjust for MOE when prioritizing communities, so noisy estimates are not misclassified as real differences; (b) avoid strict cutoffs in low-confidence areas by using broader eligibility bands; (c) supplement ACS data with administrative or community-level sources in minority-dense neighborhoods where subgroup reliability is weakest; and (d) incorporate transparency and equity audits to ensure that stratification and data gaps do not reinforce existing inequalities. By embedding these safeguards, the Department can ensure its allocation strategies are both statistically sound and socially just."
  },
  {
    "objectID": "Assignments/Assignment_1/assignment1_template.html#specific-recommendations",
    "href": "Assignments/Assignment_1/assignment1_template.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\nYour Task: Create a decision framework for algorithm implementation.\n\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\n\nrecommendations &lt;- county_reliability %&gt;%\n  select(\n    County = county_name,\n    `Median Income` = med_hh_incomeE,\n    `MOE %` = moe_percentage,\n    `Reliability Category` = Reliability\n  ) %&gt;%\n  mutate(\n    Recommendation = case_when(\n      `Reliability Category` == \"High Confidence\"     ~ \"Safe for algorithmic decisions\",\n      `Reliability Category` == \"Moderate Confidence\" ~ \"Use with caution – monitor outcomes\",\n      `Reliability Category` == \"Low Confidence\"      ~ \"Requires manual review or additional data\",\n      TRUE                                            ~ NA_character_\n    )\n  )\n\n# Add a new column with algorithm recommendations using case_when():\n# - High Confidence: \"Safe for algorithmic decisions\"\n# - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n# - Low Confidence: \"Requires manual review or additional data\"\n\n# Format as a professional table with kable()\n\nkable(\n  recommendations %&gt;%\n    arrange(Recommendation, County),\n  caption = \"Decision Framework for Algorithm Implementation (Arranged by Recommendation)\",\n  col.names = c(\"County\", \"Median Income\", \"MOE %\", \"Reliability Category\", \"Recommendation\"),\n  digits = 2,\n  format.args = list(big.mark = \",\")\n)\n\n\nDecision Framework for Algorithm Implementation (Arranged by Recommendation)\n\n\n\n\n\n\n\n\n\nCounty\nMedian Income\nMOE %\nReliability Category\nRecommendation\n\n\n\n\nAlpine\n101,125\n17.25\nLow Confidence\nRequires manual review or additional data\n\n\nMono\n82,038\n18.76\nLow Confidence\nRequires manual review or additional data\n\n\nPlumas\n67,885\n11.45\nLow Confidence\nRequires manual review or additional data\n\n\nSierra\n61,108\n15.12\nLow Confidence\nRequires manual review or additional data\n\n\nTrinity\n47,317\n12.45\nLow Confidence\nRequires manual review or additional data\n\n\nAlameda\n122,488\n1.00\nHigh Confidence\nSafe for algorithmic decisions\n\n\nButte\n66,085\n3.42\nHigh Confidence\nSafe for algorithmic decisions\n\n\nContra Costa\n120,020\n1.25\nHigh Confidence\nSafe for algorithmic decisions\n\n\nEl Dorado\n99,246\n3.36\nHigh Confidence\nSafe for algorithmic decisions\n\n\nFresno\n67,756\n1.43\nHigh Confidence\nSafe for algorithmic decisions\n\n\nHumboldt\n57,881\n3.68\nHigh Confidence\nSafe for algorithmic decisions\n\n\nImperial\n53,847\n4.11\nHigh Confidence\nSafe for algorithmic decisions\n\n\nKern\n63,883\n2.07\nHigh Confidence\nSafe for algorithmic decisions\n\n\nKings\n68,540\n3.29\nHigh Confidence\nSafe for algorithmic decisions\n\n\nLake\n56,259\n4.34\nHigh Confidence\nSafe for algorithmic decisions\n\n\nLos Angeles\n83,411\n0.53\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMadera\n73,543\n3.87\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMarin\n142,019\n2.89\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMendocino\n61,335\n3.58\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMerced\n64,772\n3.31\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMonterey\n91,043\n2.09\nHigh Confidence\nSafe for algorithmic decisions\n\n\nNapa\n105,809\n2.82\nHigh Confidence\nSafe for algorithmic decisions\n\n\nNevada\n79,395\n4.82\nHigh Confidence\nSafe for algorithmic decisions\n\n\nOrange\n109,361\n0.81\nHigh Confidence\nSafe for algorithmic decisions\n\n\nPlacer\n109,375\n1.70\nHigh Confidence\nSafe for algorithmic decisions\n\n\nRiverside\n84,505\n1.26\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSacramento\n84,010\n0.97\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Bernardino\n77,423\n1.04\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Diego\n96,974\n1.02\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Francisco\n136,689\n1.43\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Joaquin\n82,837\n1.75\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Luis Obispo\n90,158\n2.56\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Mateo\n149,907\n1.75\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSanta Barbara\n92,332\n2.05\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSanta Clara\n153,792\n1.00\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSanta Cruz\n104,409\n3.04\nHigh Confidence\nSafe for algorithmic decisions\n\n\nShasta\n68,347\n3.63\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSiskiyou\n53,898\n4.90\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSolano\n97,037\n1.78\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSonoma\n99,266\n2.00\nHigh Confidence\nSafe for algorithmic decisions\n\n\nStanislaus\n74,872\n1.83\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSutter\n72,654\n4.71\nHigh Confidence\nSafe for algorithmic decisions\n\n\nTulare\n64,474\n2.31\nHigh Confidence\nSafe for algorithmic decisions\n\n\nVentura\n102,141\n1.50\nHigh Confidence\nSafe for algorithmic decisions\n\n\nYolo\n85,097\n2.74\nHigh Confidence\nSafe for algorithmic decisions\n\n\nYuba\n66,693\n4.19\nHigh Confidence\nSafe for algorithmic decisions\n\n\nAmador\n74,853\n8.08\nModerate Confidence\nUse with caution – monitor outcomes\n\n\nCalaveras\n77,526\n5.00\nModerate Confidence\nUse with caution – monitor outcomes\n\n\nColusa\n69,619\n8.25\nModerate Confidence\nUse with caution – monitor outcomes\n\n\nDel Norte\n61,149\n7.16\nModerate Confidence\nUse with caution – monitor outcomes\n\n\nGlenn\n64,033\n6.19\nModerate Confidence\nUse with caution – monitor outcomes\n\n\nInyo\n63,417\n8.60\nModerate Confidence\nUse with caution – monitor outcomes\n\n\nLassen\n59,515\n5.97\nModerate Confidence\nUse with caution – monitor outcomes\n\n\nMariposa\n60,021\n8.82\nModerate Confidence\nUse with caution – monitor outcomes\n\n\nModoc\n54,962\n9.80\nModerate Confidence\nUse with caution – monitor outcomes\n\n\nSan Benito\n104,451\n5.23\nModerate Confidence\nUse with caution – monitor outcomes\n\n\nTehama\n59,029\n6.95\nModerate Confidence\nUse with caution – monitor outcomes\n\n\nTuolumne\n70,432\n6.66\nModerate Confidence\nUse with caution – monitor outcomes\n\n\n\n\n\nKey Recommendations:\nYour Task: Use your analysis results to provide specific guidance to the department.\n\nCounties suitable for immediate algorithmic implementation: Alameda, Butte, Contra Costa, El Dorado, Fresno, Humboldt, Imperial, Kern, Kings, Lake, Los Angeles, Madera Marin, Mendocino, Merced, Monterey, Napa, Nevada, Orange, Placer, Riverside, Sacramento, San Bernardino, San Diego, San Francisco, San Joaquin, San Luis Obispo, San Mateo, Santa Barbara, Santa Clara, Santa Cruz, Shasta,Siskiyou, Solano, Sonoma, Stainislaus, Sutter, Tulare, Ventura, Yolo, and Yuba\nCounties requiring additional oversight: Amador, Calaveras, Colusa, Del Norte, Glenn, Inyo, Lassen, Mariposa, Modoc, San Benito, Tehama, and Tuolumne\nCounties needing alternative approaches: Alpine, Mono, Plumas, Sierra, and Trinity ## Questions for Further Investigation\n\nAre high-MOE tracts clustered spatially (e.g., along rural–urban boundaries or in specific regions of the state), or do they appear evenly dispersed?\nDo MOE patterns persist across ACS releases, or do they improve over time with larger samples? A time-series comparison could reveal whether underrepresentation of minority or rural communities is a persistent structural issue, similar to how you track flood or disaster impacts across years.\nHow do MOE patterns for racial and ethnic groups vary across states? Are high MOEs for Hispanic and Black populations a uniquely California phenomenon, or do they reflect a broader national issue embedded in ACS sampling design?"
  },
  {
    "objectID": "Assignments/Assignment_1/assignment1_template.html#submission-checklist",
    "href": "Assignments/Assignment_1/assignment1_template.html#submission-checklist",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Submission Checklist",
    "text": "Submission Checklist\nBefore submitting your portfolio link on Canvas:\n\nAll code chunks run without errors\nAll “[Fill this in]” prompts have been completed\nTables are properly formatted and readable\nExecutive summary addresses all four required components\nPortfolio navigation includes this assignment\nCensus API key is properly set\nDocument renders correctly to HTML\n\nRemember: Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/your_file_name.html"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "This portfolio documents my learning journey in Public Policy Analytics (MUSA 5080).\n\n\nAdvanced spatial analysis and data science for urban planning and public policy.\n\n\n\n\nWeekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge\n\n\n\n\nMohamad Al Abbas was born in Beirut, Lebanon. He holds a Bachelor’s degree in Electrical and Computer Engineering and a Master of Arts in International Affairs. He is currently pursuing a joint PhD in Demography and Sociology, though he identifies more closely as an environmental demographer and sociologist. His research focuses on climate change and population health, particularly among children under five and pregnant women. Mohamad’s most recent work argues that using administrative borders as controls in climate–health research is problematic in low- and middle-income countries. Instead, he suggests that researchers adopt definitions based on environmental rather than political boundaries.\nView my full CV | View my Publication list\n\n\n\n\nEmail: ma96@upenn.edu\nGitHub: @MohamadAlAbbas-PhD\n\n\n\n\nMohamad is enrolled in this course to gain a stronger understanding of public policy in relation to urban spatial design, as well as to develop a more streamlined and polished pipeline for coding and research article preparation. He hopes to use this repository as a template for sharing code in publications and scholarly work."
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Advanced spatial analysis and data science for urban planning and public policy."
  },
  {
    "objectID": "index.html#portfolio-structure",
    "href": "index.html#portfolio-structure",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Weekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Mohamad Al Abbas was born in Beirut, Lebanon. He holds a Bachelor’s degree in Electrical and Computer Engineering and a Master of Arts in International Affairs. He is currently pursuing a joint PhD in Demography and Sociology, though he identifies more closely as an environmental demographer and sociologist. His research focuses on climate change and population health, particularly among children under five and pregnant women. Mohamad’s most recent work argues that using administrative borders as controls in climate–health research is problematic in low- and middle-income countries. Instead, he suggests that researchers adopt definitions based on environmental rather than political boundaries.\nView my full CV | View my Publication list"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Email: ma96@upenn.edu\nGitHub: @MohamadAlAbbas-PhD"
  },
  {
    "objectID": "index.html#why-this-course",
    "href": "index.html#why-this-course",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Mohamad is enrolled in this course to gain a stronger understanding of public policy in relation to urban spatial design, as well as to develop a more streamlined and polished pipeline for coding and research article preparation. He hopes to use this repository as a template for sharing code in publications and scholarly work."
  },
  {
    "objectID": "labs/lab_0/lab0_template.html",
    "href": "labs/lab_0/lab0_template.html",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "",
    "text": "Welcome to your first lab! In this (not graded) assignment, you’ll practice the fundamental dplyr operations I overviewed in class using car sales data. This lab will help you get comfortable with:\n\nBasic data exploration\nColumn selection and manipulation\n\nCreating new variables\nFiltering data\nGrouping and summarizing\n\nInstructions: Copy this template into your portfolio repository under a lab_0/ folder, then complete each section with your code and answers. You will write the code under the comment section in each chunk. Be sure to also copy the data folder into your lab_0 folder."
  },
  {
    "objectID": "labs/lab_0/lab0_template.html#data-structure-exploration",
    "href": "labs/lab_0/lab0_template.html#data-structure-exploration",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "1.1 Data Structure Exploration",
    "text": "1.1 Data Structure Exploration\nExplore the structure of your data and answer these questions:\n\n# Use glimpse() to see the data structure\n\n\n# Check the column names\nglimpse(car_data)\n\nRows: 50,000\nColumns: 7\n$ Manufacturer          &lt;chr&gt; \"Ford\", \"Porsche\", \"Ford\", \"Toyota\", \"VW\", \"Ford…\n$ Model                 &lt;chr&gt; \"Fiesta\", \"718 Cayman\", \"Mondeo\", \"RAV4\", \"Polo\"…\n$ `Engine size`         &lt;dbl&gt; 1.0, 4.0, 1.6, 1.8, 1.0, 1.4, 1.8, 1.4, 1.2, 2.0…\n$ `Fuel type`           &lt;chr&gt; \"Petrol\", \"Petrol\", \"Diesel\", \"Hybrid\", \"Petrol\"…\n$ `Year of manufacture` &lt;dbl&gt; 2002, 2016, 2014, 1988, 2006, 2018, 2010, 2015, …\n$ Mileage               &lt;dbl&gt; 127300, 57850, 39190, 210814, 127869, 33603, 866…\n$ Price                 &lt;dbl&gt; 3074, 49704, 24072, 1705, 4101, 29204, 14350, 30…\n\n# Look at the first few rows\nhead(car_data)\n\n# A tibble: 6 × 7\n  Manufacturer Model     `Engine size` `Fuel type` `Year of manufacture` Mileage\n  &lt;chr&gt;        &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt;\n1 Ford         Fiesta              1   Petrol                       2002  127300\n2 Porsche      718 Caym…           4   Petrol                       2016   57850\n3 Ford         Mondeo              1.6 Diesel                       2014   39190\n4 Toyota       RAV4                1.8 Hybrid                       1988  210814\n5 VW           Polo                1   Petrol                       2006  127869\n6 Ford         Focus               1.4 Petrol                       2018   33603\n# ℹ 1 more variable: Price &lt;dbl&gt;\n\n\nQuestions to answer: - How many rows and columns does the dataset have? - What types of variables do you see (numeric, character, etc.)? - Are there any column names that might cause problems? Why?\nYour answers: - Rows: 50,000 - Columns: 7\n- Variable types: 4x doubles, 3xchr types - Problematic names: Engine size, Fuel type, Year of manufacture"
  },
  {
    "objectID": "labs/lab_0/lab0_template.html#tibble-vs-data-frame",
    "href": "labs/lab_0/lab0_template.html#tibble-vs-data-frame",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "1.2 Tibble vs Data Frame",
    "text": "1.2 Tibble vs Data Frame\nCompare how tibbles and data frames display:\n\n# Look at the tibble version (what we have)\ncar_data\n\n# A tibble: 50,000 × 7\n   Manufacturer Model    `Engine size` `Fuel type` `Year of manufacture` Mileage\n   &lt;chr&gt;        &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt;\n 1 Ford         Fiesta             1   Petrol                       2002  127300\n 2 Porsche      718 Cay…           4   Petrol                       2016   57850\n 3 Ford         Mondeo             1.6 Diesel                       2014   39190\n 4 Toyota       RAV4               1.8 Hybrid                       1988  210814\n 5 VW           Polo               1   Petrol                       2006  127869\n 6 Ford         Focus              1.4 Petrol                       2018   33603\n 7 Ford         Mondeo             1.8 Diesel                       2010   86686\n 8 Toyota       Prius              1.4 Hybrid                       2015   30663\n 9 VW           Polo               1.2 Petrol                       2012   73470\n10 Ford         Focus              2   Diesel                       1992  262514\n# ℹ 49,990 more rows\n# ℹ 1 more variable: Price &lt;dbl&gt;\n\n# Convert to regular data frame and display\n# car_df &lt;- as.data.frame(car_data)\n# car_df\n\nQuestion: What differences do you notice in how they print?\nYour answer: data frame will render every single row within the dataset."
  },
  {
    "objectID": "labs/lab_0/lab0_template.html#selecting-columns",
    "href": "labs/lab_0/lab0_template.html#selecting-columns",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "2.1 Selecting Columns",
    "text": "2.1 Selecting Columns\nPractice selecting different combinations of columns:\n\n# Select just Model and Mileage columns\n\nModel_Mileage &lt;- select(car_data, Model, Mileage)\n\n# Select Manufacturer, Price, and Fuel type\n\nManu_price_Fuel &lt;- select(car_data,`Year of manufacture`, Price, `Fuel type`)\n\n# Challenge: Select all columns EXCEPT Engine Size\n\nno_engine &lt;- select(car_data, -`Engine size`)"
  },
  {
    "objectID": "labs/lab_0/lab0_template.html#renaming-columns",
    "href": "labs/lab_0/lab0_template.html#renaming-columns",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "2.2 Renaming Columns",
    "text": "2.2 Renaming Columns\nLet’s fix a problematic column name:\n\n# Rename 'Year of manufacture' to year\n\ncar_data &lt;- rename(car_data, year = `Year of manufacture`)\n\n\n# Check that it worked\nnames(car_data)\n\n[1] \"Manufacturer\" \"Model\"        \"Engine size\"  \"Fuel type\"    \"year\"        \n[6] \"Mileage\"      \"Price\"       \n\n\nQuestion: Why did we need backticks around Year of manufacture but not around year?\nYour answer: spaces"
  },
  {
    "objectID": "labs/lab_0/lab0_template.html#calculate-car-age",
    "href": "labs/lab_0/lab0_template.html#calculate-car-age",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "3.1 Calculate Car Age",
    "text": "3.1 Calculate Car Age\n\n# Create an 'age' column (2025 minus year of manufacture)\n\ncar_data &lt;- car_data %&gt;%\n  mutate(\n    age = 2025 - year,   # compute age\n  )\n\n# Create a mileage_per_year column\ncar_data &lt;- car_data %&gt;%\n  mutate(\n    mileage_per_year = Mileage / age       # compute mileage per year\n  )\n\n\n# Look at your new columns\nselect(car_data, Model, year, age, Mileage, mileage_per_year)\n\n# A tibble: 50,000 × 5\n   Model       year   age Mileage mileage_per_year\n   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;            &lt;dbl&gt;\n 1 Fiesta      2002    23  127300            5535.\n 2 718 Cayman  2016     9   57850            6428.\n 3 Mondeo      2014    11   39190            3563.\n 4 RAV4        1988    37  210814            5698.\n 5 Polo        2006    19  127869            6730.\n 6 Focus       2018     7   33603            4800.\n 7 Mondeo      2010    15   86686            5779.\n 8 Prius       2015    10   30663            3066.\n 9 Polo        2012    13   73470            5652.\n10 Focus       1992    33  262514            7955.\n# ℹ 49,990 more rows"
  },
  {
    "objectID": "labs/lab_0/lab0_template.html#categorize-cars",
    "href": "labs/lab_0/lab0_template.html#categorize-cars",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "3.2 Categorize Cars",
    "text": "3.2 Categorize Cars\n\n# Create a price_category column where if price is &lt; 15000, its is coded as budget, between 15000 and 30000 is midrange and greater than 30000 is mid-range (use case_when)\n\ncar_data &lt;- car_data %&gt;%\n  mutate(\n    price_category = case_when(\n      Price &lt; 15000 ~ \"budget\",\n      Price &gt;= 15000 & Price &lt;= 30000 ~ \"midrange\",\n      Price &gt; 30000 ~ \"luxury\"\n    )\n  )\n\n# Check your categories select the new column and show it\n\nselect(car_data, Model, Price, price_category)\n\n# A tibble: 50,000 × 3\n   Model      Price price_category\n   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;         \n 1 Fiesta      3074 budget        \n 2 718 Cayman 49704 luxury        \n 3 Mondeo     24072 midrange      \n 4 RAV4        1705 budget        \n 5 Polo        4101 budget        \n 6 Focus      29204 midrange      \n 7 Mondeo     14350 budget        \n 8 Prius      30297 luxury        \n 9 Polo        9977 budget        \n10 Focus       1049 budget        \n# ℹ 49,990 more rows"
  },
  {
    "objectID": "labs/lab_0/lab0_template.html#basic-filtering",
    "href": "labs/lab_0/lab0_template.html#basic-filtering",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "4.1 Basic Filtering",
    "text": "4.1 Basic Filtering\n\n# Find all Toyota cars\n\ntoyota_cars &lt;- car_data %&gt;%\n  filter(Manufacturer == \"Toyota\")\n\n# Find cars with mileage less than 30,000\n\nlow_mileage &lt;- car_data %&gt;%\n  filter(Mileage &lt; 30000)\n\n# Find luxury cars (from price category) with low mileage\n\nluxury_cars &lt;- car_data %&gt;%\n  filter(price_category == \"luxury\")\n\ntoyota_cars\n\n# A tibble: 12,554 × 10\n   Manufacturer Model `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Toyota       RAV4            1.8 Hybrid       1988  210814  1705    37\n 2 Toyota       Prius           1.4 Hybrid       2015   30663 30297    10\n 3 Toyota       RAV4            2.2 Petrol       2007   79393 16026    18\n 4 Toyota       Yaris           1.4 Petrol       1998   97286  4046    27\n 5 Toyota       RAV4            2.4 Hybrid       2003  117425 11667    22\n 6 Toyota       Yaris           1.2 Petrol       1992  245990   720    33\n 7 Toyota       RAV4            2   Hybrid       2018   28381 52671     7\n 8 Toyota       Prius           1   Hybrid       2003  115291  6512    22\n 9 Toyota       Prius           1   Hybrid       1990  238571   961    35\n10 Toyota       Prius           1.8 Hybrid       2017   31958 38961     8\n# ℹ 12,544 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\nlow_mileage\n\n# A tibble: 5,402 × 10\n   Manufacturer Model      `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Toyota       RAV4                 2   Hybrid       2018   28381 52671     7\n 2 VW           Golf                 2   Petrol       2020   18985 36387     5\n 3 BMW          M5                   4   Petrol       2017   22759 97758     8\n 4 Toyota       RAV4                 2.4 Petrol       2018   24588 49125     7\n 5 VW           Golf                 2   Hybrid       2018   25017 36957     7\n 6 Porsche      718 Cayman           2.4 Petrol       2021   14070 69526     4\n 7 Ford         Focus                1.8 Petrol       2020   22371 40336     5\n 8 Ford         Mondeo               1.6 Diesel       2015   21834 28435    10\n 9 VW           Passat               1.6 Diesel       2018   22122 36634     7\n10 VW           Passat               1.4 Diesel       2020   21413 39310     5\n# ℹ 5,392 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\nluxury_cars\n\n# A tibble: 6,178 × 10\n   Manufacturer Model      `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Porsche      718 Cayman           4   Petrol       2016   57850 49704     9\n 2 Toyota       Prius                1.4 Hybrid       2015   30663 30297    10\n 3 Toyota       RAV4                 2   Hybrid       2018   28381 52671     7\n 4 Porsche      911                  2.6 Petrol       2009   66273 41963    16\n 5 Toyota       Prius                1.8 Hybrid       2017   31958 38961     8\n 6 VW           Golf                 2   Petrol       2020   18985 36387     5\n 7 BMW          M5                   4   Petrol       2017   22759 97758     8\n 8 Toyota       RAV4                 2.4 Petrol       2018   24588 49125     7\n 9 Porsche      Cayenne              2.6 Diesel       2015   33693 54037    10\n10 VW           Golf                 2   Hybrid       2018   25017 36957     7\n# ℹ 6,168 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;"
  },
  {
    "objectID": "labs/lab_0/lab0_template.html#multiple-conditions",
    "href": "labs/lab_0/lab0_template.html#multiple-conditions",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "4.2 Multiple Conditions",
    "text": "4.2 Multiple Conditions\n\n# Find cars that are EITHER Honda OR Nissan\n\nhonda_nissan &lt;- car_data %&gt;%\n  filter(Manufacturer == \"Honda\" | Manufacturer == \"Nissan\")\n\n# Find cars with price between $20,000 and $35,000\n\nprice_range &lt;- car_data %&gt;%\n  filter(Price &gt;= 20000, Price &lt;= 35000)\n\n# Find diesel cars less than 10 years old\n\ndiesel_recent &lt;- car_data %&gt;%\n  filter(`Fuel type` == \"Diesel\", age &lt; 10)\n\nhonda_nissan\n\n# A tibble: 0 × 10\n# ℹ 10 variables: Manufacturer &lt;chr&gt;, Model &lt;chr&gt;, Engine size &lt;dbl&gt;,\n#   Fuel type &lt;chr&gt;, year &lt;dbl&gt;, Mileage &lt;dbl&gt;, Price &lt;dbl&gt;, age &lt;dbl&gt;,\n#   mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\nprice_range\n\n# A tibble: 7,301 × 10\n   Manufacturer Model  `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Ford         Mondeo           1.6 Diesel       2014   39190 24072    11\n 2 Ford         Focus            1.4 Petrol       2018   33603 29204     7\n 3 Toyota       Prius            1.4 Hybrid       2015   30663 30297    10\n 4 Toyota       Prius            1.4 Hybrid       2016   43893 29946     9\n 5 Toyota       Prius            1.4 Hybrid       2016   43130 30085     9\n 6 VW           Passat           1.6 Petrol       2016   64344 23641     9\n 7 Ford         Mondeo           1.6 Diesel       2015   21834 28435    10\n 8 BMW          M5               4.4 Petrol       2008  109941 31711    17\n 9 BMW          Z4               2.2 Petrol       2014   61332 26084    11\n10 Porsche      911              3.5 Petrol       2003  107705 24378    22\n# ℹ 7,291 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\ndiesel_recent\n\n# A tibble: 2,040 × 10\n   Manufacturer Model   `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Ford         Fiesta            1   Diesel       2017   38370 16257     8\n 2 VW           Passat            1.6 Diesel       2018   22122 36634     7\n 3 VW           Passat            1.4 Diesel       2020   21413 39310     5\n 4 BMW          X3                2   Diesel       2018   27389 44018     7\n 5 Ford         Mondeo            2   Diesel       2016   51724 28482     9\n 6 Porsche      Cayenne           2.6 Diesel       2019   20147 76182     6\n 7 VW           Polo              1.2 Diesel       2018   37411 19649     7\n 8 Ford         Mondeo            1.8 Diesel       2016   29439 30886     9\n 9 Ford         Mondeo            1.4 Diesel       2020   18929 37720     5\n10 Ford         Mondeo            1.4 Diesel       2018   42017 28904     7\n# ℹ 2,030 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n\nQuestion: How many diesel cars are less than 10 years old?\nYour answer: 2,040"
  },
  {
    "objectID": "labs/lab_0/lab0_template.html#basic-summaries",
    "href": "labs/lab_0/lab0_template.html#basic-summaries",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "5.1 Basic Summaries",
    "text": "5.1 Basic Summaries\n\n# Calculate average price by manufacturer\navg_price_by_brand &lt;- car_data %&gt;%\n  group_by(Manufacturer) %&gt;%\n  summarize(avg_price = mean(Price, na.rm = TRUE))\n\navg_price_by_brand\n\n# A tibble: 5 × 2\n  Manufacturer avg_price\n  &lt;chr&gt;            &lt;dbl&gt;\n1 BMW             24429.\n2 Ford            10672.\n3 Porsche         29104.\n4 Toyota          14340.\n5 VW              10363.\n\n# Calculate average mileage by fuel type\n\navg_price_by_brand &lt;- car_data %&gt;%\n  group_by(`Fuel type`) %&gt;%\n  summarize(avg_mileage = mean(Mileage, na.rm = TRUE))\n\navg_price_by_brand\n\n# A tibble: 3 × 2\n  `Fuel type` avg_mileage\n  &lt;chr&gt;             &lt;dbl&gt;\n1 Diesel          112667.\n2 Hybrid          111622.\n3 Petrol          112795.\n\n# Count cars by manufacturer\n\ncar_counts &lt;- car_data %&gt;%\n  count(Manufacturer)\n\ncar_counts\n\n# A tibble: 5 × 2\n  Manufacturer     n\n  &lt;chr&gt;        &lt;int&gt;\n1 BMW           4965\n2 Ford         14959\n3 Porsche       2609\n4 Toyota       12554\n5 VW           14913"
  },
  {
    "objectID": "labs/lab_0/lab0_template.html#categorical-summaries",
    "href": "labs/lab_0/lab0_template.html#categorical-summaries",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "5.2 Categorical Summaries",
    "text": "5.2 Categorical Summaries\n\n# Frequency table for price categories\n\nfreq_cat &lt;- car_data %&gt;%\n  count(price_category, name = \"frequency\") %&gt;%\n  mutate(proportion = frequency / sum(frequency))\n\nfreq_cat\n\n# A tibble: 3 × 3\n  price_category frequency proportion\n  &lt;chr&gt;              &lt;int&gt;      &lt;dbl&gt;\n1 budget             34040      0.681\n2 luxury              6178      0.124\n3 midrange            9782      0.196"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#what-well-cover",
    "href": "labs/week-03/lecture/week3.html#what-well-cover",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "What We’ll Cover",
    "text": "What We’ll Cover\nPart 1: Why Visualization Matters\n\nAnscombe’s Quartet and the limits of summary statistics\nVisualization in policy context\nConnection to algorithmic bias and data ethics\n\nPart 2: Grammar of Graphics\n\nggplot2 fundamentals\nAesthetic mappings and geoms\nLive demonstration\n\nPart 3: Exploratory Data Analysis\n\nEDA workflow and principles\nUnderstanding distributions and relationships\nCritical focus: Data quality and uncertainty\n\nPart 4: Data Joins & Integration\n\nCombining datasets with dplyr joins\n\nPart 5: Hands-On Lab\n\nGuided practice with census data\nCreate publication-ready visualizations\nPractice ethical data communication"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#opening-question",
    "href": "labs/week-03/lecture/week3.html#opening-question",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Opening Question",
    "text": "Opening Question\nThink about Assignment 1:\nYou created tables showing income reliability patterns across counties. But what if you needed to present these findings to:\n\nThe state legislature (2-minute briefing)\nCommunity advocacy groups\nLocal news reporters\n\nDiscussion: How might visual presentation change the impact of your analysis?"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#anscombes-quartet-the-famous-example",
    "href": "labs/week-03/lecture/week3.html#anscombes-quartet-the-famous-example",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Anscombe’s Quartet: The Famous Example",
    "text": "Anscombe’s Quartet: The Famous Example\nFour datasets with identical summary statistics:\n\nSame means (x̄ = 9, ȳ = 7.5)\nSame variances\nSame correlation (r = 0.816)\nSame regression line\n\nBut completely different patterns when visualized"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#the-policy-implications",
    "href": "labs/week-03/lecture/week3.html#the-policy-implications",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "The Policy Implications",
    "text": "The Policy Implications\nWhy this matters for your work:\n\nSummary statistics can hide critical patterns\nOutliers may represent important communities\nRelationships aren’t always linear\nVisual inspection reveals data quality issues\n\nExample: A county with “average” income might have extreme inequality that algorithms would miss without visualization."
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#connecting-week-2-ethical-data-communication",
    "href": "labs/week-03/lecture/week3.html#connecting-week-2-ethical-data-communication",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Connecting Week 2: Ethical Data Communication",
    "text": "Connecting Week 2: Ethical Data Communication\nFrom last week’s algorithmic bias discussion:\nResearch finding: Only 27% of planners warn users about unreliable ACS data - Most planners don’t report margins of error - Many lack training on statistical uncertainty - This violates AICP Code of Ethics\nYour responsibility:\n\nCreate honest, transparent visualizations\nAlways assess and communicate data quality\nConsider who might be harmed by uncertain data"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#bad-visualizations-have-real-consequences",
    "href": "labs/week-03/lecture/week3.html#bad-visualizations-have-real-consequences",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Bad Visualizations Have Real Consequences",
    "text": "Bad Visualizations Have Real Consequences\nCommon problems in government data presentation:\n\nMisleading scales or axes\nCherry-picked time periods\n\nHidden or ignored uncertainty\nMissing context about data reliability\n\nReal impact: The Jurjevich et al. study found that 72% of Portland census tracts had unreliable child poverty estimates, yet planners rarely communicated this uncertainty.\nResult: Poor policy decisions based on misunderstood data"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#the-ggplot2-philosophy",
    "href": "labs/week-03/lecture/week3.html#the-ggplot2-philosophy",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "The ggplot2 Philosophy",
    "text": "The ggplot2 Philosophy\nGrammar of Graphics principles:\nData → Aesthetics → Geometries → Visual\n\nData: Your dataset (census data, survey responses, etc.)\nAesthetics: What variables map to visual properties (x, y, color, size)\nGeometries: How to display the data (points, bars, lines)\nAdditional layers: Scales, themes, facets, annotations"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#basic-ggplot2-structure",
    "href": "labs/week-03/lecture/week3.html#basic-ggplot2-structure",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Basic ggplot2 Structure",
    "text": "Basic ggplot2 Structure\nEvery ggplot has this pattern:\nggplot(data = your_data) +   aes(x = variable1, y = variable2) +   geom_something() +   additional_layers()\nYou build plots by adding layers with +"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#live-demo-basic-scatter-plot",
    "href": "labs/week-03/lecture/week3.html#live-demo-basic-scatter-plot",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Live Demo: Basic Scatter Plot",
    "text": "Live Demo: Basic Scatter Plot"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#aesthetic-mappings-the-key-to-ggplot2",
    "href": "labs/week-03/lecture/week3.html#aesthetic-mappings-the-key-to-ggplot2",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Aesthetic Mappings: The Key to ggplot2",
    "text": "Aesthetic Mappings: The Key to ggplot2\nAesthetics map data to visual properties:\n\nx, y - position\ncolor - point/line color\nfill - area fill color\n\nsize - point/line size\nshape - point shape\nalpha - transparency\n\nImportant: Aesthetics go inside aes(), constants go outside"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#improving-plots-with-labels-and-themes",
    "href": "labs/week-03/lecture/week3.html#improving-plots-with-labels-and-themes",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Improving Plots with Labels and Themes",
    "text": "Improving Plots with Labels and Themes"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#the-eda-mindset",
    "href": "labs/week-03/lecture/week3.html#the-eda-mindset",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "The EDA Mindset",
    "text": "The EDA Mindset\nExploratory Data Analysis is detective work:\n\nWhat does the data look like? (distributions, missing values)\nWhat patterns exist? (relationships, clusters, trends)\n\nWhat’s unusual? (outliers, anomalies, data quality issues)\nWhat questions does this raise? (hypotheses for further investigation)\nHow reliable is this data?\n\nGoal: Understand your data before making decisions or building models"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#eda-workflow-with-data-quality-focus",
    "href": "labs/week-03/lecture/week3.html#eda-workflow-with-data-quality-focus",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "EDA Workflow with Data Quality Focus",
    "text": "EDA Workflow with Data Quality Focus\nEnhanced process for policy analysis:\n\nLoad and inspect - dimensions, variable types, missing data\nAssess reliability - examine margins of error, calculate coefficients of variation\nVisualize distributions - histograms, boxplots for each variable\nExplore relationships - scatter plots, correlations\nIdentify patterns - grouping, clustering, geographical patterns\nQuestion anomalies - investigate outliers and unusual patterns\nDocument limitations - prepare honest communication about data quality"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#understanding-distributions",
    "href": "labs/week-03/lecture/week3.html#understanding-distributions",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Understanding Distributions",
    "text": "Understanding Distributions\nWhy distribution shape matters:\n\nWhat to look for: Skewness, outliers, multiple peaks, gaps"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#boxplots",
    "href": "labs/week-03/lecture/week3.html#boxplots",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Boxplots!",
    "text": "Boxplots!"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#critical-data-quality-through-visualization",
    "href": "labs/week-03/lecture/week3.html#critical-data-quality-through-visualization",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Critical: Data Quality Through Visualization",
    "text": "Critical: Data Quality Through Visualization\nResearch insight: Most planners don’t visualize or communicate uncertainty\n\nPattern: Smaller populations have higher uncertainty Ethical implication: These communities might be systematically undercounted"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#research-based-recommendations-for-planners",
    "href": "labs/week-03/lecture/week3.html#research-based-recommendations-for-planners",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Research-Based Recommendations for Planners",
    "text": "Research-Based Recommendations for Planners\nJurjevich et al. (2018): 5 Essential Guidelines for Using ACS Data\n\nReport the corresponding MOEs of ACS estimates - Always include margin of error values\nInclude a footnote when not reporting MOEs - Explicitly acknowledge omission\n\nProvide context for (un)reliability - Use coefficient of variation (CV):\n\nCV &lt; 12% = reliable (green coding)\nCV 12-40% = somewhat reliable (yellow)\nCV &gt; 40% = unreliable (red coding)\n\nReduce statistical uncertainty - Collapse data detail, aggregate geographies, use multi-year estimates\nAlways conduct statistical significance tests when comparing ACS estimates over time\n\nKey insight: These practices are not just technical best practices—they are ethical requirements under the AICP Code of Ethics"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#eda-for-policy-analysis",
    "href": "labs/week-03/lecture/week3.html#eda-for-policy-analysis",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "EDA for Policy Analysis",
    "text": "EDA for Policy Analysis\nKey questions for census data:\n\nGeographic patterns: Are problems concentrated in certain areas?\nPopulation relationships: How does size affect data quality?\nDemographic patterns: Are certain communities systematically different?\nTemporal trends: How do patterns change over time?\nData integrity: Where might survey bias affect results?\nReliability assessment: Which estimates should we trust?"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#why-join-data",
    "href": "labs/week-03/lecture/week3.html#why-join-data",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Why Join Data?",
    "text": "Why Join Data?\nTo combining datasets of course:\n\nCensus demographics + Economic indicators\nSurvey responses + Geographic boundaries\n\nCurrent data + Historical trends\nAdministrative records + Survey data"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#types-of-joins-tabular",
    "href": "labs/week-03/lecture/week3.html#types-of-joins-tabular",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Types of Joins (tabular)",
    "text": "Types of Joins (tabular)\nFour main types in dplyr:\n\nleft_join() - Keep all rows from left dataset\nright_join() - Keep all rows from right dataset\n\ninner_join() - Keep only rows that match in both\nfull_join() - Keep all rows from both datasets\n\nMost common: left_join() to add columns to your main dataset"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#live-demo-joining-census-tables",
    "href": "labs/week-03/lecture/week3.html#live-demo-joining-census-tables",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Live Demo: Joining Census Tables",
    "text": "Live Demo: Joining Census Tables\n\n\n# A tibble: 6 × 6\n  GEOID NAME                    median_income income_moe college_pop college_moe\n  &lt;chr&gt; &lt;chr&gt;                           &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1 42001 Adams County, Pennsylv…         78975       3334       10195         761\n2 42003 Allegheny County, Penn…         72537        869      229538        3311\n3 42005 Armstrong County, Penn…         61011       2202        6171         438\n4 42007 Beaver County, Pennsyl…         67194       1531       22588        1012\n5 42009 Bedford County, Pennsy…         58337       2606        3396         307\n6 42011 Berks County, Pennsylv…         74617       1191       50120        1654"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#checking-join-results-and-data-quality",
    "href": "labs/week-03/lecture/week3.html#checking-join-results-and-data-quality",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Checking Join Results and Data Quality",
    "text": "Checking Join Results and Data Quality\nAlways verify joins AND assess combined reliability:\n\n\nIncome data rows: 67 \n\n\nEducation data rows: 67 \n\n\nCombined data rows: 67 \n\n\n# A tibble: 1 × 2\n  missing_income missing_education\n           &lt;int&gt;             &lt;int&gt;\n1              0                 0\n\n\n# A tibble: 6 × 3\n  NAME                           income_cv college_cv\n  &lt;chr&gt;                              &lt;dbl&gt;      &lt;dbl&gt;\n1 Adams County, Pennsylvania          4.22       7.46\n2 Allegheny County, Pennsylvania      1.20       1.44\n3 Armstrong County, Pennsylvania      3.61       7.10\n4 Beaver County, Pennsylvania         2.28       4.48\n5 Bedford County, Pennsylvania        4.47       9.04\n6 Berks County, Pennsylvania          1.60       3.30"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#lab-structure-for-today",
    "href": "labs/week-03/lecture/week3.html#lab-structure-for-today",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Lab Structure for Today",
    "text": "Lab Structure for Today\nYou’ll work through six exercises:\n\nFinding Census Variables - Learn to search for the data you need\nSingle Variable EDA - Explore distributions and identify outliers\nTwo Variable Relationships - Create meaningful scatter plots\nData Quality Visualization - Practice ethical uncertainty communication\nMultiple Variables - Color, faceting, and complex relationships\nData Integration - Join datasets and create publication-ready visualizations"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#skills-youll-practice",
    "href": "labs/week-03/lecture/week3.html#skills-youll-practice",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Skills You’ll Practice",
    "text": "Skills You’ll Practice\nggplot2 fundamentals:\n\nScatter plots, histograms, boxplots\nAesthetic mappings and customization\nProfessional themes and labels\n\nEDA workflow:\n\nDistribution analysis\nOutlier detection\n\nPattern identification\n\nEthical data practice:\n\nVisualizing and reporting margins of error\nUsing coefficient of variation to assess reliability"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#connection-to-professional-ethics",
    "href": "labs/week-03/lecture/week3.html#connection-to-professional-ethics",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Connection to Professional Ethics",
    "text": "Connection to Professional Ethics\nBy the end of today, you’ll be able to:\n\nVisually assess data quality issues\nCreate compelling presentations of demographic patterns\nCommunicate statistical uncertainty ethically and clearly\nIntegrate multiple data sources"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#questions-before-we-begin",
    "href": "labs/week-03/lecture/week3.html#questions-before-we-begin",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Questions Before We Begin?",
    "text": "Questions Before We Begin?\nReady for hands-on practice?\nRemember: Today’s skills build directly on Week 1-2 foundations:\n\nSame dplyr functions, now with visualization\nSame census data concepts, now with multiple tables\n\nLet’s create some beautiful graphs"
  },
  {
    "objectID": "Publications.html",
    "href": "Publications.html",
    "title": "Mohamad Al Abbas - MUSA 5080 Portfolio",
    "section": "",
    "text": "Born in a Haze: Early-life exposure to forest fires and child health in Indonesia\n\nExamines impacts of forest fire–related haze on children’s HAZ/WAZ growth indicators using IFLS data.\n\nDevelops clustering algorithms and k-means quintile regression for fire exposure.\n\nCrude Consequences: The Impact of Oil Spills on Child Health and Environmental Vulnerability in Nigeria\n\nIntegrates MICS data with daily oil spill trackers.\n\nExplores stratified vulnerability and child health outcomes.\n\nClimate–Health across Biomes: Stratified impacts of precipitation and temperature on children’s health\n\nMulti-country analysis of diarrhea, fever, cough, and WHZ outcomes.\n\nExamines biome interactions with climate anomalies.\n\nCross-Border Nutritional Vulnerability in Shared Ecologies (India, Bangladesh, Nepal)\n\nDHS-based spatial analysis of child undernutrition near borders.\n\nTests whether ecological context outweighs political boundaries.\n\nCRAM (Climate-Resilient Access Metric): Healthcare isolation in vulnerable regions\n\nDeveloping an index to measure resilience of healthcare access to climate shocks.\n\nFocus on flood seasonality and maternal health risks.\n\nLong-Term PM2.5 Exposure and Cardiovascular Disease in Indonesia\n\nCohort-based study using IFLS (2000–2014).\n\nModels morbidity-adjusted CVD outcomes under cumulative pollution exposure.\n\nDisaster–Health Mediation: Natural disasters as instruments for water contamination\n\nUses IV regression with MICS surveys to assess diarrheal outcomes.\n\nExplores mediation via healthcare-seeking behaviors and infrastructure."
  },
  {
    "objectID": "Publications.html#ongoing-research-projects",
    "href": "Publications.html#ongoing-research-projects",
    "title": "Mohamad Al Abbas - MUSA 5080 Portfolio",
    "section": "",
    "text": "Born in a Haze: Early-life exposure to forest fires and child health in Indonesia\n\nExamines impacts of forest fire–related haze on children’s HAZ/WAZ growth indicators using IFLS data.\n\nDevelops clustering algorithms and k-means quintile regression for fire exposure.\n\nCrude Consequences: The Impact of Oil Spills on Child Health and Environmental Vulnerability in Nigeria\n\nIntegrates MICS data with daily oil spill trackers.\n\nExplores stratified vulnerability and child health outcomes.\n\nClimate–Health across Biomes: Stratified impacts of precipitation and temperature on children’s health\n\nMulti-country analysis of diarrhea, fever, cough, and WHZ outcomes.\n\nExamines biome interactions with climate anomalies.\n\nCross-Border Nutritional Vulnerability in Shared Ecologies (India, Bangladesh, Nepal)\n\nDHS-based spatial analysis of child undernutrition near borders.\n\nTests whether ecological context outweighs political boundaries.\n\nCRAM (Climate-Resilient Access Metric): Healthcare isolation in vulnerable regions\n\nDeveloping an index to measure resilience of healthcare access to climate shocks.\n\nFocus on flood seasonality and maternal health risks.\n\nLong-Term PM2.5 Exposure and Cardiovascular Disease in Indonesia\n\nCohort-based study using IFLS (2000–2014).\n\nModels morbidity-adjusted CVD outcomes under cumulative pollution exposure.\n\nDisaster–Health Mediation: Natural disasters as instruments for water contamination\n\nUses IV regression with MICS surveys to assess diarrheal outcomes.\n\nExplores mediation via healthcare-seeking behaviors and infrastructure."
  },
  {
    "objectID": "Publications.html#in-progress-methodological-work",
    "href": "Publications.html#in-progress-methodological-work",
    "title": "Mohamad Al Abbas - MUSA 5080 Portfolio",
    "section": "In Progress Methodological Work",
    "text": "In Progress Methodological Work\n\nMultistate Life Table Models\n\nTransition modeling between Healthy–Unhealthy–Dead states in IFLS morbidity data.\n\nReplication of Payne et al. (2013) with microsimulation extensions.\n\nSpatial Climate–Health Pipelines\n\nDynamic extraction of precipitation/temperature anomalies via CHIRPS/NOAA rasters.\n\nIntegrating z-scores and climate windows aligned with DHS/MICS survey interviews."
  },
  {
    "objectID": "Publications.html#publications",
    "href": "Publications.html#publications",
    "title": "Mohamad Al Abbas - MUSA 5080 Portfolio",
    "section": "Publications",
    "text": "Publications\n\nDisparate social impacts by satellite and self-report: Floods in Pernambuco, Brazil — Population & Environment (under review)\n\nRacism, Discrimination, and Scapegoating in the Era of COVID-19 — IMS Policy and Working Paper Series\nEnvironmental Determinants of Migration: Air Pollution in Developing Nations — IMS Blog – Borders and Limitations (2022)\n\nGender Inequity in Engineering Higher Education: A Case Study of an American University in a Middle Eastern Country — ICIET Conference Proceedings (2022)\n\nThe Impact of Faculty Scholarly Collaborations with Non-Doctoral Students — IEEE IRTM Conference (2022)\n\nThe Lebanese Diaspora: From Passive to Retaliatory — IMS Blog – Borders and Limitations (2022)\n\nQuantifying Publisher’s Competence Through Scholarly Engagement — Publishing Research Quarterly (2021)\n\nA Case Study of Gender Diversity in a Middle Eastern University — IEEE IT-Based Higher Education & Training Conference (2021)\n\nSetting the Boundaries of COVID-19 Lockdown Relaxation Measures — Library Hi Tech (2021)\n\nSustainable Gender Equity Influenced by an Americanized Lebanese Pedagogy — World Conference on Women’s Studies (2021)\n\nToward an Improvement of Engineering Teaming Skills Through an In-House Professionalism Course — IEEE Transactions on Education (2020)\n\nFormation of Policies Guided by Multivariable Control Theory — Operations Research Perspectives (2020)\n\nThe Impact of Collaborative Research: A Case Study in a Developing Country — IEEE SMIT Conference (2020)\n\nOn the Impact of Multi-Authorship Scholarly Publications — IEEE ATMR Conference (2020)"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html",
    "href": "weekly-notes/week-02-notes.html",
    "title": "Week 2 Notes - Algorithmic Decision Making & Census Data",
    "section": "",
    "text": "Algorithms are a set of rules or instructions for solving a problem and completing a task.\nAlgorithms in a government are typically systems used to assist or replace human decision-makers:\n\nBased on predictions from models trained on historical data.\n\nUsing inputs (features or predictors) and outputs (outcome, dependent variable).\n\n\nLong history of government data collection:\n\nCensus data\n\nCivil registration system\n\nAdministrative records\n\n\nBudgetary constraints force governments to use algos:\n\nEfficiency: faster case processing\n\nConsistency: same rules applied to everyone\n\nObjectivity: removes human bias\n\nCost savings\n\n\nIssues with Algorithms:\n\nData cleaning decisions (might not be good)\n\nData coding or classification (misclassification, race for example)\n\nData collection (proxies for items)\n\nHow results are interpreted\n\nWhat variables you put in the model\n\nEX: Healthcare algo bias whereby black patients are discriminated against due to proxies for healthcare needs.\n\n\nCensus data is used for:\n\nUnderstanding community demos\n\nAllocate government resources\n\nTracking neighborhood change\n\nDesigning fair algos\n\nPut into the constitution by Madison\n\n\nCensuses are decennial and contain 9 basic demographic questions.\n\nACS is 3% of households, done annually, and has more detailed questions on income, education, employment, and housing costs.\n\nACS is aggregated to 5-year estimates (to have more reliable data):\n\nCounty-level ACS data is for state and regional planning\n\nCensus tract level\n\nBlock group level: local analysis, but has large margins of errors (MOEs)\n\n\nTo protect privacy the census applies mathematical noise to individual data while preserving overall patterns:\n\nMOEs might skew results slightly or cause biases\n\n\nTIGER is Topologically Integrated Geographic Encoding and Referencing system."
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "title": "Week 2 Notes - Algorithmic Decision Making & Census Data",
    "section": "",
    "text": "Algorithms are a set of rules or instructions for solving a problem and completing a task.\nAlgorithms in a government are typically systems used to assist or replace human decision-makers:\n\nBased on predictions from models trained on historical data.\n\nUsing inputs (features or predictors) and outputs (outcome, dependent variable).\n\n\nLong history of government data collection:\n\nCensus data\n\nCivil registration system\n\nAdministrative records\n\n\nBudgetary constraints force governments to use algos:\n\nEfficiency: faster case processing\n\nConsistency: same rules applied to everyone\n\nObjectivity: removes human bias\n\nCost savings\n\n\nIssues with Algorithms:\n\nData cleaning decisions (might not be good)\n\nData coding or classification (misclassification, race for example)\n\nData collection (proxies for items)\n\nHow results are interpreted\n\nWhat variables you put in the model\n\nEX: Healthcare algo bias whereby black patients are discriminated against due to proxies for healthcare needs.\n\n\nCensus data is used for:\n\nUnderstanding community demos\n\nAllocate government resources\n\nTracking neighborhood change\n\nDesigning fair algos\n\nPut into the constitution by Madison\n\n\nCensuses are decennial and contain 9 basic demographic questions.\n\nACS is 3% of households, done annually, and has more detailed questions on income, education, employment, and housing costs.\n\nACS is aggregated to 5-year estimates (to have more reliable data):\n\nCounty-level ACS data is for state and regional planning\n\nCensus tract level\n\nBlock group level: local analysis, but has large margins of errors (MOEs)\n\n\nTo protect privacy the census applies mathematical noise to individual data while preserving overall patterns:\n\nMOEs might skew results slightly or cause biases\n\n\nTIGER is Topologically Integrated Geographic Encoding and Referencing system."
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#coding-techniques",
    "href": "weekly-notes/week-02-notes.html#coding-techniques",
    "title": "Week 2 Notes - Algorithmic Decision Making & Census Data",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nGEOID: geographic identifier\n\nNAM: Human-readable location\n\nTypical output is long, but you can force output = \"wide\"\n\nstr_remove(), str_extract(), str_replace()\n\nkable() for professional formatting"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#questions-challenges",
    "href": "weekly-notes/week-02-notes.html#questions-challenges",
    "title": "Week 2 Notes - Algorithmic Decision Making & Census Data",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\n\n\nMake my interpretations understandable for a policy audience."
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#connections-to-policy",
    "href": "weekly-notes/week-02-notes.html#connections-to-policy",
    "title": "Week 2 Notes - Algorithmic Decision Making & Census Data",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\n\nAlgorithmic decision making → understanding why your analysis matters for real policy decisions\n\nData subjectivity → why we emphasize transparent, reproducible methods in this class\n\nCensus data → the foundation for most urban planning and policy analysis\n\nR skills → the tools to do this work professionally and ethically"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#reflection",
    "href": "weekly-notes/week-02-notes.html#reflection",
    "title": "Week 2 Notes - Algorithmic Decision Making & Census Data",
    "section": "Reflection",
    "text": "Reflection\n\nThe data is clean and samples are sufficiently random and large to be representative.\n\nUndocumented workers are likely to be excluded and those who are nomadic or without a permanent address."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html",
    "href": "weekly-notes/week-04-notes.html",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "Setting up repositories on"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-04-notes.html#key-concepts-learned",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "Setting up repositories on"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#coding-techniques",
    "href": "weekly-notes/week-04-notes.html#coding-techniques",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nBasics of tidyverse and its accompanying commands of filter(), select(), mutate(), and summarize()\nQuarto functions on how to bold, italics, both bold and italics, code list, and strikethrough"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#questions-challenges",
    "href": "weekly-notes/week-04-notes.html#questions-challenges",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\n\n\nEverything was clear. I would still like to mess around more with Quarto."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#connections-to-policy",
    "href": "weekly-notes/week-04-notes.html#connections-to-policy",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\n\nThis was a building block week, so not much of direct application rather tracking and documentation baseline for setup."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#reflection",
    "href": "weekly-notes/week-04-notes.html#reflection",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Reflection",
    "text": "Reflection\n\nLearning how to create a custom repository was both enjoyable and insightful.\nIt could also serve as a way to share supplementary analyses and to present code in a more public, graphical, and accessible format for non-coding audiences."
  }
]