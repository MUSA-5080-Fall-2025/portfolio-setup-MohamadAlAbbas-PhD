[
  {
    "objectID": "weekly-notes/week-05-notes.html",
    "href": "weekly-notes/week-05-notes.html",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "Setting up repositories on"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-05-notes.html#key-concepts-learned",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "Setting up repositories on"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#coding-techniques",
    "href": "weekly-notes/week-05-notes.html#coding-techniques",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nBasics of tidyverse and its accompanying commands of filter(), select(), mutate(), and summarize()\nQuarto functions on how to bold, italics, both bold and italics, code list, and strikethrough"
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#questions-challenges",
    "href": "weekly-notes/week-05-notes.html#questions-challenges",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\n\n\nEverything was clear. I would still like to mess around more with Quarto."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#connections-to-policy",
    "href": "weekly-notes/week-05-notes.html#connections-to-policy",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\n\nThis was a building block week, so not much of direct application rather tracking and documentation baseline for setup."
  },
  {
    "objectID": "weekly-notes/week-05-notes.html#reflection",
    "href": "weekly-notes/week-05-notes.html#reflection",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Reflection",
    "text": "Reflection\n\nLearning how to create a custom repository was both enjoyable and insightful.\nIt could also serve as a way to share supplementary analyses and to present code in a more public, graphical, and accessible format for non-coding audiences."
  },
  {
    "objectID": "weekly-notes/week-03-notes.html",
    "href": "weekly-notes/week-03-notes.html",
    "title": "Week 3 Notes - Data Visualization & Exploratory Analysis",
    "section": "",
    "text": "EDA is exploratory data analysis:\n\nWhat the data looks like (distribution and missings)\nWhat patterns exist (clustering and relationships)\nWhat’s unusual (outliers)\nwhat questions does this raise\nhow reliable is the dataset\n\nBest practcies:\n\nReport corresponding MOEs of ACS estimates\nInclude a footnote to acknolwedge MOEs if not reporting\nprovide unreliability context which would revolve around the coefficient of variation (CV &lt;12% being good, 12-40% somewhat reliable, and CV &gt; 40% being concerning)\nreduce statistical uncertainty, collapse or aggregate data\nstat significance tests are recommended as you go.\n\nAre there geographic patterns or correlations?\nPopulation relationships, how size affect data quality\nAre certain communities systematically different"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-03-notes.html#key-concepts-learned",
    "title": "Week 3 Notes - Data Visualization & Exploratory Analysis",
    "section": "",
    "text": "EDA is exploratory data analysis:\n\nWhat the data looks like (distribution and missings)\nWhat patterns exist (clustering and relationships)\nWhat’s unusual (outliers)\nwhat questions does this raise\nhow reliable is the dataset\n\nBest practcies:\n\nReport corresponding MOEs of ACS estimates\nInclude a footnote to acknolwedge MOEs if not reporting\nprovide unreliability context which would revolve around the coefficient of variation (CV &lt;12% being good, 12-40% somewhat reliable, and CV &gt; 40% being concerning)\nreduce statistical uncertainty, collapse or aggregate data\nstat significance tests are recommended as you go.\n\nAre there geographic patterns or correlations?\nPopulation relationships, how size affect data quality\nAre certain communities systematically different"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#coding-techniques",
    "href": "weekly-notes/week-03-notes.html#coding-techniques",
    "title": "Week 3 Notes - Data Visualization & Exploratory Analysis",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nggplot2:\n\nData is the actual datasets\nAesthetics, variables mapped to visual properties (x ,y ,color, size )\nGeometries, how to display the data (points, bars, lines)\nAdditional layers: scales, themes, facets and annotations\n\nAesthetics:\n\nx, y, are data positions\ncolor of the point/line\nfill, is the area color\nsize, point/line size\nshape, point shape\nalpha, transparency\n\nleft_join() - keep all rows from left dataset\nright_join() - keep all rows from right dataset\ninner_join() - Keep matching only\nfull_join() - just merge the datasets"
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#questions-challenges",
    "href": "weekly-notes/week-03-notes.html#questions-challenges",
    "title": "Week 3 Notes - Data Visualization & Exploratory Analysis",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nEverything was clear."
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#connections-to-policy",
    "href": "weekly-notes/week-03-notes.html#connections-to-policy",
    "title": "Week 3 Notes - Data Visualization & Exploratory Analysis",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\n\nAnalyzing bias within data before running analysis and providing recommendations\nthis is done to allow us to ensure that no group is discriminated or biased against within the recommendations."
  },
  {
    "objectID": "weekly-notes/week-03-notes.html#reflection",
    "href": "weekly-notes/week-03-notes.html#reflection",
    "title": "Week 3 Notes - Data Visualization & Exploratory Analysis",
    "section": "Reflection",
    "text": "Reflection\n\ngood practices in terms of coding and communication ."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html",
    "href": "weekly-notes/week-01-notes.html",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "Public Sector Data scinece has several considerations different from private sector\n\nPublic good, and equity consideration\nNeeds to be interpretable and not have algorithmic bias\n\nGit is for version control system (VCS)\nGithub is the cloud computing backend of Git\nGithub Lingo\n\nFolder -&gt; Repo\nCommit -&gt; Post a snapshot\nPush means to send files to cloud\nPull downloads the files to local machine\n\nSetting up repositories on\n“YAML Ain’t Markup Language”"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-01-notes.html#key-concepts-learned",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "Public Sector Data scinece has several considerations different from private sector\n\nPublic good, and equity consideration\nNeeds to be interpretable and not have algorithmic bias\n\nGit is for version control system (VCS)\nGithub is the cloud computing backend of Git\nGithub Lingo\n\nFolder -&gt; Repo\nCommit -&gt; Post a snapshot\nPush means to send files to cloud\nPull downloads the files to local machine\n\nSetting up repositories on\n“YAML Ain’t Markup Language”"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#coding-techniques",
    "href": "weekly-notes/week-01-notes.html#coding-techniques",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nBasics of tidyverse and its accompanying commands of filter(), select(), mutate(), and summarize()\nQuarto functions on how to bold, italics, both bold and italics, code list, and strikethrough\nQuarto uses — as the header call\n“#” are used as the header calls\nTibbles\n\nselect() choose columns\nfilter() choose rows\nmutate() create new variables\nsummarize() calculate statistics\ngroup_by() operate on sub-groups\n\nSummarize and group_by tend ot happen together and typically using a pipeline %&gt;%\n%&gt;% pipelines are used to connect multiple codes in one command line rather than a sequence."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#questions-challenges",
    "href": "weekly-notes/week-01-notes.html#questions-challenges",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\nEverything was clear. I would still like to mess around more with Quarto.\nI messed around with Quarto again @ 9/14/2025"
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#connections-to-policy",
    "href": "weekly-notes/week-01-notes.html#connections-to-policy",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\nThis was a building block week, so not much of direct application rather tracking and documentation baseline for setup."
  },
  {
    "objectID": "weekly-notes/week-01-notes.html#reflection",
    "href": "weekly-notes/week-01-notes.html#reflection",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Reflection",
    "text": "Reflection\n\nLearning how to create a custom repository was both enjoyable and insightful.\nIt could also serve as a way to share supplementary analyses and to present code in a more public, graphical, and accessible format for non-coding audiences."
  },
  {
    "objectID": "midterm/slides/Midterm_slides_v3.html#research-question",
    "href": "midterm/slides/Midterm_slides_v3.html#research-question",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Research Question",
    "text": "Research Question\n\nHow accurately can we predict 2023–2024 residential sale prices in Philadelphia using property characteristics and neighborhood/contextual features, in order to improve the city’s automated property tax assessment model?\n\nWhy This Matters\n\nMore accurate assessments -&gt; fairer taxes and fewer disputes.\nNeighborhood and spatial insights capture real market dynamics beyond building features.\nTransparent and data-driven modeling builds trust and accountability in city governance."
  },
  {
    "objectID": "midterm/slides/Midterm_slides_v3.html#data-overview",
    "href": "midterm/slides/Midterm_slides_v3.html#data-overview",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Data Overview",
    "text": "Data Overview\nOur Data Foundation\n\nPhiladelphia Property Sales (2023–2024)\nOver 17,200 residential transactions citywide capturing sale price, location, and property structural details.\nNeighborhood & Contextual Data\nCombined city (OpenDataPhilly) and census (ACS 2023) information on:\n\nDemographics: race, household income, employment, education\nPublic safety: proximity to violent and petty crime\nAccessibility: transit stops, bike lanes\nAmenities & services: parks, schools, hospitals, fire department, food retail"
  },
  {
    "objectID": "midterm/slides/Midterm_slides_v3.html#what-do-home-prices-look-like",
    "href": "midterm/slides/Midterm_slides_v3.html#what-do-home-prices-look-like",
    "title": "Philadelphia Housing Price Prediction",
    "section": "What Do Home Prices Look Like?",
    "text": "What Do Home Prices Look Like?\n\nHistogram of Sale PricesKey Findings\n\nMost homes sell for under $400,000.\n\nA small number of luxury properties push up the high end.\n\nReflects strong housing inequality across neighborhoods."
  },
  {
    "objectID": "midterm/slides/Midterm_slides_v3.html#where-are-expensive-homes",
    "href": "midterm/slides/Midterm_slides_v3.html#where-are-expensive-homes",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Where Are Expensive Homes?",
    "text": "Where Are Expensive Homes?\n\n\nKey Findings:\n\nCenter City and Northwest Philadelphia are highest-value clusters.\n\nRiver Wards and University City show emerging appreciation.\n\nNorth Philadelphia shows predominantly lower-priced housing.\n\n\n\n\n\n\nGeographical Distribution of Sale Prices"
  },
  {
    "objectID": "midterm/slides/Midterm_slides_v3.html#neighborhood-home-prices",
    "href": "midterm/slides/Midterm_slides_v3.html#neighborhood-home-prices",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Neighborhood & Home Prices?",
    "text": "Neighborhood & Home Prices?\n\nBubble Map of Sale Prices vs. Household Income by NeighborhoodKey Findings:\n\nNeighborhood wealth remains a dominant factor in home values.\nLow-income, high-poverty neighborhoods continue to see limited appreciation."
  },
  {
    "objectID": "midterm/slides/Midterm_slides_v3.html#how-does-model-performance-improve",
    "href": "midterm/slides/Midterm_slides_v3.html#how-does-model-performance-improve",
    "title": "Philadelphia Housing Price Prediction",
    "section": "How Does Model Performance Improve?",
    "text": "How Does Model Performance Improve?\n\n\n\nModel\nCV RMSE\nR²\n\n\n\n\nStructural Only\n136,121\n0.67\n\n\nSpatial\n108,112\n0.79\n\n\nFixed Effects\n93,038\n0.85\n\n\nHyper-model\n64,974\n0.92\n\n\n\nBottom Line\nEach additional data layer improves accuracy, with neighborhood / market-value features producing the largest jump (lowest RMSE and highest R²)."
  },
  {
    "objectID": "midterm/slides/Midterm_slides_v3.html#what-drives-the-improvement",
    "href": "midterm/slides/Midterm_slides_v3.html#what-drives-the-improvement",
    "title": "Philadelphia Housing Price Prediction",
    "section": "What Drives the Improvement?",
    "text": "What Drives the Improvement?\n\nFrom taking into accounts only the structural/building quality variables in our first model;\nAdding Spatial Data accounts for proximity to parks, public schools, food retail, and violent crime.\nAdding Census & Socioeconomic Context captures income, education, and housing quality.\n\nAdding Neighborhood Fixed Effects controls for unobserved local traits.\n\nGrouping Small Neighborhoods stabilizes estimates where data are sparse (sales&lt;10).\nIncluding Market Value Benchmarks aligns predictions with broader price trends.\n\nTakeaway:\nPhiladelphia’s housing market is highly localized — accuracy improves most when models reflect neighborhood structure and market context. The Hyper model which optimized by groups small neighborhoods, is more parsimonious while maintaining high predictive performance."
  },
  {
    "objectID": "midterm/slides/Midterm_slides_v3.html#best-models",
    "href": "midterm/slides/Midterm_slides_v3.html#best-models",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Best Models",
    "text": "Best Models\n\n\n\nModel\nRMSE ($)\nR²\n\n\n\n\nModel 3: Fixed Effects\n$93,038\n0.85\n\n\nModel 4: Hyper Model\n$64,974\n0.92\n\n\n\nBoth models capture structural, spatial, census, and socioeconomic factors,\nwhile incorporating neighborhood effects to reflect local market variation.\nKey Insight:\nAdding small-neighborhood grouping and market value signals delivers a major leap in accuracy — boosting R² from 0.85 → 0.93. By going down to the block level, our model captures fine-grained local variation, showing that neighborhood and market context are essential for fair property assessments."
  },
  {
    "objectID": "midterm/slides/Midterm_slides_v3.html#key-findings",
    "href": "midterm/slides/Midterm_slides_v3.html#key-findings",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Key Findings",
    "text": "Key Findings\nTop Predictors\n\nNeighborhoods (Prestige/unobserved characteristics)\nMarket Value\n\nInteresting Effects\n\nHigher % Black in the tract is associated with lower sale prices, holding all else constant.\nHigher % age 65+ (more seniors) is associated with higher sale prices.\nViolent incidents within ~600 feet of the home are associated with lower sale prices."
  },
  {
    "objectID": "midterm/slides/Midterm_slides_v3.html#hardest-to-predict",
    "href": "midterm/slides/Midterm_slides_v3.html#hardest-to-predict",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Hardest To Predict",
    "text": "Hardest To Predict\n\nVisualization of Residual by Neighborhood\nResiduals are calculated as actual price minus predicted price.\nDowntown neighborhoods tend to have under predicted values, while suburban areas show over predicted values"
  },
  {
    "objectID": "midterm/slides/Midterm_slides_v3.html#policy-recommendations",
    "href": "midterm/slides/Midterm_slides_v3.html#policy-recommendations",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Policy Recommendations",
    "text": "Policy Recommendations\nKey Actions\n\nLending and rehab programs should include an equity adjustment so borrowers in historically underpriced Black areas can actually access capital.\nKeep long-term older homeowners in place with repair help and tax relief, because neighborhood stability itself raises surrounding home values.\nNever deploy pricing algorithms like this for taxes or loan caps without a bias check; the model should be used to monitor inequity, not lock it in.\nFund repairs and upgrades for small owners in undervalued areas — not just in already-rich areas — so families who maintain their homes can actually build wealth.\nUse models like this as a bias check — not to set taxes or loan limits directly, but to detect where the market may be undervaluing certain neighborhoods so policy can respond."
  },
  {
    "objectID": "midterm/slides/Midterm_slides_v3.html#limitations-next-steps",
    "href": "midterm/slides/Midterm_slides_v3.html#limitations-next-steps",
    "title": "Philadelphia Housing Price Prediction",
    "section": "Limitations & Next Steps",
    "text": "Limitations & Next Steps\nLimitations\n\nData coverage: After cleaning, the dataset dropped from ~24K to ~17K sales; if removals weren’t random, the model may overrepresent “normal” transactions and underrepresent edge cases.\nLagged timing: A single nearby incident doesn’t instantly cut value; what matters is a sustained history of violence that shapes how buyers perceive safety and status on that block.\nInterior detail: We can’t actually see renovation quality (finishes, systems, upgrades), so the “interior condition” label is coarse — and assessed market value is likely soaking up a lot of that missing nuance.\n\nNext Steps\n\nIncorporate Spatial Models: Explore spatial regression and spatial lag models to better capture location-based dependencies and spatial effects on housing prices.\nField Validation & Data Enrichment: Conduct field surveys and integrate cross-validated dataset to assess model and ensure predictions accurately reflect real market conditions."
  },
  {
    "objectID": "midterm/slides/Midterm_slides_v3.html#thank-you",
    "href": "midterm/slides/Midterm_slides_v3.html#thank-you",
    "title": "Philadelphia Housing Price Prediction",
    "section": "THANK YOU",
    "text": "THANK YOU"
  },
  {
    "objectID": "labs/week-04/scripts/week4_inclass_practice.html",
    "href": "labs/week-04/scripts/week4_inclass_practice.html",
    "title": "Week 4 In-Class Practice Exercises",
    "section": "",
    "text": "library(sf)\nlibrary(tidyverse)\nlibrary(tigris)\nlibrary(tidycensus)\nlibrary(scales)\nlibrary(patchwork)\nlibrary(here)\n# Set Census API key\ncensus_api_key(\"807ea1c0820a3e1e46dde3c53438622057fcc1ba\")\n\n# Load the data (same as lecture)\npa_counties &lt;- st_read(here(\"data/Pennsylvania_County_Boundaries.shp\"))\n\nReading layer `Pennsylvania_County_Boundaries' from data source \n  `C:\\Users\\MEPI\\Documents\\GitHub\\portfolio-setup-MohamadAlAbbas-PhD\\labs\\week-04\\data\\Pennsylvania_County_Boundaries.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 67 features and 19 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -8963377 ymin: 4825316 xmax: -8314404 ymax: 5201413\nProjected CRS: WGS 84 / Pseudo-Mercator\n\ndistricts &lt;- st_read(here(\"data/districts.geojson\"))\n\nReading layer `U.S._Congressional_Districts_for_Pennsylvania' from data source \n  `C:\\Users\\MEPI\\Documents\\GitHub\\portfolio-setup-MohamadAlAbbas-PhD\\labs\\week-04\\data\\districts.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 17 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -80.51939 ymin: 39.71986 xmax: -74.68956 ymax: 42.26935\nGeodetic CRS:  WGS 84\n\nhospitals &lt;- st_read(here(\"data/hospitals.geojson\"))\n\nReading layer `hospitals' from data source \n  `C:\\Users\\MEPI\\Documents\\GitHub\\portfolio-setup-MohamadAlAbbas-PhD\\labs\\week-04\\data\\hospitals.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 223 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -80.49621 ymin: 39.75163 xmax: -74.86704 ymax: 42.13403\nGeodetic CRS:  WGS 84\n\ncensus_tracts &lt;- tracts(state = \"PA\", cb = TRUE)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\nmetro_areas &lt;- core_based_statistical_areas(cb = TRUE)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |================                                                      |  24%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=====================================                                 |  54%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\n# Standardize CRS\nhospitals &lt;- st_transform(hospitals, st_crs(pa_counties))\ncensus_tracts &lt;- st_transform(census_tracts, st_crs(pa_counties))\nmetro_areas &lt;- st_transform(metro_areas, st_crs(pa_counties))\ndistricts &lt;- st_transform(districts, st_crs(census_tracts))"
  },
  {
    "objectID": "labs/week-04/scripts/week4_inclass_practice.html#setup",
    "href": "labs/week-04/scripts/week4_inclass_practice.html#setup",
    "title": "Week 4 In-Class Practice Exercises",
    "section": "",
    "text": "library(sf)\nlibrary(tidyverse)\nlibrary(tigris)\nlibrary(tidycensus)\nlibrary(scales)\nlibrary(patchwork)\nlibrary(here)\n# Set Census API key\ncensus_api_key(\"807ea1c0820a3e1e46dde3c53438622057fcc1ba\")\n\n# Load the data (same as lecture)\npa_counties &lt;- st_read(here(\"data/Pennsylvania_County_Boundaries.shp\"))\n\nReading layer `Pennsylvania_County_Boundaries' from data source \n  `C:\\Users\\MEPI\\Documents\\GitHub\\portfolio-setup-MohamadAlAbbas-PhD\\labs\\week-04\\data\\Pennsylvania_County_Boundaries.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 67 features and 19 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -8963377 ymin: 4825316 xmax: -8314404 ymax: 5201413\nProjected CRS: WGS 84 / Pseudo-Mercator\n\ndistricts &lt;- st_read(here(\"data/districts.geojson\"))\n\nReading layer `U.S._Congressional_Districts_for_Pennsylvania' from data source \n  `C:\\Users\\MEPI\\Documents\\GitHub\\portfolio-setup-MohamadAlAbbas-PhD\\labs\\week-04\\data\\districts.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 17 features and 8 fields\nGeometry type: POLYGON\nDimension:     XY\nBounding box:  xmin: -80.51939 ymin: 39.71986 xmax: -74.68956 ymax: 42.26935\nGeodetic CRS:  WGS 84\n\nhospitals &lt;- st_read(here(\"data/hospitals.geojson\"))\n\nReading layer `hospitals' from data source \n  `C:\\Users\\MEPI\\Documents\\GitHub\\portfolio-setup-MohamadAlAbbas-PhD\\labs\\week-04\\data\\hospitals.geojson' \n  using driver `GeoJSON'\nSimple feature collection with 223 features and 11 fields\nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: -80.49621 ymin: 39.75163 xmax: -74.86704 ymax: 42.13403\nGeodetic CRS:  WGS 84\n\ncensus_tracts &lt;- tracts(state = \"PA\", cb = TRUE)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |===================================                                   |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\nmetro_areas &lt;- core_based_statistical_areas(cb = TRUE)\n\n\n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   1%\n  |                                                                            \n  |=                                                                     |   2%\n  |                                                                            \n  |==                                                                    |   2%\n  |                                                                            \n  |==                                                                    |   3%\n  |                                                                            \n  |===                                                                   |   4%\n  |                                                                            \n  |===                                                                   |   5%\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |====                                                                  |   6%\n  |                                                                            \n  |=====                                                                 |   7%\n  |                                                                            \n  |=====                                                                 |   8%\n  |                                                                            \n  |======                                                                |   8%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=======                                                               |   9%\n  |                                                                            \n  |=======                                                               |  10%\n  |                                                                            \n  |========                                                              |  11%\n  |                                                                            \n  |========                                                              |  12%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |==========                                                            |  14%\n  |                                                                            \n  |==========                                                            |  15%\n  |                                                                            \n  |===========                                                           |  15%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |============                                                          |  17%\n  |                                                                            \n  |============                                                          |  18%\n  |                                                                            \n  |=============                                                         |  18%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |==============                                                        |  21%\n  |                                                                            \n  |===============                                                       |  21%\n  |                                                                            \n  |===============                                                       |  22%\n  |                                                                            \n  |================                                                      |  22%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |================                                                      |  24%\n  |                                                                            \n  |=================                                                     |  24%\n  |                                                                            \n  |=================                                                     |  25%\n  |                                                                            \n  |==================                                                    |  25%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |===================                                                   |  28%\n  |                                                                            \n  |====================                                                  |  28%\n  |                                                                            \n  |====================                                                  |  29%\n  |                                                                            \n  |=====================                                                 |  29%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=====================                                                 |  31%\n  |                                                                            \n  |======================                                                |  31%\n  |                                                                            \n  |======================                                                |  32%\n  |                                                                            \n  |=======================                                               |  32%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |========================                                              |  35%\n  |                                                                            \n  |=========================                                             |  35%\n  |                                                                            \n  |=========================                                             |  36%\n  |                                                                            \n  |==========================                                            |  36%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |===========================                                           |  38%\n  |                                                                            \n  |===========================                                           |  39%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |============================                                          |  41%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |=============================                                         |  42%\n  |                                                                            \n  |==============================                                        |  42%\n  |                                                                            \n  |==============================                                        |  43%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |===============================                                       |  45%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |================================                                      |  46%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |=================================                                     |  48%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |===================================                                   |  49%\n  |                                                                            \n  |===================================                                   |  50%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=====================================                                 |  52%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=====================================                                 |  54%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |======================================                                |  55%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |========================================                              |  57%\n  |                                                                            \n  |========================================                              |  58%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |==========================================                            |  59%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |==========================================                            |  61%\n  |                                                                            \n  |===========================================                           |  61%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |============================================                          |  62%\n  |                                                                            \n  |============================================                          |  63%\n  |                                                                            \n  |=============================================                         |  64%\n  |                                                                            \n  |=============================================                         |  65%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |===============================================                       |  66%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |===============================================                       |  68%\n  |                                                                            \n  |================================================                      |  68%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |=================================================                     |  70%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |==================================================                    |  71%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |===================================================                   |  72%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |====================================================                  |  75%\n  |                                                                            \n  |=====================================================                 |  75%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |======================================================                |  77%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=======================================================               |  78%\n  |                                                                            \n  |=======================================================               |  79%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |========================================================              |  81%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |=========================================================             |  82%\n  |                                                                            \n  |==========================================================            |  82%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |===========================================================           |  84%\n  |                                                                            \n  |===========================================================           |  85%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |============================================================          |  86%\n  |                                                                            \n  |=============================================================         |  87%\n  |                                                                            \n  |=============================================================         |  88%\n  |                                                                            \n  |==============================================================        |  88%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |===============================================================       |  89%\n  |                                                                            \n  |===============================================================       |  90%\n  |                                                                            \n  |===============================================================       |  91%\n  |                                                                            \n  |================================================================      |  91%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |=================================================================     |  93%\n  |                                                                            \n  |==================================================================    |  94%\n  |                                                                            \n  |==================================================================    |  95%\n  |                                                                            \n  |===================================================================   |  95%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |====================================================================  |  96%\n  |                                                                            \n  |====================================================================  |  97%\n  |                                                                            \n  |====================================================================  |  98%\n  |                                                                            \n  |===================================================================== |  98%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\n# Standardize CRS\nhospitals &lt;- st_transform(hospitals, st_crs(pa_counties))\ncensus_tracts &lt;- st_transform(census_tracts, st_crs(pa_counties))\nmetro_areas &lt;- st_transform(metro_areas, st_crs(pa_counties))\ndistricts &lt;- st_transform(districts, st_crs(census_tracts))"
  },
  {
    "objectID": "labs/week-04/scripts/week4_inclass_practice.html#exercise-1-find-your-countys-neighbors-10-minutes",
    "href": "labs/week-04/scripts/week4_inclass_practice.html#exercise-1-find-your-countys-neighbors-10-minutes",
    "title": "Week 4 In-Class Practice Exercises",
    "section": "Exercise 1: Find Your County’s Neighbors (10 minutes)",
    "text": "Exercise 1: Find Your County’s Neighbors (10 minutes)\nGoal: Practice spatial filtering with different predicates\n\n1.1 Pick a Pennsylvania County\nYour Task: Choose any PA county and find all counties that border it.\n\n# Step 1: Look at available county names\nunique(pa_counties$COUNTY_NAM)\n\n [1] \"MONTGOMERY\"     \"BRADFORD\"       \"BUCKS\"          \"TIOGA\"         \n [5] \"UNION\"          \"VENANGO\"        \"WASHINGTON\"     \"WAYNE\"         \n [9] \"MCKEAN\"         \"MERCER\"         \"MIFFLIN\"        \"MONTOUR\"       \n[13] \"NORTHAMPTON\"    \"NORTHUMBERLAND\" \"PERRY\"          \"PIKE\"          \n[17] \"POTTER\"         \"SCHUYLKILL\"     \"SNYDER\"         \"SOMERSET\"      \n[21] \"SULLIVAN\"       \"LEBANON\"        \"BUTLER\"         \"CAMBRIA\"       \n[25] \"CAMERON\"        \"CARBON\"         \"CENTRE\"         \"CLARION\"       \n[29] \"CLEARFIELD\"     \"CLINTON\"        \"COLUMBIA\"       \"CRAWFORD\"      \n[33] \"CUMBERLAND\"     \"DAUPHIN\"        \"INDIANA\"        \"JEFFERSON\"     \n[37] \"JUNIATA\"        \"LANCASTER\"      \"WESTMORELAND\"   \"WYOMING\"       \n[41] \"YORK\"           \"PHILADELPHIA\"   \"LEHIGH\"         \"LUZERNE\"       \n[45] \"LYCOMING\"       \"LAWRENCE\"       \"DELAWARE\"       \"ELK\"           \n[49] \"ERIE\"           \"FAYETTE\"        \"FOREST\"         \"FRANKLIN\"      \n[53] \"FULTON\"         \"GREENE\"         \"HUNTINGDON\"     \"ADAMS\"         \n[57] \"ALLEGHENY\"      \"ARMSTRONG\"      \"BEAVER\"         \"BEDFORD\"       \n[61] \"BLAIR\"          \"SUSQUEHANNA\"    \"WARREN\"         \"BERKS\"         \n[65] \"CHESTER\"        \"MONROE\"         \"LACKAWANNA\"    \n\n# Step 2: Pick one county (change this to your choice!)\nmy_county &lt;- pa_counties %&gt;%\n  filter(COUNTY_NAM == \"CENTRE\")  # Change \"CENTRE\" to your county\n\n# Step 3: Find neighbors using st_touches\nmy_neighbors &lt;- pa_counties %&gt;%\n  st_filter(my_county, .predicate = st_touches)\n\n# Step 4: How many neighbors does your county have?\ncat(\"Number of neighboring counties:\", nrow(my_neighbors), \"\\n\")\n\nNumber of neighboring counties: 6 \n\nprint(\"Neighbor names:\")\n\n[1] \"Neighbor names:\"\n\nprint(my_neighbors$COUNTY_NAM)\n\n[1] \"UNION\"      \"MIFFLIN\"    \"CLEARFIELD\" \"CLINTON\"    \"HUNTINGDON\"\n[6] \"BLAIR\"     \n\n\n\n\n1.2 Map Your Results\nYour Task: Create a map showing your county and its neighbors in different colors.\n\n# Create the map\nggplot() +\n  geom_sf(data = pa_counties, fill = \"lightgray\", color = \"white\") +\n  geom_sf(data = my_neighbors, fill = \"lightblue\", alpha = 0.7) +\n  geom_sf(data = my_county, fill = \"darkblue\") +\n  labs(\n    title = paste(\"Neighbors of\", my_county$COUNTY_NAM[1], \"County\"),\n    subtitle = paste(nrow(my_neighbors), \"neighboring counties\")\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n1.3 Challenge: Compare with st_intersects\nYour Task: What happens if you use st_intersects instead of st_touches? Why is the count different?\n\n# Use st_intersects\nintersecting_counties &lt;- pa_counties %&gt;%\n  st_filter(my_county, .predicate = st_intersects)\n\ncat(\"With st_touches:\", nrow(my_neighbors), \"counties\\n\")\n\nWith st_touches: 6 counties\n\ncat(\"With st_intersects:\", nrow(intersecting_counties), \"counties\\n\")\n\nWith st_intersects: 7 counties\n\ncat(\"Difference:\", nrow(intersecting_counties) - nrow(my_neighbors), \"\\n\")\n\nDifference: 1 \n\n\nQuestion: Why is there a difference of 1? What does this tell you about the difference between st_touches and st_intersects?"
  },
  {
    "objectID": "labs/week-04/scripts/week4_inclass_practice.html#exercise-2-hospital-service-areas-15-minutes",
    "href": "labs/week-04/scripts/week4_inclass_practice.html#exercise-2-hospital-service-areas-15-minutes",
    "title": "Week 4 In-Class Practice Exercises",
    "section": "Exercise 2: Hospital Service Areas (15 minutes)",
    "text": "Exercise 2: Hospital Service Areas (15 minutes)\nGoal: Practice buffering and measuring accessibility\n\n2.1 Create Hospital Service Areas\nYour Task: Create 15-mile (24140 meter) service areas around all hospitals in your county.\n\n# Step 1: Filter hospitals in your county\n# First do a spatial join to assign counties to hospitals\nhospitals_with_county &lt;- hospitals %&gt;%\n  st_join(pa_counties %&gt;% select(COUNTY_NAM))\n\n# Filter for your county's hospitals\nmy_county_hospitals &lt;- hospitals_with_county %&gt;%\n  filter(COUNTY_NAM == \"CENTRE\")  # Change to match your county\n\ncat(\"Number of hospitals in county:\", nrow(my_county_hospitals), \"\\n\")\n\nNumber of hospitals in county: 3 \n\n# Step 2: Project to accurate CRS for buffering\nmy_county_hospitals_proj &lt;- my_county_hospitals %&gt;%\n  st_transform(3365)  # Pennsylvania State Plane South\n\n# Step 3: Create 15-mile buffers (24140 meters = 15 miles)\nhospital_service_areas &lt;- my_county_hospitals_proj %&gt;%\n  st_buffer(dist = 79200)  # 15 miles in feet for PA State Plane\n\n# Step 4: Transform back for mapping\nhospital_service_areas &lt;- st_transform(hospital_service_areas, st_crs(pa_counties))\n\n\n\n2.2 Map Service Coverage\nYour Task: Create a map showing hospitals and their service areas.\n\nggplot() +\n  geom_sf(data = my_county, fill = \"white\", color = \"gray\") +\n  geom_sf(data = hospital_service_areas, fill = \"lightblue\", alpha = 0.4) +\n  geom_sf(data = my_county_hospitals, color = \"red\", size = 2) +\n  labs(\n    title = paste(\"Hospital Service Areas in\", my_county$COUNTY_NAM[1], \"County\"),\n    subtitle = \"Red points = Hospitals, Blue areas = 15-mile service zones\"\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n2.3 Calculate Coverage\nYour Task: What percentage of your county is within 15 miles of a hospital?\n\n# Union all service areas into one polygon\ncombined_service_area &lt;- hospital_service_areas %&gt;%\n  st_union()\n\n# Calculate areas (need to be in projected CRS)\nmy_county_proj &lt;- st_transform(my_county, 3365)\ncombined_service_proj &lt;- st_transform(combined_service_area, 3365)\n\n# Find intersection\ncoverage_area &lt;- st_intersection(my_county_proj, combined_service_proj)\n\n# Calculate percentages\ncounty_area &lt;- as.numeric(st_area(my_county_proj))\ncovered_area &lt;- as.numeric(st_area(coverage_area))\ncoverage_pct &lt;- (covered_area / county_area) * 100\n\ncat(\"County area:\", round(county_area / 1e6, 1), \"sq km\\n\")\n\nCounty area: 30993.6 sq km\n\ncat(\"Covered area:\", round(covered_area / 1e6, 1), \"sq km\\n\")\n\nCovered area: 19205.1 sq km\n\ncat(\"Coverage:\", round(coverage_pct, 1), \"%\\n\")\n\nCoverage: 62 %\n\n\nQuestion: Is your county well-served by hospitals? What areas might be underserved?"
  },
  {
    "objectID": "labs/week-04/scripts/week4_inclass_practice.html#exercise-3-congressional-district-analysis-15-minutes",
    "href": "labs/week-04/scripts/week4_inclass_practice.html#exercise-3-congressional-district-analysis-15-minutes",
    "title": "Week 4 In-Class Practice Exercises",
    "section": "Exercise 3: Congressional District Analysis (15 minutes)",
    "text": "Exercise 3: Congressional District Analysis (15 minutes)\nGoal: Practice spatial joins and aggregation\n\n4.1 Join Districts to Counties\nYour Task: Figure out which congressional districts overlap with each county.\n\n# Spatial join: districts to counties\ndistricts_by_county &lt;- districts %&gt;%\n  st_join(pa_counties %&gt;% select(COUNTY_NAM)) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(COUNTY_NAM) %&gt;%\n  summarize(\n    n_districts = n_distinct(OBJECTID),\n    district_ids = paste(unique(MSLINK), collapse = \", \")\n  ) %&gt;%\n  arrange(desc(n_districts))\n\n# Which counties have the most districts?\nhead(districts_by_county, 10)\n\n# A tibble: 10 × 3\n   COUNTY_NAM   n_districts district_ids            \n   &lt;chr&gt;              &lt;int&gt; &lt;chr&gt;                   \n 1 MONTGOMERY             7 19, 2, 14, 20, 4, 15, 21\n 2 BERKS                  6 8, 19, 2, 14, 4, 5      \n 3 ALLEGHENY              5 11, 12, 17, 1, 7        \n 4 PHILADELPHIA           5 19, 14, 20, 15, 21      \n 5 WESTMORELAND           5 11, 12, 17, 3, 7        \n 6 BUCKS                  4 19, 2, 14, 20           \n 7 CHESTER                4 14, 4, 5, 15            \n 8 DAUPHIN                4 8, 3, 5, 6              \n 9 JUNIATA                4 8, 12, 3, 6             \n10 LANCASTER              4 8, 4, 5, 6              \n\n\n\n\n4.2 Calculate District Statistics\nYour Task: Get demographic data for census tracts and aggregate to districts.\n\n# Get tract-level demographics\ntract_demographics &lt;- get_acs(\n  geography = \"tract\",\n  variables = c(\n    total_pop = \"B01003_001\",\n    median_income = \"B19013_001\",\n    white_pop = \"B03002_003\",\n    black_pop = \"B03002_004\",\n    hispanic_pop = \"B03002_012\"\n  ),\n  state = \"PA\",\n  year = 2022,\n  output = \"wide\"\n)\n\n# Join to tract boundaries\ntracts_with_data &lt;- census_tracts %&gt;%\n  left_join(tract_demographics, by = \"GEOID\")\n\n# Spatial join to districts and aggregate\ndistrict_demographics &lt;- tracts_with_data %&gt;%\n  st_join(districts) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(OBJECTID, MSLINK) %&gt;%\n  summarize(\n    total_population = sum(total_popE, na.rm = TRUE),\n    median_income = weighted.mean(median_incomeE, total_popE, na.rm = TRUE),\n    pct_white = sum(white_popE, na.rm = TRUE) / sum(total_popE, na.rm = TRUE) * 100,\n    pct_black = sum(black_popE, na.rm = TRUE) / sum(total_popE, na.rm = TRUE) * 100,\n    pct_hispanic = sum(hispanic_popE, na.rm = TRUE) / sum(total_popE, na.rm = TRUE) * 100,\n    n_tracts = n()\n  ) %&gt;%\n  arrange(desc(total_population))\n\n# Show results\nhead(district_demographics, 10)\n\n# A tibble: 10 × 8\n# Groups:   OBJECTID [10]\n   OBJECTID MSLINK total_population median_income pct_white pct_black\n      &lt;int&gt;  &lt;int&gt;            &lt;dbl&gt;         &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1      113     14          1229246       107602.      72.0     11.0 \n 2      108     11          1010001        76612.      76.6     13.4 \n 3      120      7          1006212        88796.      83.3      8.15\n 4      107      8           993966        70241.      88.7      2.24\n 5      117      3           986114        69017.      91.0      2.25\n 6      109     12           979419        63952.      92.2      1.81\n 7      118      4           974715       111250.      73.0      5.00\n 8      114     17           972225        71139.      91.8      2.75\n 9      110     19           932212       113798.      80.7      3.79\n10      123      6           927718        80080.      75.6      8.65\n# ℹ 2 more variables: pct_hispanic &lt;dbl&gt;, n_tracts &lt;int&gt;\n\n\n\n\n4.3 Map District Demographics\nYour Task: Create a choropleth map of median income by congressional district.\n\n# Join demographics back to district boundaries\ndistricts_with_demographics &lt;- districts %&gt;%\n  left_join(district_demographics, by = \"OBJECTID\")\n\n# Create the map\nggplot(districts_with_demographics) +\n  geom_sf(aes(fill = median_income), color = \"white\", size = 0.5) +\n  scale_fill_viridis_c(\n    name = \"Median\\nIncome\",\n    labels = dollar,\n    option = \"plasma\"\n  ) +\n  labs(\n    title = \"Median Household Income by Congressional District\",\n    subtitle = \"Pennsylvania\",\n    caption = \"Source: ACS 2018-2022\"\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\n\n\n4.4 Challenge: Find Diverse Districts\nYour Task: Which districts are the most racially diverse?\n\n# Calculate diversity index (simple version: higher = more diverse)\n# A perfectly even distribution would be ~33% each for 3 groups\ndistrict_demographics &lt;- district_demographics %&gt;%\n  mutate(\n    diversity_score = 100 - abs(pct_white - 33.3) - abs(pct_black - 33.3) - abs(pct_hispanic - 33.3)\n  ) %&gt;%\n  arrange(desc(diversity_score))\n\n# Most diverse districts\nhead(district_demographics %&gt;% select(MSLINK, pct_white, pct_black, pct_hispanic, diversity_score), 5)\n\n# A tibble: 5 × 6\n# Groups:   OBJECTID [5]\n  OBJECTID MSLINK pct_white pct_black pct_hispanic diversity_score\n     &lt;int&gt;  &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;           &lt;dbl&gt;\n1      115     20      37.9     25.4         23.7             77.8\n2      122     21      33.5     49.7          5.73            55.9\n3      121     15      58.8     24.4          5.77            38.1\n4      111      2      71.2      4.88        18.3             18.7\n5      113     14      72.0     11.0          7.09            12.8\n\n# Least diverse districts\ntail(district_demographics %&gt;% select(MSLINK, pct_white, pct_black, pct_hispanic, diversity_score), 5)\n\n# A tibble: 5 × 6\n# Groups:   OBJECTID [5]\n  OBJECTID MSLINK pct_white pct_black pct_hispanic diversity_score\n     &lt;int&gt;  &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;           &lt;dbl&gt;\n1      107      8      88.7      2.24         5.73           -14.1\n2      116      1      89.3      3.66         2.53           -16.4\n3      117      3      91.0      2.25         3.24           -18.8\n4      114     17      91.8      2.75         1.49           -20.9\n5      109     12      92.2      1.81         2.09           -21.6"
  },
  {
    "objectID": "labs/week-04/scripts/week4_inclass_practice.html#exercise-5-projection-effects-10-minutes",
    "href": "labs/week-04/scripts/week4_inclass_practice.html#exercise-5-projection-effects-10-minutes",
    "title": "Week 4 In-Class Practice Exercises",
    "section": "Exercise 5: Projection Effects (10 minutes)",
    "text": "Exercise 5: Projection Effects (10 minutes)\nGoal: Understand how CRS affects calculations\n\n5.1 Calculate Areas in Different Projections\nYour Task: Calculate county areas using different coordinate systems and compare.\n\n# Calculate areas in different CRS\narea_comparison &lt;- pa_counties %&gt;%\n  # Geographic (WGS84) - WRONG for areas!\n  st_transform(4326) %&gt;%\n  mutate(area_geographic = as.numeric(st_area(.))) %&gt;%\n  # PA State Plane South - Good for PA\n  st_transform(3365) %&gt;%\n  mutate(area_state_plane = as.numeric(st_area(.))) %&gt;%\n  # Albers Equal Area - Good for areas\n  st_transform(5070) %&gt;%\n  mutate(area_albers = as.numeric(st_area(.))) %&gt;%\n  st_drop_geometry() %&gt;%\n  select(COUNTY_NAM, starts_with(\"area_\")) %&gt;%\n  mutate(\n    # Calculate errors compared to Albers (most accurate for area)\n    error_geographic_pct = abs(area_geographic - area_albers) / area_albers * 100,\n    error_state_plane_pct = abs(area_state_plane - area_albers) / area_state_plane * 100\n  )\n\n# Show counties with biggest errors\narea_comparison %&gt;%\n  arrange(desc(error_geographic_pct)) %&gt;%\n  select(COUNTY_NAM, error_geographic_pct, error_state_plane_pct) %&gt;%\n  head(10)\n\n    COUNTY_NAM error_geographic_pct error_state_plane_pct\n1         ERIE            0.1520567              90.71567\n2  SUSQUEHANNA            0.1480129              90.71426\n3       MCKEAN            0.1479719              90.71414\n4       WARREN            0.1478826              90.71421\n5     BRADFORD            0.1473177              90.71402\n6        TIOGA            0.1469541              90.71390\n7       POTTER            0.1463317              90.71371\n8     CRAWFORD            0.1447572              90.71326\n9        WAYNE            0.1440222              90.71306\n10     WYOMING            0.1409741              90.71215\n\n\n\n\n5.2 Visualize the Error\nYour Task: Map which counties have the biggest area calculation errors.\n\n# Join error data back to counties\ncounties_with_errors &lt;- pa_counties %&gt;%\n  left_join(\n    area_comparison %&gt;% select(COUNTY_NAM, error_geographic_pct),\n    by = \"COUNTY_NAM\"\n  )\n\n# Map the error\nggplot(counties_with_errors) +\n  geom_sf(aes(fill = error_geographic_pct), color = \"white\") +\n  scale_fill_viridis_c(\n    name = \"Area\\nError %\",\n    option = \"magma\"\n  ) +\n  labs(\n    title = \"Area Calculation Errors by County\",\n    subtitle = \"Using geographic coordinates (WGS84) instead of projected CRS\"\n  ) +\n  theme_void()\n\n\n\n\n\n\n\n\nQuestion: Which counties have the largest errors? Why might this be?"
  },
  {
    "objectID": "labs/week-04/scripts/week4_inclass_practice.html#bonus-challenge-combined-analysis-if-time-permits",
    "href": "labs/week-04/scripts/week4_inclass_practice.html#bonus-challenge-combined-analysis-if-time-permits",
    "title": "Week 4 In-Class Practice Exercises",
    "section": "Bonus Challenge: Combined Analysis (If Time Permits)",
    "text": "Bonus Challenge: Combined Analysis (If Time Permits)\nGoal: Combine multiple operations for a complex policy question\n\nResearch Question\nWhich Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\nYour Task: Combine what you’ve learned to identify vulnerable, underserved communities.\nSteps: 1. Get demographic (elderly and income) data for census tracts 2. Identify vulnerable tracts (low income AND high elderly population) 3. Calculate distance to nearest hospital 4. Check which ones are more than 15 miles from a hospital 5. Aggregate to county level 6. Create comprehensive map 7. Create a summary table\n\n# Your code here!"
  },
  {
    "objectID": "labs/week-04/scripts/week4_inclass_practice.html#reflection-questions",
    "href": "labs/week-04/scripts/week4_inclass_practice.html#reflection-questions",
    "title": "Week 4 In-Class Practice Exercises",
    "section": "Reflection Questions",
    "text": "Reflection Questions\nAfter completing these exercises, reflect on:\n\nWhen did you need to transform CRS? Why was this necessary?\nWhat’s the difference between st_filter() and st_intersection()? When would you use each?\nHow does the choice of predicate (st_touches, st_intersects, st_within) change your results?"
  },
  {
    "objectID": "labs/week-04/scripts/week4_inclass_practice.html#summary-of-key-functions-used",
    "href": "labs/week-04/scripts/week4_inclass_practice.html#summary-of-key-functions-used",
    "title": "Week 4 In-Class Practice Exercises",
    "section": "Summary of Key Functions Used",
    "text": "Summary of Key Functions Used\n\n\n\n\n\n\n\n\nFunction\nPurpose\nExample Use\n\n\n\n\nst_filter()\nSelect features by spatial relationship\nFind neighboring counties\n\n\nst_buffer()\nCreate zones around features\nHospital service areas\n\n\nst_intersects()\nTest spatial overlap\nCheck access to services\n\n\nst_disjoint()\nTest spatial separation\nFind rural areas\n\n\nst_join()\nJoin by location\nAdd county info to tracts\n\n\nst_union()\nCombine geometries\nMerge overlapping buffers\n\n\nst_intersection()\nClip geometries\nCalculate overlap areas\n\n\nst_transform()\nChange CRS\nAccurate distance/area calculations\n\n\nst_area()\nCalculate areas\nCounty sizes, coverage\n\n\nst_distance()\nCalculate distances\nDistance to facilities\n\n\n\nImportant Reminder: Always check and standardize CRS when working with spatial data from multiple sources!"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#what-well-cover",
    "href": "labs/week-03/lecture/week3.html#what-well-cover",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "What We’ll Cover",
    "text": "What We’ll Cover\nPart 1: Why Visualization Matters\n\nAnscombe’s Quartet and the limits of summary statistics\nVisualization in policy context\nConnection to algorithmic bias and data ethics\n\nPart 2: Grammar of Graphics\n\nggplot2 fundamentals\nAesthetic mappings and geoms\nLive demonstration\n\nPart 3: Exploratory Data Analysis\n\nEDA workflow and principles\nUnderstanding distributions and relationships\nCritical focus: Data quality and uncertainty\n\nPart 4: Data Joins & Integration\n\nCombining datasets with dplyr joins\n\nPart 5: Hands-On Lab\n\nGuided practice with census data\nCreate publication-ready visualizations\nPractice ethical data communication"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#opening-question",
    "href": "labs/week-03/lecture/week3.html#opening-question",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Opening Question",
    "text": "Opening Question\nThink about Assignment 1:\nYou created tables showing income reliability patterns across counties. But what if you needed to present these findings to:\n\nThe state legislature (2-minute briefing)\nCommunity advocacy groups\nLocal news reporters\n\nDiscussion: How might visual presentation change the impact of your analysis?"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#anscombes-quartet-the-famous-example",
    "href": "labs/week-03/lecture/week3.html#anscombes-quartet-the-famous-example",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Anscombe’s Quartet: The Famous Example",
    "text": "Anscombe’s Quartet: The Famous Example\nFour datasets with identical summary statistics:\n\nSame means (x̄ = 9, ȳ = 7.5)\nSame variances\nSame correlation (r = 0.816)\nSame regression line\n\nBut completely different patterns when visualized"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#the-policy-implications",
    "href": "labs/week-03/lecture/week3.html#the-policy-implications",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "The Policy Implications",
    "text": "The Policy Implications\nWhy this matters for your work:\n\nSummary statistics can hide critical patterns\nOutliers may represent important communities\nRelationships aren’t always linear\nVisual inspection reveals data quality issues\n\nExample: A county with “average” income might have extreme inequality that algorithms would miss without visualization."
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#connecting-week-2-ethical-data-communication",
    "href": "labs/week-03/lecture/week3.html#connecting-week-2-ethical-data-communication",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Connecting Week 2: Ethical Data Communication",
    "text": "Connecting Week 2: Ethical Data Communication\nFrom last week’s algorithmic bias discussion:\nResearch finding: Only 27% of planners warn users about unreliable ACS data - Most planners don’t report margins of error - Many lack training on statistical uncertainty - This violates AICP Code of Ethics\nYour responsibility:\n\nCreate honest, transparent visualizations\nAlways assess and communicate data quality\nConsider who might be harmed by uncertain data"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#bad-visualizations-have-real-consequences",
    "href": "labs/week-03/lecture/week3.html#bad-visualizations-have-real-consequences",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Bad Visualizations Have Real Consequences",
    "text": "Bad Visualizations Have Real Consequences\nCommon problems in government data presentation:\n\nMisleading scales or axes\nCherry-picked time periods\n\nHidden or ignored uncertainty\nMissing context about data reliability\n\nReal impact: The Jurjevich et al. study found that 72% of Portland census tracts had unreliable child poverty estimates, yet planners rarely communicated this uncertainty.\nResult: Poor policy decisions based on misunderstood data"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#the-ggplot2-philosophy",
    "href": "labs/week-03/lecture/week3.html#the-ggplot2-philosophy",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "The ggplot2 Philosophy",
    "text": "The ggplot2 Philosophy\nGrammar of Graphics principles:\nData → Aesthetics → Geometries → Visual\n\nData: Your dataset (census data, survey responses, etc.)\nAesthetics: What variables map to visual properties (x, y, color, size)\nGeometries: How to display the data (points, bars, lines)\nAdditional layers: Scales, themes, facets, annotations"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#basic-ggplot2-structure",
    "href": "labs/week-03/lecture/week3.html#basic-ggplot2-structure",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Basic ggplot2 Structure",
    "text": "Basic ggplot2 Structure\nEvery ggplot has this pattern:\nggplot(data = your_data) +   aes(x = variable1, y = variable2) +   geom_something() +   additional_layers()\nYou build plots by adding layers with +"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#live-demo-basic-scatter-plot",
    "href": "labs/week-03/lecture/week3.html#live-demo-basic-scatter-plot",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Live Demo: Basic Scatter Plot",
    "text": "Live Demo: Basic Scatter Plot"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#aesthetic-mappings-the-key-to-ggplot2",
    "href": "labs/week-03/lecture/week3.html#aesthetic-mappings-the-key-to-ggplot2",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Aesthetic Mappings: The Key to ggplot2",
    "text": "Aesthetic Mappings: The Key to ggplot2\nAesthetics map data to visual properties:\n\nx, y - position\ncolor - point/line color\nfill - area fill color\n\nsize - point/line size\nshape - point shape\nalpha - transparency\n\nImportant: Aesthetics go inside aes(), constants go outside"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#improving-plots-with-labels-and-themes",
    "href": "labs/week-03/lecture/week3.html#improving-plots-with-labels-and-themes",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Improving Plots with Labels and Themes",
    "text": "Improving Plots with Labels and Themes"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#the-eda-mindset",
    "href": "labs/week-03/lecture/week3.html#the-eda-mindset",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "The EDA Mindset",
    "text": "The EDA Mindset\nExploratory Data Analysis is detective work:\n\nWhat does the data look like? (distributions, missing values)\nWhat patterns exist? (relationships, clusters, trends)\n\nWhat’s unusual? (outliers, anomalies, data quality issues)\nWhat questions does this raise? (hypotheses for further investigation)\nHow reliable is this data?\n\nGoal: Understand your data before making decisions or building models"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#eda-workflow-with-data-quality-focus",
    "href": "labs/week-03/lecture/week3.html#eda-workflow-with-data-quality-focus",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "EDA Workflow with Data Quality Focus",
    "text": "EDA Workflow with Data Quality Focus\nEnhanced process for policy analysis:\n\nLoad and inspect - dimensions, variable types, missing data\nAssess reliability - examine margins of error, calculate coefficients of variation\nVisualize distributions - histograms, boxplots for each variable\nExplore relationships - scatter plots, correlations\nIdentify patterns - grouping, clustering, geographical patterns\nQuestion anomalies - investigate outliers and unusual patterns\nDocument limitations - prepare honest communication about data quality"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#understanding-distributions",
    "href": "labs/week-03/lecture/week3.html#understanding-distributions",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Understanding Distributions",
    "text": "Understanding Distributions\nWhy distribution shape matters:\n\nWhat to look for: Skewness, outliers, multiple peaks, gaps"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#boxplots",
    "href": "labs/week-03/lecture/week3.html#boxplots",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Boxplots!",
    "text": "Boxplots!"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#critical-data-quality-through-visualization",
    "href": "labs/week-03/lecture/week3.html#critical-data-quality-through-visualization",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Critical: Data Quality Through Visualization",
    "text": "Critical: Data Quality Through Visualization\nResearch insight: Most planners don’t visualize or communicate uncertainty\n\nPattern: Smaller populations have higher uncertainty Ethical implication: These communities might be systematically undercounted"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#research-based-recommendations-for-planners",
    "href": "labs/week-03/lecture/week3.html#research-based-recommendations-for-planners",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Research-Based Recommendations for Planners",
    "text": "Research-Based Recommendations for Planners\nJurjevich et al. (2018): 5 Essential Guidelines for Using ACS Data\n\nReport the corresponding MOEs of ACS estimates - Always include margin of error values\nInclude a footnote when not reporting MOEs - Explicitly acknowledge omission\n\nProvide context for (un)reliability - Use coefficient of variation (CV):\n\nCV &lt; 12% = reliable (green coding)\nCV 12-40% = somewhat reliable (yellow)\nCV &gt; 40% = unreliable (red coding)\n\nReduce statistical uncertainty - Collapse data detail, aggregate geographies, use multi-year estimates\nAlways conduct statistical significance tests when comparing ACS estimates over time\n\nKey insight: These practices are not just technical best practices—they are ethical requirements under the AICP Code of Ethics"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#eda-for-policy-analysis",
    "href": "labs/week-03/lecture/week3.html#eda-for-policy-analysis",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "EDA for Policy Analysis",
    "text": "EDA for Policy Analysis\nKey questions for census data:\n\nGeographic patterns: Are problems concentrated in certain areas?\nPopulation relationships: How does size affect data quality?\nDemographic patterns: Are certain communities systematically different?\nTemporal trends: How do patterns change over time?\nData integrity: Where might survey bias affect results?\nReliability assessment: Which estimates should we trust?"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#why-join-data",
    "href": "labs/week-03/lecture/week3.html#why-join-data",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Why Join Data?",
    "text": "Why Join Data?\nTo combining datasets of course:\n\nCensus demographics + Economic indicators\nSurvey responses + Geographic boundaries\n\nCurrent data + Historical trends\nAdministrative records + Survey data"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#types-of-joins-tabular",
    "href": "labs/week-03/lecture/week3.html#types-of-joins-tabular",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Types of Joins (tabular)",
    "text": "Types of Joins (tabular)\nFour main types in dplyr:\n\nleft_join() - Keep all rows from left dataset\nright_join() - Keep all rows from right dataset\n\ninner_join() - Keep only rows that match in both\nfull_join() - Keep all rows from both datasets\n\nMost common: left_join() to add columns to your main dataset"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#live-demo-joining-census-tables",
    "href": "labs/week-03/lecture/week3.html#live-demo-joining-census-tables",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Live Demo: Joining Census Tables",
    "text": "Live Demo: Joining Census Tables\n\n\n# A tibble: 6 × 6\n  GEOID NAME                    median_income income_moe college_pop college_moe\n  &lt;chr&gt; &lt;chr&gt;                           &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1 42001 Adams County, Pennsylv…         78975       3334       10195         761\n2 42003 Allegheny County, Penn…         72537        869      229538        3311\n3 42005 Armstrong County, Penn…         61011       2202        6171         438\n4 42007 Beaver County, Pennsyl…         67194       1531       22588        1012\n5 42009 Bedford County, Pennsy…         58337       2606        3396         307\n6 42011 Berks County, Pennsylv…         74617       1191       50120        1654"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#checking-join-results-and-data-quality",
    "href": "labs/week-03/lecture/week3.html#checking-join-results-and-data-quality",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Checking Join Results and Data Quality",
    "text": "Checking Join Results and Data Quality\nAlways verify joins AND assess combined reliability:\n\n\nIncome data rows: 67 \n\n\nEducation data rows: 67 \n\n\nCombined data rows: 67 \n\n\n# A tibble: 1 × 2\n  missing_income missing_education\n           &lt;int&gt;             &lt;int&gt;\n1              0                 0\n\n\n# A tibble: 6 × 3\n  NAME                           income_cv college_cv\n  &lt;chr&gt;                              &lt;dbl&gt;      &lt;dbl&gt;\n1 Adams County, Pennsylvania          4.22       7.46\n2 Allegheny County, Pennsylvania      1.20       1.44\n3 Armstrong County, Pennsylvania      3.61       7.10\n4 Beaver County, Pennsylvania         2.28       4.48\n5 Bedford County, Pennsylvania        4.47       9.04\n6 Berks County, Pennsylvania          1.60       3.30"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#lab-structure-for-today",
    "href": "labs/week-03/lecture/week3.html#lab-structure-for-today",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Lab Structure for Today",
    "text": "Lab Structure for Today\nYou’ll work through six exercises:\n\nFinding Census Variables - Learn to search for the data you need\nSingle Variable EDA - Explore distributions and identify outliers\nTwo Variable Relationships - Create meaningful scatter plots\nData Quality Visualization - Practice ethical uncertainty communication\nMultiple Variables - Color, faceting, and complex relationships\nData Integration - Join datasets and create publication-ready visualizations"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#skills-youll-practice",
    "href": "labs/week-03/lecture/week3.html#skills-youll-practice",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Skills You’ll Practice",
    "text": "Skills You’ll Practice\nggplot2 fundamentals:\n\nScatter plots, histograms, boxplots\nAesthetic mappings and customization\nProfessional themes and labels\n\nEDA workflow:\n\nDistribution analysis\nOutlier detection\n\nPattern identification\n\nEthical data practice:\n\nVisualizing and reporting margins of error\nUsing coefficient of variation to assess reliability"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#connection-to-professional-ethics",
    "href": "labs/week-03/lecture/week3.html#connection-to-professional-ethics",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Connection to Professional Ethics",
    "text": "Connection to Professional Ethics\nBy the end of today, you’ll be able to:\n\nVisually assess data quality issues\nCreate compelling presentations of demographic patterns\nCommunicate statistical uncertainty ethically and clearly\nIntegrate multiple data sources"
  },
  {
    "objectID": "labs/week-03/lecture/week3.html#questions-before-we-begin",
    "href": "labs/week-03/lecture/week3.html#questions-before-we-begin",
    "title": "Data Visualization & Exploratory Analysis",
    "section": "Questions Before We Begin?",
    "text": "Questions Before We Begin?\nReady for hands-on practice?\nRemember: Today’s skills build directly on Week 1-2 foundations:\n\nSame dplyr functions, now with visualization\nSame census data concepts, now with multiple tables\n\nLet’s create some beautiful graphs"
  },
  {
    "objectID": "instructions_week1.html",
    "href": "instructions_week1.html",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Welcome to MUSA 5080! This guide will help you set up your personal portfolio repository for the semester.\n\n\nBy the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey\n\n\n\n\nThis is what you are building: Dr. Delmelle’s sample portfolio\n\n\n\n\nBefore starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed\n\n\n\n\n\nYou should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL\n\n\n\n\n\nIf you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!\n\n\n\n\nEach week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes\n\n\n\n\n\n\nWait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version\n\n\n\n\n\n\nCommit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis\n\n\n\n\n\nQuarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial\n\n\n\n\nDuring Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too\n\n\n\nBefore submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "instructions_week1.html#what-youre-building",
    "href": "instructions_week1.html#what-youre-building",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "By the end of this setup, you’ll have: - Your own portfolio repository on GitHub - live website showcasing your work - A place to document your learning journey"
  },
  {
    "objectID": "instructions_week1.html#example",
    "href": "instructions_week1.html#example",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "This is what you are building: Dr. Delmelle’s sample portfolio"
  },
  {
    "objectID": "instructions_week1.html#prerequisites",
    "href": "instructions_week1.html#prerequisites",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before starting, make sure you have: - [ ] A GitHub account (create one here if needed) - [ ] Quarto installed on your computer (download here) - [ ] R and RStudio installed"
  },
  {
    "objectID": "instructions_week1.html#step-by-step-setup",
    "href": "instructions_week1.html#step-by-step-setup",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "You should already be in your personal repository (created when you accepted the GitHub Classroom assignment). Now let’s personalize it!\n\n\n\nClick on the _quarto.yml file\nClick the pencil icon (✏️) to edit\nChange \"Your Name - MUSA 5080 Portfolio\" to include your actual name\nExample: \"Jane Smith - MUSA 5080 Portfolio\"\nClick “Commit changes” at the bottom\n\n\n\n\n\nClick on the index.qmd file\nClick the pencil icon (✏️) to edit\nUpdate the “About Me” section with your information:\n\nYour name and background\nYour email address\nYour GitHub username\nWhy you’re taking this course\n\nClick “Commit changes”\n\n\n\n\n\nNavigate to the weekly-notes folder\nClick on week-01-notes.qmd\nClick the pencil icon (✏️) to edit\nFill in your notes from the first class\nClick “Commit changes”\n\n\n\n\n\nThis step makes your portfolio visible as a live website!\n\nGo to Settings: Click the “Settings” tab at the top of your repository\nFind Pages: Scroll down and click “Pages” in the left sidebar\nConfigure Source:\n\nSource: Select “Deploy from a branch”\nBranch: Select “main”\nFolder: Select “/ docs”\n\nSave: Click “Save”\nWait: GitHub will show a message that your site is being built (this takes 1-5 minutes)\n\n\n\n\n\nFind Your URL: After a few minutes, GitHub will show your website URL at the top of the Pages settings\n\nIt will look like: https://yourusername.github.io/repository-name\n\nVisit Your Site: Click the link to see your live portfolio!\nBookmark It: Save this URL - you’ll submit it to Canvas\n\n\n\n\n\nCopy your live website URL\nGo to the Canvas assignment\nSubmit your URL"
  },
  {
    "objectID": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "href": "instructions_week1.html#working-on-your-portfolio-locally-optional-but-recommended",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "If you want to work on your computer and see changes before publishing:\n\n\n# Replace [your-repo-url] with your actual repository URL\ngit clone [your-repo-url]\ncd [your-repository-name]\n\n\n\n# Edit your files using RStudio\n# Preview your changes:\nquarto render\nquarto preview\n\n# When ready, save your changes:\ngit add .\ngit commit -m \"Update portfolio\"\ngit push\nYour live website will automatically update when you push changes!"
  },
  {
    "objectID": "instructions_week1.html#weekly-workflow",
    "href": "instructions_week1.html#weekly-workflow",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Each week you’ll: 1. Create a new file: weekly-notes/week-XX-notes.qmd 2. Copy the template from week-01-notes.qmd 3. Fill in your reflections and key concepts 4. Commit and push your changes"
  },
  {
    "objectID": "instructions_week1.html#troubleshooting",
    "href": "instructions_week1.html#troubleshooting",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Wait longer: GitHub Pages can take up to 10 minutes to build\nCheck Actions tab: Look for any red X marks indicating build failures\nVerify Pages settings: Make sure you selected “main” branch and “/docs” folder\n\n\n\n\n\nCheck permissions: Make sure you’re in YOUR repository, not the template\nSign in: Ensure you’re signed into GitHub\n\n\n\n\n\nCheck YAML syntax: Make sure your _quarto.yml file has proper formatting\nVerify file names: Files should end in .qmd not .md\nLook at error messages: The Actions tab will show specific error details\n\n\n\n\n\nDon’t panic! Every change is tracked in Git\nSee history: Click the “History” button on any file to see previous versions\nRevert changes: You can always go back to a previous version"
  },
  {
    "objectID": "instructions_week1.html#pro-tips",
    "href": "instructions_week1.html#pro-tips",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Commit often: Save your work frequently with descriptive commit messages\nUse branches: For major changes, create a new branch and merge when ready\nPreview locally: Use quarto preview to see changes before publishing\nKeep it professional: This portfolio can be shared with future employers!\nDocument everything: Good documentation is as important as good analysis"
  },
  {
    "objectID": "instructions_week1.html#additional-resources",
    "href": "instructions_week1.html#additional-resources",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Quarto Documentation\nGitHub Docs\nMarkdown Guide\nGit Tutorial"
  },
  {
    "objectID": "instructions_week1.html#getting-help",
    "href": "instructions_week1.html#getting-help",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "During Class: - Raise your hand for immediate help - Work with classmates - collaboration is encouraged for setup!\nOutside Class: - Office Hours: Mondays 1:30-3:00 PM - Email: delmelle@design.upenn.edu - GitHub Issues: Create an issue in your repository for technical problems - Canvas Discussion: Post questions others might have too"
  },
  {
    "objectID": "instructions_week1.html#checklist",
    "href": "instructions_week1.html#checklist",
    "title": "Portfolio Setup Instructions",
    "section": "",
    "text": "Before submitting, make sure you’ve: - [ ] Customized _quarto.yml with your name - [ ] Updated index.qmd with your information - [ ] Completed Week 1 notes - [ ] Enabled GitHub Pages - [ ] Verified your website loads correctly - [ ] Submitted your URL to Canvas\n\nNeed help? Don’t struggle alone - reach out during office hours (mine + TAs) or in class!"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Curriculum Vitae: Mohamad Al Abbas",
    "section": "",
    "text": "PhD in Demography, University of Pennsylvania, Expected May 2028\n\nMA in International Affairs, Lebanese American University, May 2023\nBE in Electrical Engineering, Lebanese American University, August 2019\n\n\n\n\n\nTeaching Assistant, University of Pennsylvania – Graduate School of Education (2025–present)\n\nLead lab sessions on STATA for Principles of Monitoring and Evaluation, a graduate-level course\n\nResearch Assistant, University of Pennsylvania – PIRE Project (2023–present)\n\nNSF-funded project on environmental degradation and child development\n\nPaper: Born in a Haze (forest fires & health outcomes in Indonesia)\n\n\nStatistical Consultant, Department of Sociology, University of Pennsylvania (2023–present)\n\nSupport PhD students in Sociology and Demography\n\nConduct workshops on machine learning techniques\n\n\nResearch Intern, UNDP Office of Audit & Investigations (2022–2023)\n\nDrafted investigation plans, reports, and conducted interviews\n\n\nResearch Intern, Center for International Policy – Technology Policy Program (2022–2023)\n\nDesigned Social Media Harms tracker\n\nAuthored briefs and attended U.S. legislative hearings\n\n\nResearch Assistant – Individual Contractor, UN ESCWA (2022)\n\nBuilt/improved social demographic database (29% → 83% completion)\n\n\nMonitoring & Evaluation Officer, LAU – Graduate Studies & Research (2019–2021)\n\nGraduate Teaching Assistant, LAU – Electrical & Computer Engineering (2019–2021)\n\n\n\n\n\nBenjamin Franklin Fellowship, University of Pennsylvania (2024–2028)\n\nDean’s Fellowship, University of Pennsylvania (2023–2024)\n\nMEPI – Tomorrow’s Leaders Graduate Fellowship, LAU (2021–2023)\n\nOutstanding Researcher Award, LAU (2021)\n\nBest Presentation Award, ICIET Japan (2022)\n\n\n\n\n\nProgramming: R, Python, Stata, Java, Assembly\n\nMachine Learning: NLP, RL, Neural Networks (CNN & RNN), Decision Trees\n\nSoftware: Tableau, MATLAB, SPSS, LaTeX\n\nLanguages: English (Bilingual), Arabic (Native)\n\n\n\n\n\nEmail: ma96@upenn.edu\n\nLinkedIn: linkedin.com/in/mohammadalabbas96\n\nORCID: 0000-0002-2084-8856"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Curriculum Vitae: Mohamad Al Abbas",
    "section": "",
    "text": "PhD in Demography, University of Pennsylvania, Expected May 2028\n\nMA in International Affairs, Lebanese American University, May 2023\nBE in Electrical Engineering, Lebanese American University, August 2019"
  },
  {
    "objectID": "cv.html#experience",
    "href": "cv.html#experience",
    "title": "Curriculum Vitae: Mohamad Al Abbas",
    "section": "",
    "text": "Teaching Assistant, University of Pennsylvania – Graduate School of Education (2025–present)\n\nLead lab sessions on STATA for Principles of Monitoring and Evaluation, a graduate-level course\n\nResearch Assistant, University of Pennsylvania – PIRE Project (2023–present)\n\nNSF-funded project on environmental degradation and child development\n\nPaper: Born in a Haze (forest fires & health outcomes in Indonesia)\n\n\nStatistical Consultant, Department of Sociology, University of Pennsylvania (2023–present)\n\nSupport PhD students in Sociology and Demography\n\nConduct workshops on machine learning techniques\n\n\nResearch Intern, UNDP Office of Audit & Investigations (2022–2023)\n\nDrafted investigation plans, reports, and conducted interviews\n\n\nResearch Intern, Center for International Policy – Technology Policy Program (2022–2023)\n\nDesigned Social Media Harms tracker\n\nAuthored briefs and attended U.S. legislative hearings\n\n\nResearch Assistant – Individual Contractor, UN ESCWA (2022)\n\nBuilt/improved social demographic database (29% → 83% completion)\n\n\nMonitoring & Evaluation Officer, LAU – Graduate Studies & Research (2019–2021)\n\nGraduate Teaching Assistant, LAU – Electrical & Computer Engineering (2019–2021)"
  },
  {
    "objectID": "cv.html#fellowships-awards",
    "href": "cv.html#fellowships-awards",
    "title": "Curriculum Vitae: Mohamad Al Abbas",
    "section": "",
    "text": "Benjamin Franklin Fellowship, University of Pennsylvania (2024–2028)\n\nDean’s Fellowship, University of Pennsylvania (2023–2024)\n\nMEPI – Tomorrow’s Leaders Graduate Fellowship, LAU (2021–2023)\n\nOutstanding Researcher Award, LAU (2021)\n\nBest Presentation Award, ICIET Japan (2022)"
  },
  {
    "objectID": "cv.html#skills",
    "href": "cv.html#skills",
    "title": "Curriculum Vitae: Mohamad Al Abbas",
    "section": "",
    "text": "Programming: R, Python, Stata, Java, Assembly\n\nMachine Learning: NLP, RL, Neural Networks (CNN & RNN), Decision Trees\n\nSoftware: Tableau, MATLAB, SPSS, LaTeX\n\nLanguages: English (Bilingual), Arabic (Native)"
  },
  {
    "objectID": "cv.html#contact",
    "href": "cv.html#contact",
    "title": "Curriculum Vitae: Mohamad Al Abbas",
    "section": "",
    "text": "Email: ma96@upenn.edu\n\nLinkedIn: linkedin.com/in/mohammadalabbas96\n\nORCID: 0000-0002-2084-8856"
  },
  {
    "objectID": "Assignments/Assignment_1/assignment1_template.html",
    "href": "Assignments/Assignment_1/assignment1_template.html",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the California Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues.\n\n\n\n\nApply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "Assignments/Assignment_1/assignment1_template.html#scenario",
    "href": "Assignments/Assignment_1/assignment1_template.html#scenario",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "You are a data analyst for the California Department of Human Services. The department is considering implementing an algorithmic system to identify communities that should receive priority for social service funding and outreach programs. Your supervisor has asked you to evaluate the quality and reliability of available census data to inform this decision.\nDrawing on our Week 2 discussion of algorithmic bias, you need to assess not just what the data shows, but how reliable it is and what communities might be affected by data quality issues."
  },
  {
    "objectID": "Assignments/Assignment_1/assignment1_template.html#learning-objectives",
    "href": "Assignments/Assignment_1/assignment1_template.html#learning-objectives",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "",
    "text": "Apply dplyr functions to real census data for policy analysis\nEvaluate data quality using margins of error\nConnect technical analysis to algorithmic decision-making\nIdentify potential equity implications of data reliability issues\nCreate professional documentation for policy stakeholders"
  },
  {
    "objectID": "Assignments/Assignment_1/assignment1_template.html#data-retrieval",
    "href": "Assignments/Assignment_1/assignment1_template.html#data-retrieval",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.1 Data Retrieval",
    "text": "2.1 Data Retrieval\nYour Task: Use get_acs() to retrieve county-level data for your chosen state.\nRequirements: - Geography: county level - Variables: median household income (B19013_001) and total population (B01003_001)\n- Year: 2022 - Survey: acs5 - Output format: wide\nHint: Remember to give your variables descriptive names using the variables = c(name = \"code\") syntax.\n\n# Write your get_acs() code here\ncounty_vars &lt;- c( med_hh_income = \"B19013_001\", total_pop     = \"B01003_001\")\n\ncounty_raw &lt;- get_acs(geography = \"county\", state =  my_state, survey = \"acs5\", year = 2022, variables = county_vars, output = \"wide\")\n\n# Clean the county names to remove state name and \"County\" \n# Hint: use mutate() with str_remove()\n\ncounty &lt;- county_raw %&gt;%\n  mutate(\n    county_name = str_remove(NAME, paste0(\", \", my_state)),\n    county_name = str_remove(county_name, \" County$\")\n  ) %&gt;%\n  select(GEOID, county_name, med_hh_incomeE, med_hh_incomeM, total_popE, total_popM)\n\n# Display the first few rows\nhead(county)\n\n# A tibble: 6 × 6\n  GEOID county_name med_hh_incomeE med_hh_incomeM total_popE total_popM\n  &lt;chr&gt; &lt;chr&gt;                &lt;dbl&gt;          &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;\n1 06001 Alameda             122488           1231    1663823         NA\n2 06003 Alpine              101125          17442       1515        206\n3 06005 Amador               74853           6048      40577         NA\n4 06007 Butte                66085           2261     213605         NA\n5 06009 Calaveras            77526           3875      45674         NA\n6 06011 Colusa               69619           5745      21811         NA"
  },
  {
    "objectID": "Assignments/Assignment_1/assignment1_template.html#data-quality-assessment",
    "href": "Assignments/Assignment_1/assignment1_template.html#data-quality-assessment",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.2 Data Quality Assessment",
    "text": "2.2 Data Quality Assessment\nYour Task: Calculate margin of error percentages and create reliability categories.\nRequirements: - Calculate MOE percentage: (margin of error / estimate) * 100 - Create reliability categories: - High Confidence: MOE &lt; 5% - Moderate Confidence: MOE 5-10%\n- Low Confidence: MOE &gt; 10% - Create a flag for unreliable estimates (MOE &gt; 10%)\nHint: Use mutate() with case_when() for the categories.\n\n# Calculate MOE percentage and reliability categories using mutate()\ncounty_reliability &lt;- county %&gt;%\n  mutate(\n    moe_percentage = round((med_hh_incomeM/med_hh_incomeE) * 100, 2),\n    Reliability = case_when(\n      moe_percentage &lt; 5 ~ \"High Confidence\",\n      moe_percentage &gt;= 5 & moe_percentage &lt;= 10 ~ \"Moderate Confidence\",\n      moe_percentage &gt; 10 ~ \"Low Confidence\"\n    )\n  )\n\n# Create a summary showing count of counties in each reliability category\n# Hint: use count() and mutate() to add percentages\n\nreliability_summary &lt;- county_reliability %&gt;%\n  count(Reliability, name = \"Count\") %&gt;%\n  mutate(Proportion = round(100 * Count / sum(Count), 1))\n\nkable(reliability_summary, caption = \"County Income Reliability Categories\")\n\n\nCounty Income Reliability Categories\n\n\nReliability\nCount\nProportion\n\n\n\n\nHigh Confidence\n41\n70.7\n\n\nLow Confidence\n5\n8.6\n\n\nModerate Confidence\n12\n20.7"
  },
  {
    "objectID": "Assignments/Assignment_1/assignment1_template.html#high-uncertainty-counties",
    "href": "Assignments/Assignment_1/assignment1_template.html#high-uncertainty-counties",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "2.3 High Uncertainty Counties",
    "text": "2.3 High Uncertainty Counties\nYour Task: Identify the 5 counties with the highest MOE percentages.\nRequirements: - Sort by MOE percentage (highest first) - Select the top 5 counties - Display: county name, median income, margin of error, MOE percentage, reliability category - Format as a professional table using kable()\nHint: Use arrange(), slice(), and select() functions.\n\n# Create table of top 5 counties by MOE percentage\n\ntop5 &lt;- county_reliability %&gt;%\n  arrange(desc(moe_percentage)) %&gt;%\n  slice(1:5) %&gt;%\n  select(\n    county_name,\n    med_hh_incomeE,\n    med_hh_incomeM,\n    moe_percentage,\n    Reliability\n  )\n  \n# Format as table with kable() - include appropriate column names and caption\n\nkable(\n  top5,\n  caption = \"Top 5 Counties by Median Household Income MOE Percentage\",\n  col.names = c(\"County\", \"Median Income\", \"Margin of Error\", \"MOE %\", \"Reliability Category\"),\n  digits = 2,\n  align = c(\"l\", \"c\", \"c\", \"c\", \"c\"),\n  format.args = list(big.mark = \",\")\n)\n\n\nTop 5 Counties by Median Household Income MOE Percentage\n\n\n\n\n\n\n\n\n\nCounty\nMedian Income\nMargin of Error\nMOE %\nReliability Category\n\n\n\n\nMono\n82,038\n15,388\n18.76\nLow Confidence\n\n\nAlpine\n101,125\n17,442\n17.25\nLow Confidence\n\n\nSierra\n61,108\n9,237\n15.12\nLow Confidence\n\n\nTrinity\n47,317\n5,890\n12.45\nLow Confidence\n\n\nPlumas\n67,885\n7,772\n11.45\nLow Confidence\n\n\n\n\n\nData Quality Commentary:\nAll five of these counties are among the lowest-density areas in California. Because their populations are so small, the ACS relies on limited samples to generate median income estimates, which introduces greater variability. This explains the large disparities and the relatively high margins of error (11–19%). As a result, algorithms that classify or rank counties using these figures could produce erroneous outcomes if they neglect the margins of error. For example, Alpine County appears to have a median income exceeding $100,000, but its margin of error is more than $17,000 an uncertainty that is enormous relative to its ~1,000 residents. This is both a sampling size and representativeness issue, highlighting how misleading the raw point estimate can be without MOE context."
  },
  {
    "objectID": "Assignments/Assignment_1/assignment1_template.html#focus-area-selection",
    "href": "Assignments/Assignment_1/assignment1_template.html#focus-area-selection",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.1 Focus Area Selection",
    "text": "3.1 Focus Area Selection\nYour Task: Select 2-3 counties from your reliability analysis for detailed tract-level study.\nStrategy: Choose counties that represent different reliability levels (e.g., 1 high confidence, 1 moderate, 1 low confidence) to compare how data quality varies.\n\n# Use filter() to select 2-3 counties from your county_reliability data\n# Store the selected counties in a variable called selected_counties\n\nselected_counties &lt;- bind_rows(\n  county_reliability %&gt;%\n    filter(Reliability == \"High Confidence\") %&gt;%\n    slice(1),\n  county_reliability %&gt;%\n    filter(Reliability == \"Moderate Confidence\") %&gt;%\n    slice(1),\n  county_reliability %&gt;%\n    filter(Reliability == \"Low Confidence\") %&gt;%\n    slice(1),\n) %&gt;%\n  select(\n    County = county_name,\n    `Median Income` = med_hh_incomeE,\n    `Margin of Error` = med_hh_incomeM,\n    `MOE %` = moe_percentage,\n    Reliability = Reliability,\n    Population = total_popE\n  )\n\n# Display the selected counties with their key characteristics\n# Show: county name, median income, MOE percentage, reliability category\nkable(\n  selected_counties,\n  caption = \"Largest County by Population in Each Reliability Category\",\n  align = c(\"l\", \"r\", \"r\", \"r\", \"l\", \"r\"),\n  format.args = list(big.mark = \",\")\n)\n\n\nLargest County by Population in Each Reliability Category\n\n\n\n\n\n\n\n\n\n\nCounty\nMedian Income\nMargin of Error\nMOE %\nReliability\nPopulation\n\n\n\n\nAlameda\n122,488\n1,231\n1.00\nHigh Confidence\n1,663,823\n\n\nAmador\n74,853\n6,048\n8.08\nModerate Confidence\n40,577\n\n\nAlpine\n101,125\n17,442\n17.25\nLow Confidence\n1,515\n\n\n\n\n\nComment on the output: Because I specified no randomness in how the slice is sampling the data across the reliability categories, it quite literally picked the first match it had. Which is why all three are arranged alphabetically. On the positive side we still have atleast 1 sample from each category and Alpine is still with us :)! The lowest county by density in California."
  },
  {
    "objectID": "Assignments/Assignment_1/assignment1_template.html#tract-level-demographics",
    "href": "Assignments/Assignment_1/assignment1_template.html#tract-level-demographics",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.2 Tract-Level Demographics",
    "text": "3.2 Tract-Level Demographics\nYour Task: Get demographic data for census tracts in your selected counties.\nRequirements: - Geography: tract level - Variables: white alone (B03002_003), Black/African American (B03002_004), Hispanic/Latino (B03002_012), total population (B03002_001) - Use the same state and year as before - Output format: wide - Challenge: You’ll need county codes, not names. Look at the GEOID patterns in your county data for hints.\n\n# Define your race/ethnicity variables with descriptive names\n\nrace_vars &lt;- c(\n  total    = \"B03002_001\",\n  white    = \"B03002_003\",\n  black    = \"B03002_004\",\n  hispanic = \"B03002_012\"\n)\n\n\n# Use get_acs() to retrieve tract-level data\n# Hint: You may need to specify county codes in the county parameter\n\nselected_geoid &lt;- c(\"06001\",\"06003\",\"06005\")  # Alameda, Alpine, Amador\n\ncounty_codes &lt;- stringr::str_sub(selected_geoid, 3, 5)  # -&gt; \"001\",\"003\",\"005\"\n\ntract_raw &lt;- get_acs(\n  geography = \"tract\",\n  state     = my_state,\n  county    = county_codes,\n  survey    = \"acs5\",\n  year      = 2022,\n  variables = race_vars,\n  output    = \"wide\"\n)\n\n# Calculate percentage of each group using mutate()\n# Create percentages for white, Black, and Hispanic populations\n\ntract_percent &lt;- tract_raw %&gt;%\n  mutate(\n    county_code  = substr(GEOID, 3, 5),\n    pct_white    = 100 * (whiteE    / totalE),\n    pct_black    = 100 * (blackE    / totalE),\n    pct_hispanic = 100 * (hispanicE / totalE),\n    # Make a readable tract label and remove state/country suffixes\n    tract_label  = stringr::str_remove(NAME, paste0(\", \", my_state)),\n    tract_label  = stringr::str_remove(tract_label, \", United States$\")\n  ) %&gt;%\n  left_join(\n    county %&gt;%\n      filter(GEOID %in% selected_geoid) %&gt;%\n      transmute(\n        county_code = substr(GEOID, 3, 5),\n        county_name\n      ),\n    by = \"county_code\"\n  ) %&gt;%\n  select(\n    GEOID, county_name, tract_label,\n    totalE, whiteE, blackE, hispanicE,\n    totalM, whiteM, blackM, hispanicM,\n    pct_white, pct_black, pct_hispanic\n  )\n\n# Add readable tract and county name columns using str_extract() or similar\n\nkable(\n  tract_percent %&gt;%\n    select(\n      County = county_name,\n      Tract  = tract_label,\n      `White (%)`    = pct_white,\n      `Black (%)`    = pct_black,\n      `Hispanic (%)` = pct_hispanic\n    ) %&gt;%\n    arrange(County, Tract),\n  caption = \"Tract-Level Racial/Ethnic Composition in Selected Counties\",\n  align = c(\"l\",\"l\",\"r\",\"r\",\"r\"),\n  format.args = list(big.mark = \",\", digits = 1)\n)\n\n\nTract-Level Racial/Ethnic Composition in Selected Counties\n\n\n\n\n\n\n\n\n\nCounty\nTract\nWhite (%)\nBlack (%)\nHispanic (%)\n\n\n\n\nAlameda\nCensus Tract 4001; Alameda County; California\n69\n5.23\n5\n\n\nAlameda\nCensus Tract 4002; Alameda County; California\n70\n1.77\n7\n\n\nAlameda\nCensus Tract 4003; Alameda County; California\n59\n9.41\n10\n\n\nAlameda\nCensus Tract 4004; Alameda County; California\n64\n11.66\n7\n\n\nAlameda\nCensus Tract 4005; Alameda County; California\n41\n25.15\n15\n\n\nAlameda\nCensus Tract 4006; Alameda County; California\n44\n26.19\n7\n\n\nAlameda\nCensus Tract 4007; Alameda County; California\n48\n23.14\n10\n\n\nAlameda\nCensus Tract 4008; Alameda County; California\n51\n14.83\n9\n\n\nAlameda\nCensus Tract 4009; Alameda County; California\n37\n24.33\n19\n\n\nAlameda\nCensus Tract 4010; Alameda County; California\n33\n23.55\n22\n\n\nAlameda\nCensus Tract 4011; Alameda County; California\n42\n16.36\n12\n\n\nAlameda\nCensus Tract 4012; Alameda County; California\n57\n7.98\n13\n\n\nAlameda\nCensus Tract 4013; Alameda County; California\n36\n28.75\n9\n\n\nAlameda\nCensus Tract 4014; Alameda County; California\n26\n31.82\n22\n\n\nAlameda\nCensus Tract 4015; Alameda County; California\n22\n54.89\n15\n\n\nAlameda\nCensus Tract 4016; Alameda County; California\n19\n32.79\n29\n\n\nAlameda\nCensus Tract 4017; Alameda County; California\n45\n13.80\n17\n\n\nAlameda\nCensus Tract 4018; Alameda County; California\n24\n37.98\n27\n\n\nAlameda\nCensus Tract 4022; Alameda County; California\n32\n27.69\n23\n\n\nAlameda\nCensus Tract 4024; Alameda County; California\n13\n54.59\n9\n\n\nAlameda\nCensus Tract 4025; Alameda County; California\n16\n43.68\n12\n\n\nAlameda\nCensus Tract 4026; Alameda County; California\n9\n38.46\n6\n\n\nAlameda\nCensus Tract 4027; Alameda County; California\n26\n37.42\n17\n\n\nAlameda\nCensus Tract 4028.01; Alameda County; California\n27\n43.36\n5\n\n\nAlameda\nCensus Tract 4028.02; Alameda County; California\n23\n42.87\n4\n\n\nAlameda\nCensus Tract 4029; Alameda County; California\n20\n22.32\n10\n\n\nAlameda\nCensus Tract 4030; Alameda County; California\n9\n5.04\n2\n\n\nAlameda\nCensus Tract 4031; Alameda County; California\n40\n16.64\n12\n\n\nAlameda\nCensus Tract 4033.01; Alameda County; California\n10\n2.79\n2\n\n\nAlameda\nCensus Tract 4033.02; Alameda County; California\n52\n4.81\n6\n\n\nAlameda\nCensus Tract 4034.01; Alameda County; California\n42\n16.45\n9\n\n\nAlameda\nCensus Tract 4034.02; Alameda County; California\n43\n15.17\n12\n\n\nAlameda\nCensus Tract 4035.01; Alameda County; California\n38\n15.09\n8\n\n\nAlameda\nCensus Tract 4035.02; Alameda County; California\n49\n11.52\n17\n\n\nAlameda\nCensus Tract 4036; Alameda County; California\n38\n23.22\n17\n\n\nAlameda\nCensus Tract 4037.01; Alameda County; California\n42\n21.73\n10\n\n\nAlameda\nCensus Tract 4037.02; Alameda County; California\n55\n16.49\n11\n\n\nAlameda\nCensus Tract 4038; Alameda County; California\n62\n15.12\n7\n\n\nAlameda\nCensus Tract 4039; Alameda County; California\n63\n9.27\n11\n\n\nAlameda\nCensus Tract 4040; Alameda County; California\n51\n11.99\n20\n\n\nAlameda\nCensus Tract 4041.01; Alameda County; California\n74\n8.39\n4\n\n\nAlameda\nCensus Tract 4041.02; Alameda County; California\n53\n5.74\n11\n\n\nAlameda\nCensus Tract 4042; Alameda County; California\n67\n4.93\n7\n\n\nAlameda\nCensus Tract 4043; Alameda County; California\n75\n1.28\n6\n\n\nAlameda\nCensus Tract 4044; Alameda County; California\n59\n2.44\n3\n\n\nAlameda\nCensus Tract 4045.01; Alameda County; California\n66\n5.16\n6\n\n\nAlameda\nCensus Tract 4045.02; Alameda County; California\n66\n5.78\n5\n\n\nAlameda\nCensus Tract 4046; Alameda County; California\n58\n5.33\n4\n\n\nAlameda\nCensus Tract 4047; Alameda County; California\n68\n4.57\n9\n\n\nAlameda\nCensus Tract 4048; Alameda County; California\n53\n11.48\n10\n\n\nAlameda\nCensus Tract 4049; Alameda County; California\n54\n6.06\n16\n\n\nAlameda\nCensus Tract 4050; Alameda County; California\n57\n13.45\n10\n\n\nAlameda\nCensus Tract 4051; Alameda County; California\n66\n5.66\n7\n\n\nAlameda\nCensus Tract 4052; Alameda County; California\n34\n13.84\n6\n\n\nAlameda\nCensus Tract 4053.01; Alameda County; California\n49\n13.93\n17\n\n\nAlameda\nCensus Tract 4053.02; Alameda County; California\n34\n23.29\n6\n\n\nAlameda\nCensus Tract 4054.01; Alameda County; California\n25\n12.54\n24\n\n\nAlameda\nCensus Tract 4054.02; Alameda County; California\n20\n15.27\n23\n\n\nAlameda\nCensus Tract 4055; Alameda County; California\n21\n19.51\n15\n\n\nAlameda\nCensus Tract 4056; Alameda County; California\n31\n17.21\n22\n\n\nAlameda\nCensus Tract 4057; Alameda County; California\n15\n27.97\n25\n\n\nAlameda\nCensus Tract 4058; Alameda County; California\n12\n20.33\n21\n\n\nAlameda\nCensus Tract 4059.01; Alameda County; California\n5\n17.11\n34\n\n\nAlameda\nCensus Tract 4059.02; Alameda County; California\n12\n7.46\n29\n\n\nAlameda\nCensus Tract 4060; Alameda County; California\n18\n19.22\n20\n\n\nAlameda\nCensus Tract 4061; Alameda County; California\n18\n8.87\n47\n\n\nAlameda\nCensus Tract 4062.01; Alameda County; California\n6\n13.70\n48\n\n\nAlameda\nCensus Tract 4062.02; Alameda County; California\n7\n12.51\n62\n\n\nAlameda\nCensus Tract 4063; Alameda County; California\n11\n19.32\n35\n\n\nAlameda\nCensus Tract 4064; Alameda County; California\n23\n16.55\n31\n\n\nAlameda\nCensus Tract 4065; Alameda County; California\n12\n13.26\n49\n\n\nAlameda\nCensus Tract 4066.01; Alameda County; California\n23\n28.74\n24\n\n\nAlameda\nCensus Tract 4066.02; Alameda County; California\n14\n22.32\n34\n\n\nAlameda\nCensus Tract 4067; Alameda County; California\n46\n6.80\n17\n\n\nAlameda\nCensus Tract 4068; Alameda County; California\n28\n20.98\n24\n\n\nAlameda\nCensus Tract 4069; Alameda County; California\n40\n16.97\n8\n\n\nAlameda\nCensus Tract 4070; Alameda County; California\n18\n15.64\n36\n\n\nAlameda\nCensus Tract 4071.01; Alameda County; California\n12\n18.48\n40\n\n\nAlameda\nCensus Tract 4071.02; Alameda County; California\n7\n23.69\n37\n\n\nAlameda\nCensus Tract 4072; Alameda County; California\n14\n3.22\n71\n\n\nAlameda\nCensus Tract 4073; Alameda County; California\n11\n8.42\n65\n\n\nAlameda\nCensus Tract 4074; Alameda County; California\n6\n36.49\n44\n\n\nAlameda\nCensus Tract 4075; Alameda County; California\n7\n22.82\n48\n\n\nAlameda\nCensus Tract 4076; Alameda County; California\n25\n25.03\n34\n\n\nAlameda\nCensus Tract 4077; Alameda County; California\n45\n21.17\n18\n\n\nAlameda\nCensus Tract 4078; Alameda County; California\n36\n21.74\n24\n\n\nAlameda\nCensus Tract 4079; Alameda County; California\n50\n12.89\n13\n\n\nAlameda\nCensus Tract 4080; Alameda County; California\n61\n8.22\n11\n\n\nAlameda\nCensus Tract 4081; Alameda County; California\n37\n25.00\n7\n\n\nAlameda\nCensus Tract 4082; Alameda County; California\n17\n40.51\n29\n\n\nAlameda\nCensus Tract 4083; Alameda County; California\n22\n38.13\n21\n\n\nAlameda\nCensus Tract 4084; Alameda County; California\n4\n39.49\n47\n\n\nAlameda\nCensus Tract 4085; Alameda County; California\n3\n39.33\n49\n\n\nAlameda\nCensus Tract 4086; Alameda County; California\n6\n43.23\n41\n\n\nAlameda\nCensus Tract 4087; Alameda County; California\n9\n32.38\n51\n\n\nAlameda\nCensus Tract 4088; Alameda County; California\n4\n34.17\n46\n\n\nAlameda\nCensus Tract 4089; Alameda County; California\n4\n21.13\n68\n\n\nAlameda\nCensus Tract 4090; Alameda County; California\n3\n29.31\n59\n\n\nAlameda\nCensus Tract 4091; Alameda County; California\n10\n20.38\n62\n\n\nAlameda\nCensus Tract 4092; Alameda County; California\n1\n27.30\n62\n\n\nAlameda\nCensus Tract 4093; Alameda County; California\n2\n23.63\n66\n\n\nAlameda\nCensus Tract 4094; Alameda County; California\n5\n18.07\n66\n\n\nAlameda\nCensus Tract 4095; Alameda County; California\n12\n17.69\n64\n\n\nAlameda\nCensus Tract 4096; Alameda County; California\n3\n24.27\n70\n\n\nAlameda\nCensus Tract 4097; Alameda County; California\n8\n35.78\n36\n\n\nAlameda\nCensus Tract 4098; Alameda County; California\n18\n50.19\n15\n\n\nAlameda\nCensus Tract 4099; Alameda County; California\n23\n41.05\n13\n\n\nAlameda\nCensus Tract 4100; Alameda County; California\n33\n47.28\n5\n\n\nAlameda\nCensus Tract 4101; Alameda County; California\n10\n51.86\n18\n\n\nAlameda\nCensus Tract 4102; Alameda County; California\n11\n44.22\n37\n\n\nAlameda\nCensus Tract 4103; Alameda County; California\n2\n16.49\n78\n\n\nAlameda\nCensus Tract 4104; Alameda County; California\n5\n31.26\n52\n\n\nAlameda\nCensus Tract 4105; Alameda County; California\n21\n58.88\n12\n\n\nAlameda\nCensus Tract 4201; Alameda County; California\n54\n4.64\n13\n\n\nAlameda\nCensus Tract 4202; Alameda County; California\n35\n5.65\n5\n\n\nAlameda\nCensus Tract 4203.01; Alameda County; California\n38\n2.42\n9\n\n\nAlameda\nCensus Tract 4203.02; Alameda County; California\n35\n11.73\n7\n\n\nAlameda\nCensus Tract 4204.01; Alameda County; California\n32\n0.00\n24\n\n\nAlameda\nCensus Tract 4204.02; Alameda County; California\n32\n3.13\n28\n\n\nAlameda\nCensus Tract 4205; Alameda County; California\n49\n2.24\n16\n\n\nAlameda\nCensus Tract 4206; Alameda County; California\n64\n1.03\n8\n\n\nAlameda\nCensus Tract 4211; Alameda County; California\n77\n1.66\n5\n\n\nAlameda\nCensus Tract 4212; Alameda County; California\n70\n0.67\n2\n\n\nAlameda\nCensus Tract 4213; Alameda County; California\n72\n0.17\n3\n\n\nAlameda\nCensus Tract 4214; Alameda County; California\n78\n1.89\n9\n\n\nAlameda\nCensus Tract 4215; Alameda County; California\n81\n1.60\n5\n\n\nAlameda\nCensus Tract 4216; Alameda County; California\n71\n2.41\n4\n\n\nAlameda\nCensus Tract 4217; Alameda County; California\n65\n4.09\n6\n\n\nAlameda\nCensus Tract 4218; Alameda County; California\n70\n0.88\n5\n\n\nAlameda\nCensus Tract 4219; Alameda County; California\n50\n13.26\n7\n\n\nAlameda\nCensus Tract 4220; Alameda County; California\n49\n4.85\n7\n\n\nAlameda\nCensus Tract 4221; Alameda County; California\n47\n11.69\n20\n\n\nAlameda\nCensus Tract 4222; Alameda County; California\n63\n2.53\n7\n\n\nAlameda\nCensus Tract 4223; Alameda County; California\n53\n12.45\n7\n\n\nAlameda\nCensus Tract 4224; Alameda County; California\n38\n3.20\n12\n\n\nAlameda\nCensus Tract 4225; Alameda County; California\n45\n4.10\n13\n\n\nAlameda\nCensus Tract 4227; Alameda County; California\n36\n3.87\n17\n\n\nAlameda\nCensus Tract 4228; Alameda County; California\n28\n5.73\n19\n\n\nAlameda\nCensus Tract 4229.01; Alameda County; California\n41\n0.00\n3\n\n\nAlameda\nCensus Tract 4229.02; Alameda County; California\n30\n6.72\n7\n\n\nAlameda\nCensus Tract 4230; Alameda County; California\n57\n7.54\n8\n\n\nAlameda\nCensus Tract 4231; Alameda County; California\n41\n23.74\n10\n\n\nAlameda\nCensus Tract 4232; Alameda County; California\n39\n17.55\n33\n\n\nAlameda\nCensus Tract 4233; Alameda County; California\n46\n19.85\n17\n\n\nAlameda\nCensus Tract 4234; Alameda County; California\n47\n13.28\n22\n\n\nAlameda\nCensus Tract 4235; Alameda County; California\n55\n10.08\n17\n\n\nAlameda\nCensus Tract 4236.01; Alameda County; California\n63\n3.73\n13\n\n\nAlameda\nCensus Tract 4236.02; Alameda County; California\n47\n1.50\n14\n\n\nAlameda\nCensus Tract 4237; Alameda County; California\n60\n2.38\n17\n\n\nAlameda\nCensus Tract 4238; Alameda County; California\n73\n1.08\n6\n\n\nAlameda\nCensus Tract 4239.01; Alameda County; California\n52\n12.67\n20\n\n\nAlameda\nCensus Tract 4239.02; Alameda County; California\n69\n5.95\n7\n\n\nAlameda\nCensus Tract 4240.01; Alameda County; California\n46\n18.12\n16\n\n\nAlameda\nCensus Tract 4240.02; Alameda County; California\n38\n31.17\n17\n\n\nAlameda\nCensus Tract 4251.01; Alameda County; California\n45\n5.93\n11\n\n\nAlameda\nCensus Tract 4251.02; Alameda County; California\n29\n12.77\n10\n\n\nAlameda\nCensus Tract 4251.03; Alameda County; California\n31\n26.62\n7\n\n\nAlameda\nCensus Tract 4251.04; Alameda County; California\n42\n15.55\n10\n\n\nAlameda\nCensus Tract 4261; Alameda County; California\n68\n0.00\n2\n\n\nAlameda\nCensus Tract 4262; Alameda County; California\n68\n1.44\n6\n\n\nAlameda\nCensus Tract 4271; Alameda County; California\n58\n1.21\n15\n\n\nAlameda\nCensus Tract 4272; Alameda County; California\n33\n3.31\n17\n\n\nAlameda\nCensus Tract 4273; Alameda County; California\n32\n16.26\n8\n\n\nAlameda\nCensus Tract 4276; Alameda County; California\n29\n15.57\n15\n\n\nAlameda\nCensus Tract 4277; Alameda County; California\n61\n2.14\n11\n\n\nAlameda\nCensus Tract 4278; Alameda County; California\n51\n4.38\n9\n\n\nAlameda\nCensus Tract 4279; Alameda County; California\n52\n0.66\n14\n\n\nAlameda\nCensus Tract 4280; Alameda County; California\n32\n12.04\n14\n\n\nAlameda\nCensus Tract 4281; Alameda County; California\n51\n6.82\n10\n\n\nAlameda\nCensus Tract 4282; Alameda County; California\n51\n3.32\n13\n\n\nAlameda\nCensus Tract 4283.01; Alameda County; California\n26\n5.11\n9\n\n\nAlameda\nCensus Tract 4283.02; Alameda County; California\n44\n0.87\n7\n\n\nAlameda\nCensus Tract 4284; Alameda County; California\n37\n7.21\n16\n\n\nAlameda\nCensus Tract 4285; Alameda County; California\n42\n10.93\n16\n\n\nAlameda\nCensus Tract 4286; Alameda County; California\n42\n5.97\n11\n\n\nAlameda\nCensus Tract 4287; Alameda County; California\n23\n17.70\n13\n\n\nAlameda\nCensus Tract 4301.01; Alameda County; California\n34\n2.93\n10\n\n\nAlameda\nCensus Tract 4301.02; Alameda County; California\n53\n0.89\n14\n\n\nAlameda\nCensus Tract 4302; Alameda County; California\n51\n3.45\n10\n\n\nAlameda\nCensus Tract 4303; Alameda County; California\n44\n0.49\n24\n\n\nAlameda\nCensus Tract 4304; Alameda County; California\n47\n4.27\n7\n\n\nAlameda\nCensus Tract 4305; Alameda County; California\n24\n37.25\n16\n\n\nAlameda\nCensus Tract 4306; Alameda County; California\n39\n3.92\n16\n\n\nAlameda\nCensus Tract 4307; Alameda County; California\n39\n4.97\n18\n\n\nAlameda\nCensus Tract 4308; Alameda County; California\n43\n0.84\n16\n\n\nAlameda\nCensus Tract 4309; Alameda County; California\n28\n6.48\n28\n\n\nAlameda\nCensus Tract 4310; Alameda County; California\n27\n16.31\n16\n\n\nAlameda\nCensus Tract 4311; Alameda County; California\n26\n27.71\n26\n\n\nAlameda\nCensus Tract 4312; Alameda County; California\n36\n14.15\n28\n\n\nAlameda\nCensus Tract 4321; Alameda County; California\n32\n24.38\n25\n\n\nAlameda\nCensus Tract 4322; Alameda County; California\n27\n9.76\n37\n\n\nAlameda\nCensus Tract 4323; Alameda County; California\n25\n12.60\n30\n\n\nAlameda\nCensus Tract 4324; Alameda County; California\n13\n4.41\n55\n\n\nAlameda\nCensus Tract 4325.01; Alameda County; California\n20\n3.74\n28\n\n\nAlameda\nCensus Tract 4325.02; Alameda County; California\n11\n19.37\n30\n\n\nAlameda\nCensus Tract 4326.01; Alameda County; California\n24\n22.17\n25\n\n\nAlameda\nCensus Tract 4326.02; Alameda County; California\n14\n25.30\n30\n\n\nAlameda\nCensus Tract 4327; Alameda County; California\n52\n4.90\n21\n\n\nAlameda\nCensus Tract 4328; Alameda County; California\n37\n6.84\n14\n\n\nAlameda\nCensus Tract 4330; Alameda County; California\n36\n8.13\n16\n\n\nAlameda\nCensus Tract 4331.02; Alameda County; California\n7\n6.95\n29\n\n\nAlameda\nCensus Tract 4331.03; Alameda County; California\n13\n11.43\n40\n\n\nAlameda\nCensus Tract 4331.04; Alameda County; California\n24\n17.42\n38\n\n\nAlameda\nCensus Tract 4332; Alameda County; California\n10\n9.73\n33\n\n\nAlameda\nCensus Tract 4333; Alameda County; California\n18\n0.50\n27\n\n\nAlameda\nCensus Tract 4334; Alameda County; California\n13\n8.51\n8\n\n\nAlameda\nCensus Tract 4335; Alameda County; California\n25\n2.92\n21\n\n\nAlameda\nCensus Tract 4336; Alameda County; California\n28\n5.07\n20\n\n\nAlameda\nCensus Tract 4337; Alameda County; California\n13\n2.72\n52\n\n\nAlameda\nCensus Tract 4338.01; Alameda County; California\n9\n13.38\n47\n\n\nAlameda\nCensus Tract 4338.02; Alameda County; California\n8\n12.58\n21\n\n\nAlameda\nCensus Tract 4339; Alameda County; California\n9\n26.33\n43\n\n\nAlameda\nCensus Tract 4340; Alameda County; California\n18\n13.14\n45\n\n\nAlameda\nCensus Tract 4351.02; Alameda County; California\n25\n14.47\n27\n\n\nAlameda\nCensus Tract 4351.03; Alameda County; California\n35\n7.40\n6\n\n\nAlameda\nCensus Tract 4351.04; Alameda County; California\n15\n7.38\n31\n\n\nAlameda\nCensus Tract 4352; Alameda County; California\n23\n22.36\n27\n\n\nAlameda\nCensus Tract 4353; Alameda County; California\n22\n15.13\n34\n\n\nAlameda\nCensus Tract 4354; Alameda County; California\n23\n15.79\n33\n\n\nAlameda\nCensus Tract 4355; Alameda County; California\n26\n12.57\n48\n\n\nAlameda\nCensus Tract 4356.01; Alameda County; California\n12\n10.39\n57\n\n\nAlameda\nCensus Tract 4356.02; Alameda County; California\n20\n13.45\n52\n\n\nAlameda\nCensus Tract 4357; Alameda County; California\n21\n2.43\n50\n\n\nAlameda\nCensus Tract 4358; Alameda County; California\n21\n4.50\n38\n\n\nAlameda\nCensus Tract 4359; Alameda County; California\n29\n0.66\n25\n\n\nAlameda\nCensus Tract 4360; Alameda County; California\n27\n0.61\n40\n\n\nAlameda\nCensus Tract 4361; Alameda County; California\n17\n4.35\n32\n\n\nAlameda\nCensus Tract 4362; Alameda County; California\n11\n12.79\n64\n\n\nAlameda\nCensus Tract 4363.01; Alameda County; California\n6\n10.75\n46\n\n\nAlameda\nCensus Tract 4363.02; Alameda County; California\n17\n11.72\n46\n\n\nAlameda\nCensus Tract 4364.02; Alameda County; California\n40\n13.76\n24\n\n\nAlameda\nCensus Tract 4364.03; Alameda County; California\n27\n9.38\n26\n\n\nAlameda\nCensus Tract 4364.04; Alameda County; California\n45\n11.29\n21\n\n\nAlameda\nCensus Tract 4365; Alameda County; California\n13\n9.60\n49\n\n\nAlameda\nCensus Tract 4366.01; Alameda County; California\n11\n10.61\n58\n\n\nAlameda\nCensus Tract 4366.02; Alameda County; California\n7\n6.17\n53\n\n\nAlameda\nCensus Tract 4367; Alameda County; California\n11\n13.31\n48\n\n\nAlameda\nCensus Tract 4368; Alameda County; California\n11\n5.93\n49\n\n\nAlameda\nCensus Tract 4369; Alameda County; California\n12\n6.97\n58\n\n\nAlameda\nCensus Tract 4370; Alameda County; California\n22\n5.79\n38\n\n\nAlameda\nCensus Tract 4371.01; Alameda County; California\n10\n6.27\n27\n\n\nAlameda\nCensus Tract 4371.02; Alameda County; California\n10\n6.91\n42\n\n\nAlameda\nCensus Tract 4372; Alameda County; California\n11\n6.43\n32\n\n\nAlameda\nCensus Tract 4373; Alameda County; California\n13\n12.22\n34\n\n\nAlameda\nCensus Tract 4374; Alameda County; California\n15\n3.75\n52\n\n\nAlameda\nCensus Tract 4375; Alameda County; California\n11\n5.90\n56\n\n\nAlameda\nCensus Tract 4376; Alameda County; California\n11\n9.46\n31\n\n\nAlameda\nCensus Tract 4377.01; Alameda County; California\n10\n15.62\n50\n\n\nAlameda\nCensus Tract 4377.02; Alameda County; California\n6\n1.91\n85\n\n\nAlameda\nCensus Tract 4378; Alameda County; California\n15\n7.69\n37\n\n\nAlameda\nCensus Tract 4379; Alameda County; California\n12\n11.32\n41\n\n\nAlameda\nCensus Tract 4380; Alameda County; California\n23\n11.47\n23\n\n\nAlameda\nCensus Tract 4381; Alameda County; California\n12\n6.31\n37\n\n\nAlameda\nCensus Tract 4382.01; Alameda County; California\n7\n5.62\n56\n\n\nAlameda\nCensus Tract 4382.03; Alameda County; California\n25\n3.62\n21\n\n\nAlameda\nCensus Tract 4382.04; Alameda County; California\n9\n4.03\n36\n\n\nAlameda\nCensus Tract 4383; Alameda County; California\n6\n6.46\n37\n\n\nAlameda\nCensus Tract 4384; Alameda County; California\n16\n10.86\n24\n\n\nAlameda\nCensus Tract 4401; Alameda County; California\n40\n5.19\n21\n\n\nAlameda\nCensus Tract 4402; Alameda County; California\n3\n0.46\n70\n\n\nAlameda\nCensus Tract 4403.01; Alameda County; California\n29\n5.74\n34\n\n\nAlameda\nCensus Tract 4403.04; Alameda County; California\n10\n7.94\n12\n\n\nAlameda\nCensus Tract 4403.05; Alameda County; California\n21\n1.28\n14\n\n\nAlameda\nCensus Tract 4403.06; Alameda County; California\n9\n5.89\n11\n\n\nAlameda\nCensus Tract 4403.07; Alameda County; California\n14\n5.78\n21\n\n\nAlameda\nCensus Tract 4403.08; Alameda County; California\n13\n4.11\n26\n\n\nAlameda\nCensus Tract 4403.31; Alameda County; California\n12\n3.79\n16\n\n\nAlameda\nCensus Tract 4403.32; Alameda County; California\n8\n1.56\n8\n\n\nAlameda\nCensus Tract 4403.33; Alameda County; California\n7\n0.81\n5\n\n\nAlameda\nCensus Tract 4403.34; Alameda County; California\n11\n5.77\n13\n\n\nAlameda\nCensus Tract 4403.36; Alameda County; California\n15\n12.57\n10\n\n\nAlameda\nCensus Tract 4403.37; Alameda County; California\n6\n4.91\n9\n\n\nAlameda\nCensus Tract 4403.38; Alameda County; California\n22\n0.49\n9\n\n\nAlameda\nCensus Tract 4411; Alameda County; California\n44\n0.04\n16\n\n\nAlameda\nCensus Tract 4412; Alameda County; California\n32\n2.60\n12\n\n\nAlameda\nCensus Tract 4413.01; Alameda County; California\n20\n10.90\n7\n\n\nAlameda\nCensus Tract 4413.02; Alameda County; California\n16\n2.71\n11\n\n\nAlameda\nCensus Tract 4414.01; Alameda County; California\n18\n1.15\n10\n\n\nAlameda\nCensus Tract 4414.02; Alameda County; California\n18\n1.27\n7\n\n\nAlameda\nCensus Tract 4415.01; Alameda County; California\n8\n5.01\n5\n\n\nAlameda\nCensus Tract 4415.03; Alameda County; California\n7\n0.24\n4\n\n\nAlameda\nCensus Tract 4415.21; Alameda County; California\n11\n0.25\n6\n\n\nAlameda\nCensus Tract 4415.22; Alameda County; California\n18\n2.62\n7\n\n\nAlameda\nCensus Tract 4415.23; Alameda County; California\n11\n4.20\n7\n\n\nAlameda\nCensus Tract 4415.24; Alameda County; California\n5\n0.36\n1\n\n\nAlameda\nCensus Tract 4415.25; Alameda County; California\n7\n3.49\n11\n\n\nAlameda\nCensus Tract 4416.01; Alameda County; California\n31\n8.17\n14\n\n\nAlameda\nCensus Tract 4416.02; Alameda County; California\n28\n8.46\n23\n\n\nAlameda\nCensus Tract 4417.01; Alameda County; California\n12\n0.46\n20\n\n\nAlameda\nCensus Tract 4417.02; Alameda County; California\n19\n10.01\n16\n\n\nAlameda\nCensus Tract 4418; Alameda County; California\n32\n1.77\n6\n\n\nAlameda\nCensus Tract 4419.21; Alameda County; California\n19\n0.31\n24\n\n\nAlameda\nCensus Tract 4419.23; Alameda County; California\n12\n3.41\n13\n\n\nAlameda\nCensus Tract 4419.24; Alameda County; California\n17\n1.51\n10\n\n\nAlameda\nCensus Tract 4419.26; Alameda County; California\n13\n3.25\n25\n\n\nAlameda\nCensus Tract 4419.27; Alameda County; California\n19\n3.74\n10\n\n\nAlameda\nCensus Tract 4419.28; Alameda County; California\n17\n13.20\n8\n\n\nAlameda\nCensus Tract 4419.29; Alameda County; California\n21\n0.85\n8\n\n\nAlameda\nCensus Tract 4420; Alameda County; California\n15\n0.13\n10\n\n\nAlameda\nCensus Tract 4421; Alameda County; California\n9\n1.60\n1\n\n\nAlameda\nCensus Tract 4422; Alameda County; California\n13\n2.43\n6\n\n\nAlameda\nCensus Tract 4423.01; Alameda County; California\n20\n2.37\n15\n\n\nAlameda\nCensus Tract 4423.02; Alameda County; California\n15\n5.66\n15\n\n\nAlameda\nCensus Tract 4424; Alameda County; California\n25\n1.75\n25\n\n\nAlameda\nCensus Tract 4425.01; Alameda County; California\n13\n3.17\n28\n\n\nAlameda\nCensus Tract 4425.02; Alameda County; California\n17\n4.24\n30\n\n\nAlameda\nCensus Tract 4426.01; Alameda County; California\n28\n3.47\n28\n\n\nAlameda\nCensus Tract 4426.02; Alameda County; California\n29\n10.38\n17\n\n\nAlameda\nCensus Tract 4427; Alameda County; California\n30\n1.05\n11\n\n\nAlameda\nCensus Tract 4428; Alameda County; California\n24\n0.74\n16\n\n\nAlameda\nCensus Tract 4429; Alameda County; California\n15\n5.15\n13\n\n\nAlameda\nCensus Tract 4430.01; Alameda County; California\n18\n4.89\n32\n\n\nAlameda\nCensus Tract 4430.02; Alameda County; California\n16\n1.74\n17\n\n\nAlameda\nCensus Tract 4431.02; Alameda County; California\n11\n0.00\n6\n\n\nAlameda\nCensus Tract 4431.03; Alameda County; California\n19\n1.21\n3\n\n\nAlameda\nCensus Tract 4431.04; Alameda County; California\n14\n6.27\n2\n\n\nAlameda\nCensus Tract 4431.05; Alameda County; California\n10\n0.43\n2\n\n\nAlameda\nCensus Tract 4432; Alameda County; California\n14\n0.30\n1\n\n\nAlameda\nCensus Tract 4433.01; Alameda County; California\n18\n1.02\n9\n\n\nAlameda\nCensus Tract 4433.21; Alameda County; California\n3\n1.69\n7\n\n\nAlameda\nCensus Tract 4433.22; Alameda County; California\n19\n1.09\n7\n\n\nAlameda\nCensus Tract 4441; Alameda County; California\n29\n6.61\n25\n\n\nAlameda\nCensus Tract 4442; Alameda County; California\n21\n1.34\n30\n\n\nAlameda\nCensus Tract 4443.01; Alameda County; California\n29\n0.76\n26\n\n\nAlameda\nCensus Tract 4443.03; Alameda County; California\nNaN\nNaN\nNaN\n\n\nAlameda\nCensus Tract 4443.04; Alameda County; California\n12\n0.81\n32\n\n\nAlameda\nCensus Tract 4444; Alameda County; California\n15\n2.87\n54\n\n\nAlameda\nCensus Tract 4445; Alameda County; California\n18\n4.61\n44\n\n\nAlameda\nCensus Tract 4446.01; Alameda County; California\n17\n1.20\n18\n\n\nAlameda\nCensus Tract 4446.02; Alameda County; California\n15\n5.82\n10\n\n\nAlameda\nCensus Tract 4501.01; Alameda County; California\n23\n2.96\n8\n\n\nAlameda\nCensus Tract 4501.02; Alameda County; California\n19\n14.83\n14\n\n\nAlameda\nCensus Tract 4502; Alameda County; California\n40\n3.57\n9\n\n\nAlameda\nCensus Tract 4503; Alameda County; California\n47\n6.27\n15\n\n\nAlameda\nCensus Tract 4504; Alameda County; California\n35\n3.79\n19\n\n\nAlameda\nCensus Tract 4505.01; Alameda County; California\n54\n0.00\n13\n\n\nAlameda\nCensus Tract 4505.02; Alameda County; California\n42\n0.65\n9\n\n\nAlameda\nCensus Tract 4506.01; Alameda County; California\n47\n0.74\n7\n\n\nAlameda\nCensus Tract 4506.03; Alameda County; California\n42\n0.43\n15\n\n\nAlameda\nCensus Tract 4506.04; Alameda County; California\n53\n0.44\n13\n\n\nAlameda\nCensus Tract 4506.05; Alameda County; California\n49\n0.26\n9\n\n\nAlameda\nCensus Tract 4506.06; Alameda County; California\n50\n0.00\n5\n\n\nAlameda\nCensus Tract 4506.07; Alameda County; California\n33\n1.72\n15\n\n\nAlameda\nCensus Tract 4506.08; Alameda County; California\n41\n1.67\n8\n\n\nAlameda\nCensus Tract 4506.09; Alameda County; California\n48\n2.03\n14\n\n\nAlameda\nCensus Tract 4507.01; Alameda County; California\n45\n1.66\n7\n\n\nAlameda\nCensus Tract 4507.41; Alameda County; California\n45\n1.18\n21\n\n\nAlameda\nCensus Tract 4507.42; Alameda County; California\n49\n0.18\n9\n\n\nAlameda\nCensus Tract 4507.43; Alameda County; California\n20\n7.91\n13\n\n\nAlameda\nCensus Tract 4507.44; Alameda County; California\n47\n0.00\n13\n\n\nAlameda\nCensus Tract 4507.45; Alameda County; California\n33\n0.44\n7\n\n\nAlameda\nCensus Tract 4507.46; Alameda County; California\n47\n0.43\n19\n\n\nAlameda\nCensus Tract 4507.50; Alameda County; California\n20\n3.56\n7\n\n\nAlameda\nCensus Tract 4507.51; Alameda County; California\n13\n2.69\n5\n\n\nAlameda\nCensus Tract 4507.52; Alameda County; California\n14\n2.37\n7\n\n\nAlameda\nCensus Tract 4511.02; Alameda County; California\n74\n0.51\n7\n\n\nAlameda\nCensus Tract 4511.03; Alameda County; California\n86\n1.76\n12\n\n\nAlameda\nCensus Tract 4511.04; Alameda County; California\n54\n0.04\n18\n\n\nAlameda\nCensus Tract 4512.01; Alameda County; California\n48\n1.64\n25\n\n\nAlameda\nCensus Tract 4512.02; Alameda County; California\n45\n0.93\n17\n\n\nAlameda\nCensus Tract 4513; Alameda County; California\n54\n1.22\n27\n\n\nAlameda\nCensus Tract 4514.01; Alameda County; California\n39\n5.35\n38\n\n\nAlameda\nCensus Tract 4514.03; Alameda County; California\n58\n1.60\n23\n\n\nAlameda\nCensus Tract 4514.04; Alameda County; California\n32\n1.06\n59\n\n\nAlameda\nCensus Tract 4515.01; Alameda County; California\n59\n5.07\n14\n\n\nAlameda\nCensus Tract 4515.03; Alameda County; California\n51\n0.59\n20\n\n\nAlameda\nCensus Tract 4515.04; Alameda County; California\n46\n0.00\n27\n\n\nAlameda\nCensus Tract 4515.05; Alameda County; California\n66\n1.01\n13\n\n\nAlameda\nCensus Tract 4515.06; Alameda County; California\n37\n3.97\n36\n\n\nAlameda\nCensus Tract 4516.01; Alameda County; California\n70\n0.00\n9\n\n\nAlameda\nCensus Tract 4516.02; Alameda County; California\n65\n2.20\n19\n\n\nAlameda\nCensus Tract 4517.01; Alameda County; California\n54\n4.26\n13\n\n\nAlameda\nCensus Tract 4517.03; Alameda County; California\n61\n0.73\n13\n\n\nAlameda\nCensus Tract 4517.04; Alameda County; California\n69\n0.23\n12\n\n\nAlameda\nCensus Tract 9819; Alameda County; California\n60\n0.00\n40\n\n\nAlameda\nCensus Tract 9820; Alameda County; California\n50\n10.00\n0\n\n\nAlameda\nCensus Tract 9821; Alameda County; California\n28\n8.88\n18\n\n\nAlameda\nCensus Tract 9832; Alameda County; California\n54\n13.84\n5\n\n\nAlameda\nCensus Tract 9900; Alameda County; California\nNaN\nNaN\nNaN\n\n\nAlpine\nCensus Tract 100; Alpine County; California\n58\n0.00\n14\n\n\nAmador\nCensus Tract 1.01; Amador County; California\n80\n0.44\n13\n\n\nAmador\nCensus Tract 1.02; Amador County; California\n85\n0.32\n8\n\n\nAmador\nCensus Tract 2.01; Amador County; California\n86\n0.72\n10\n\n\nAmador\nCensus Tract 2.02; Amador County; California\n73\n2.30\n14\n\n\nAmador\nCensus Tract 3.01; Amador County; California\n46\n9.89\n36\n\n\nAmador\nCensus Tract 3.03; Amador County; California\n80\n0.09\n9\n\n\nAmador\nCensus Tract 3.04; Amador County; California\n78\n0.80\n10\n\n\nAmador\nCensus Tract 4.01; Amador County; California\n79\n0.70\n8\n\n\nAmador\nCensus Tract 4.02; Amador County; California\n78\n0.17\n16\n\n\nAmador\nCensus Tract 5; Amador County; California\n72\n0.09\n24"
  },
  {
    "objectID": "Assignments/Assignment_1/assignment1_template.html#demographic-analysis",
    "href": "Assignments/Assignment_1/assignment1_template.html#demographic-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "3.3 Demographic Analysis",
    "text": "3.3 Demographic Analysis\nYour Task: Analyze the demographic patterns in your selected areas.\n\n# Find the tract with the highest percentage of Hispanic/Latino residents\n# Hint: use arrange() and slice() to get the top tract\n\ntop_hispanic_tract &lt;- tract_percent %&gt;%\n  arrange(desc(pct_hispanic)) %&gt;%\n  slice(1) %&gt;%\n  select(GEOID, tract_label, county_name, pct_hispanic)\n\nkable(top_hispanic_tract, caption = \"Tract with Highest % Hispanic/Latino\")\n\n\nTract with Highest % Hispanic/Latino\n\n\n\n\n\n\n\n\nGEOID\ntract_label\ncounty_name\npct_hispanic\n\n\n\n\n06001437702\nCensus Tract 4377.02; Alameda County; California\nAlameda\n85\n\n\n\n\n# Calculate average demographics by county using group_by() and summarize()\n# Show: number of tracts, average percentage for each racial/ethnic group\n\ncounty_demo_avgs &lt;- tract_percent %&gt;%\n  group_by(county_name) %&gt;%\n  summarise(\n    \"Number of Tracts\" = n(),\n    \"Average White Percentage\" = mean(pct_white, na.rm = TRUE),\n    \"Average Black Percentage\" = mean(pct_black, na.rm = TRUE),\n    \"Average Hispanic Percentage\"  = mean(pct_hispanic, na.rm = TRUE)\n  )\n\n# Create a nicely formatted table of your results using kable()\n\nkable(\n  county_demo_avgs,\n  caption = \"Average Tract Demographics by County\",\n  digits = 1,\n  align = c(\"l\",\"c\",\"c\",\"c\",\"c\"),\n  col.names = c(\"County Names\", \"Number of Tracts\", \"Average White Percentage\",\"Average Black Percentage\",\"Average Hispanic Percentage\")\n)\n\n\nAverage Tract Demographics by County\n\n\n\n\n\n\n\n\n\nCounty Names\nNumber of Tracts\nAverage White Percentage\nAverage Black Percentage\nAverage Hispanic Percentage\n\n\n\n\nAlameda\n379\n31.0\n10.7\n21.4\n\n\nAlpine\n1\n58.1\n0.0\n14.1\n\n\nAmador\n10\n75.7\n1.6\n14.9"
  },
  {
    "objectID": "Assignments/Assignment_1/assignment1_template.html#moe-analysis-for-demographic-variables",
    "href": "Assignments/Assignment_1/assignment1_template.html#moe-analysis-for-demographic-variables",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.1 MOE Analysis for Demographic Variables",
    "text": "4.1 MOE Analysis for Demographic Variables\nYour Task: Examine margins of error for demographic variables to see if some communities have less reliable data.\nRequirements: - Calculate MOE percentages for each demographic variable - Flag tracts where any demographic variable has MOE &gt; 15% - Create summary statistics\n\n# Calculate MOE percentages for white, Black, and Hispanic variables\n# Hint: use the same formula as before (margin/estimate * 100)\n\n# Create a flag for tracts with high MOE on any demographic variable\n# Use logical operators (| for OR) in an ifelse() statement\n\ncounty_codes_state &lt;- unique(stringr::str_sub(county$GEOID, 3, 5))\n\nrace_vars &lt;- c(\n  total    = \"B03002_001\",\n  white    = \"B03002_003\",\n  black    = \"B03002_004\",\n  hispanic = \"B03002_012\"\n)\n\ntract_state_raw &lt;- get_acs(\n  geography = \"tract\",\n  state     = my_state,\n  county    = county_codes_state,  # all counties in the state\n  survey    = \"acs5\",\n  year      = 2022,\n  variables = race_vars,\n  output    = \"wide\"\n)\n\ntract_state_percent &lt;- tract_state_raw %&gt;%\n  mutate(\n    county_code  = substr(GEOID, 3, 5),\n    pct_white    = 100 * (whiteE    / totalE),\n    pct_black    = 100 * (blackE    / totalE),\n    pct_hispanic = 100 * (hispanicE / totalE),\n    tract_label  = stringr::str_remove(NAME, paste0(\", \", my_state)),\n    tract_label  = stringr::str_remove(tract_label, \", United States$\")\n  ) %&gt;%\n  left_join(\n    county %&gt;%\n      transmute(\n        county_code = substr(GEOID, 3, 5),\n        county_name\n      ),\n    by = \"county_code\"\n  )\n\ntract_quality &lt;- tract_state_percent %&gt;%\n  mutate(\n    moe_total_pct    = 100 * (totalM    / totalE),\n    moe_white_pct    = 100 * (whiteM    / whiteE),\n    moe_black_pct    = 100 * (blackM    / blackE),\n    moe_hispanic_pct = 100 * (hispanicM / hispanicE),\n    high_moe_flag = (moe_white_pct &gt; 15) | (moe_black_pct &gt; 15) | (moe_hispanic_pct &gt; 15)\n  )\n\ntract_quality_summary &lt;- tract_quality %&gt;%\n  summarise(\n    tracts_total     = n(),\n    tracts_high_moe  = sum(high_moe_flag, na.rm = TRUE),\n    percent_high_moe = round(100 * tracts_high_moe / tracts_total, 1)\n  )\n\nkable(\n  tract_quality_summary,\n  caption   = \"Tract-Level High-MOE Summary (&gt;15% on any demographic variable) — Statewide\",\n  col.names = c(\"Total Tracts\", \"High-MOE Tracts\", \"Percent High-MOE (%)\"),\n  align     = \"c\"\n)\n\n\nTract-Level High-MOE Summary (&gt;15% on any demographic variable) — Statewide\n\n\nTotal Tracts\nHigh-MOE Tracts\nPercent High-MOE (%)\n\n\n\n\n9129\n9123\n99.9"
  },
  {
    "objectID": "Assignments/Assignment_1/assignment1_template.html#pattern-analysis",
    "href": "Assignments/Assignment_1/assignment1_template.html#pattern-analysis",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "4.2 Pattern Analysis",
    "text": "4.2 Pattern Analysis\nYour Task: Investigate whether data quality problems are randomly distributed or concentrated in certain types of communities.\n\n# Group tracts by whether they have high MOE issues\n# Calculate average characteristics for each group:\n# - population size, demographic percentages\n\npattern_table &lt;- tract_quality %&gt;%\n  group_by(high_moe_flag, county_name) %&gt;%\n  summarise(\n    tracts         = n(),\n    avg_pop        = mean(totalE, na.rm = TRUE),\n    avg_pct_white  = mean(pct_white, na.rm = TRUE),\n    avg_pct_black  = mean(pct_black, na.rm = TRUE),\n    avg_pct_hispanic = mean(pct_hispanic, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  arrange(desc(high_moe_flag), county_name)\n\nkable(\n  pattern_table,\n  caption = \"Characteristics by High-MOE Status (Any Demographic Variable &gt; 15% MOE) — Statewide\",\n  col.names = c(\n    \"High-MOE (Any &gt;15%)\",\n    \"County\",\n    \"Tracts\",\n    \"Avg Pop\",\n    \"Avg White (%)\",\n    \"Avg Black (%)\",\n    \"Avg Hispanic (%)\"\n  ),\n  align = c(\"c\",\"l\",\"r\",\"r\",\"r\",\"r\",\"r\"),\n  digits = c(NA, NA, 0, 0, 1, 1, 1),\n  format.args = list(big.mark = \",\")\n)\n\n\nCharacteristics by High-MOE Status (Any Demographic Variable &gt; 15% MOE) — Statewide\n\n\n\n\n\n\n\n\n\n\n\nHigh-MOE (Any &gt;15%)\nCounty\nTracts\nAvg Pop\nAvg White (%)\nAvg Black (%)\nAvg Hispanic (%)\n\n\n\n\nTRUE\nAlameda\n379\n4,390\n31.0\n10.7\n21.4\n\n\nTRUE\nAlpine\n1\n1,515\n58.1\n0.0\n14.1\n\n\nTRUE\nAmador\n10\n4,058\n75.7\n1.6\n14.9\n\n\nTRUE\nButte\n54\n3,956\n69.3\n1.5\n17.4\n\n\nTRUE\nCalaveras\n14\n3,262\n81.0\n0.9\n11.6\n\n\nTRUE\nColusa\n6\n3,635\n34.0\n1.6\n60.5\n\n\nTRUE\nContra Costa\n242\n4,804\n42.6\n8.0\n25.1\n\n\nTRUE\nDel Norte\n9\n3,051\n59.5\n2.2\n19.6\n\n\nTRUE\nEl Dorado\n55\n3,486\n76.0\n0.6\n13.8\n\n\nTRUE\nFresno\n225\n4,481\n28.4\n4.1\n54.1\n\n\nTRUE\nGlenn\n8\n3,582\n54.0\n0.3\n39.2\n\n\nTRUE\nHumboldt\n36\n3,781\n72.1\n1.3\n11.6\n\n\nTRUE\nImperial\n40\n4,489\n11.5\n2.6\n82.2\n\n\nTRUE\nInyo\n6\n3,138\n62.1\n0.8\n23.2\n\n\nTRUE\nKern\n236\n3,843\n33.1\n4.7\n54.0\n\n\nTRUE\nKings\n30\n4,830\n30.0\n5.0\n57.6\n\n\nTRUE\nLake\n21\n3,239\n69.3\n2.5\n20.0\n\n\nTRUE\nLassen\n8\n3,020\n75.0\n2.8\n12.0\n\n\nTRUE\nLos Angeles\n2,497\n3,976\n26.3\n7.6\n47.6\n\n\nTRUE\nMadera\n33\n4,552\n33.9\n2.0\n58.3\n\n\nTRUE\nMarin\n63\n4,135\n69.2\n2.5\n16.5\n\n\nTRUE\nMariposa\n6\n2,855\n76.8\n0.9\n13.4\n\n\nTRUE\nMendocino\n24\n3,798\n64.6\n0.5\n24.9\n\n\nTRUE\nMerced\n63\n4,481\n25.4\n2.8\n62.1\n\n\nTRUE\nModoc\n4\n2,163\n76.6\n1.4\n15.1\n\n\nTRUE\nMono\n4\n3,305\n64.1\n0.2\n27.8\n\n\nTRUE\nMonterey\n104\n4,208\n35.2\n2.0\n52.6\n\n\nTRUE\nNapa\n40\n3,435\n54.6\n2.1\n31.7\n\n\nTRUE\nNevada\n26\n3,935\n83.4\n0.3\n9.6\n\n\nTRUE\nOrange\n614\n5,171\n41.3\n1.5\n32.4\n\n\nTRUE\nPlacer\n92\n4,420\n70.9\n1.4\n14.5\n\n\nTRUE\nPlumas\n7\n2,807\n85.2\n0.6\n8.6\n\n\nTRUE\nRiverside\n518\n4,690\n34.6\n5.7\n49.9\n\n\nTRUE\nSacramento\n363\n4,350\n43.2\n9.1\n23.8\n\n\nTRUE\nSan Benito\n12\n5,396\n32.8\n0.8\n59.5\n\n\nTRUE\nSan Bernardino\n465\n4,682\n28.5\n7.1\n53.3\n\n\nTRUE\nSan Diego\n737\n4,464\n45.5\n4.4\n33.3\n\n\nTRUE\nSan Francisco\n244\n3,488\n39.5\n5.1\n15.1\n\n\nTRUE\nSan Joaquin\n174\n4,480\n29.8\n6.7\n43.6\n\n\nTRUE\nSan Luis Obispo\n70\n4,024\n67.1\n1.3\n23.2\n\n\nTRUE\nSan Mateo\n174\n4,335\n37.9\n2.1\n23.5\n\n\nTRUE\nSanta Barbara\n109\n4,085\n46.0\n1.8\n43.6\n\n\nTRUE\nSanta Clara\n408\n4,698\n29.6\n2.3\n25.3\n\n\nTRUE\nSanta Cruz\n70\n3,837\n56.7\n0.8\n34.2\n\n\nTRUE\nShasta\n50\n3,637\n77.7\n0.9\n10.6\n\n\nTRUE\nSierra\n1\n2,916\n86.6\n0.2\n11.4\n\n\nTRUE\nSiskiyou\n16\n2,753\n73.9\n1.2\n14.5\n\n\nTRUE\nSolano\n99\n4,497\n36.3\n12.7\n28.2\n\n\nTRUE\nSonoma\n122\n4,004\n63.6\n1.4\n25.6\n\n\nTRUE\nStanislaus\n112\n4,929\n38.7\n2.7\n48.9\n\n\nTRUE\nSutter\n21\n4,719\n45.8\n1.8\n32.5\n\n\nTRUE\nTehama\n14\n4,677\n65.9\n0.9\n26.0\n\n\nTRUE\nTrinity\n4\n3,972\n79.2\n1.7\n7.0\n\n\nTRUE\nTulare\n103\n4,597\n27.4\n1.2\n65.3\n\n\nTRUE\nTuolumne\n18\n3,055\n78.1\n1.8\n13.5\n\n\nTRUE\nVentura\n190\n4,432\n45.0\n1.7\n42.1\n\n\nTRUE\nYolo\n53\n4,097\n46.5\n2.7\n31.1\n\n\nTRUE\nYuba\n19\n4,300\n56.0\n3.2\n26.7\n\n\nFALSE\nKings\n1\n7,612\n18.3\n25.1\n49.9\n\n\nFALSE\nLassen\n1\n7,717\n30.4\n22.1\n42.4\n\n\nFALSE\nLos Angeles\n1\n8,994\n16.8\n33.6\n41.4\n\n\nFALSE\nMadera\n1\n7,043\n25.2\n14.9\n49.2\n\n\nFALSE\nSan Bernardino\n1\n3,618\n15.5\n25.5\n50.4\n\n\nFALSE\nSolano\n1\n5,774\n18.5\n43.6\n29.0\n\n\n\n\ntract_flag_driver &lt;- tract_quality %&gt;%\n  mutate(\n    flag_white    = moe_white_pct    &gt; 15,\n    flag_black    = moe_black_pct    &gt; 15,\n    flag_hispanic = moe_hispanic_pct &gt; 15,\n    driver_groups = case_when(\n      flag_white & !flag_black & !flag_hispanic ~ \"White\",\n      !flag_white & flag_black & !flag_hispanic ~ \"Black\",\n      !flag_white & !flag_black & flag_hispanic ~ \"Hispanic\",\n      flag_white | flag_black | flag_hispanic   ~ \"Multiple\",\n      TRUE                                       ~ \"None\"\n    )\n  )\n\ndriver_totals &lt;- tract_flag_driver %&gt;%\n  filter(high_moe_flag) %&gt;%\n  summarise(\n    White    = sum(flag_white,    na.rm = TRUE),\n    Black    = sum(flag_black,    na.rm = TRUE),\n    Hispanic = sum(flag_hispanic, na.rm = TRUE)\n  ) %&gt;%\n  tidyr::pivot_longer(everything(),\n                      names_to = \"Group\",\n                      values_to = \"Flagged Tracts\")\n\nkable(\n  driver_totals %&gt;% arrange(desc(`Flagged Tracts`)),\n  caption   = \"Which Groups Drove High-MOE Flags (MOE% &gt; 15) — Statewide\",\n  col.names = c(\"Group\", \"Flagged Tracts\"),\n  align     = c(\"l\",\"r\"),\n  format.args = list(big.mark = \",\")\n)\n\n\nWhich Groups Drove High-MOE Flags (MOE% &gt; 15) — Statewide\n\n\nGroup\nFlagged Tracts\n\n\n\n\nBlack\n9,109\n\n\nHispanic\n8,631\n\n\nWhite\n8,144\n\n\n\n\n\nPattern Analysis: If we are to detect patterns using the instructions given, then 99.9% of all census tracts are within the MOE margins, meaning there is some type of data error present. That is a clear indication that the issue is effectively random at the tract level. However, collapsing the results by demographic group rather than by tract reveals that the burden of error is not evenly shared: it falls overwhelmingly on minority populations, with Black communities the most affected. This stems from well-known challenges of under-sampling in the ACS, which causes large variations within smaller racial categories. Tracts with high MOEs tend to have smaller populations overall, amplifying sampling error, and even within White-majority tracts the subgroup estimates for Black and Hispanic residents frequently exceed the 15% MOE threshold. In some cases, there are so few observations that subgroup estimates are unstable or missing altogether. Together, these dynamics show that while the pattern may look random at the tract level, the reliability problem is systematically tied to the representation of minority populations, raising clear concerns for algorithmic decision-making."
  },
  {
    "objectID": "Assignments/Assignment_1/assignment1_template.html#analysis-integration-and-professional-summary",
    "href": "Assignments/Assignment_1/assignment1_template.html#analysis-integration-and-professional-summary",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "5.1 Analysis Integration and Professional Summary",
    "text": "5.1 Analysis Integration and Professional Summary\nExecutive Summary:\nAcross county- and tract-level analyses, two systematic patterns consistently appear. First, tracts and counties with smaller populations tend to have disproportionately high margins of error, making their estimates far less stable than those from larger areas. Second, the reliability of racial and ethnic subgroup estimates varies sharply: Black and Hispanic populations are much more likely to have margins of error above 15%, and in some cases, the ACS does not capture enough observations to produce valid estimates. Together, these patterns show that measurement error is pervasive but not random it reflects structural features of both tract size and demographic composition.\nCommunities facing the greatest risk of algorithmic bias are those that are either very small and rural or racially/ethnically diverse. Rural tracts, because of small sample sizes, may be flagged as unreliable and thus deprioritized in automated systems, despite having genuine needs. At the same time, urban minority communities, particularly those with large Hispanic or Black populations, often show the highest subgroup MOEs, meaning their conditions could be systematically misclassified or underestimated. In both cases, the communities already at risk of marginalization are the same ones where the data is least reliable.\nThe drivers of these problems are structural. In rural areas, small sample sizes inflate margins of error, while in diverse urban tracts, underrepresentation of minority subgroups disrupts the accuracy of need assessments. This underrepresentation is tied to long-standing stratification in data collection, where certain groups are less visible in surveys, and to socio-spatial self-selection, where minorities concentrate in particular neighborhoods that are often harder to measure with precision. These processes produce systematic biases: the very communities whose needs are greatest — low-income, minority, and geographically marginalized — are those most likely to be misrepresented in the data.\nThe Department should treat reliability as central to its algorithmic framework. Specifically, it should (a) adjust for MOE when prioritizing communities, so noisy estimates are not misclassified as real differences; (b) avoid strict cutoffs in low-confidence areas by using broader eligibility bands; (c) supplement ACS data with administrative or community-level sources in minority-dense neighborhoods where subgroup reliability is weakest; and (d) incorporate transparency and equity audits to ensure that stratification and data gaps do not reinforce existing inequalities. By embedding these safeguards, the Department can ensure its allocation strategies are both statistically sound and socially just."
  },
  {
    "objectID": "Assignments/Assignment_1/assignment1_template.html#specific-recommendations",
    "href": "Assignments/Assignment_1/assignment1_template.html#specific-recommendations",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "6.3 Specific Recommendations",
    "text": "6.3 Specific Recommendations\nYour Task: Create a decision framework for algorithm implementation.\n\n# Create a summary table using your county reliability data\n# Include: county name, median income, MOE percentage, reliability category\n\nrecommendations &lt;- county_reliability %&gt;%\n  select(\n    County = county_name,\n    `Median Income` = med_hh_incomeE,\n    `MOE %` = moe_percentage,\n    `Reliability Category` = Reliability\n  ) %&gt;%\n  mutate(\n    Recommendation = case_when(\n      `Reliability Category` == \"High Confidence\"     ~ \"Safe for algorithmic decisions\",\n      `Reliability Category` == \"Moderate Confidence\" ~ \"Use with caution – monitor outcomes\",\n      `Reliability Category` == \"Low Confidence\"      ~ \"Requires manual review or additional data\",\n      TRUE                                            ~ NA_character_\n    )\n  )\n\n# Add a new column with algorithm recommendations using case_when():\n# - High Confidence: \"Safe for algorithmic decisions\"\n# - Moderate Confidence: \"Use with caution - monitor outcomes\"  \n# - Low Confidence: \"Requires manual review or additional data\"\n\n# Format as a professional table with kable()\n\nkable(\n  recommendations %&gt;%\n    arrange(Recommendation, County),\n  caption = \"Decision Framework for Algorithm Implementation (Arranged by Recommendation)\",\n  col.names = c(\"County\", \"Median Income\", \"MOE %\", \"Reliability Category\", \"Recommendation\"),\n  digits = 2,\n  format.args = list(big.mark = \",\")\n)\n\n\nDecision Framework for Algorithm Implementation (Arranged by Recommendation)\n\n\n\n\n\n\n\n\n\nCounty\nMedian Income\nMOE %\nReliability Category\nRecommendation\n\n\n\n\nAlpine\n101,125\n17.25\nLow Confidence\nRequires manual review or additional data\n\n\nMono\n82,038\n18.76\nLow Confidence\nRequires manual review or additional data\n\n\nPlumas\n67,885\n11.45\nLow Confidence\nRequires manual review or additional data\n\n\nSierra\n61,108\n15.12\nLow Confidence\nRequires manual review or additional data\n\n\nTrinity\n47,317\n12.45\nLow Confidence\nRequires manual review or additional data\n\n\nAlameda\n122,488\n1.00\nHigh Confidence\nSafe for algorithmic decisions\n\n\nButte\n66,085\n3.42\nHigh Confidence\nSafe for algorithmic decisions\n\n\nContra Costa\n120,020\n1.25\nHigh Confidence\nSafe for algorithmic decisions\n\n\nEl Dorado\n99,246\n3.36\nHigh Confidence\nSafe for algorithmic decisions\n\n\nFresno\n67,756\n1.43\nHigh Confidence\nSafe for algorithmic decisions\n\n\nHumboldt\n57,881\n3.68\nHigh Confidence\nSafe for algorithmic decisions\n\n\nImperial\n53,847\n4.11\nHigh Confidence\nSafe for algorithmic decisions\n\n\nKern\n63,883\n2.07\nHigh Confidence\nSafe for algorithmic decisions\n\n\nKings\n68,540\n3.29\nHigh Confidence\nSafe for algorithmic decisions\n\n\nLake\n56,259\n4.34\nHigh Confidence\nSafe for algorithmic decisions\n\n\nLos Angeles\n83,411\n0.53\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMadera\n73,543\n3.87\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMarin\n142,019\n2.89\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMendocino\n61,335\n3.58\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMerced\n64,772\n3.31\nHigh Confidence\nSafe for algorithmic decisions\n\n\nMonterey\n91,043\n2.09\nHigh Confidence\nSafe for algorithmic decisions\n\n\nNapa\n105,809\n2.82\nHigh Confidence\nSafe for algorithmic decisions\n\n\nNevada\n79,395\n4.82\nHigh Confidence\nSafe for algorithmic decisions\n\n\nOrange\n109,361\n0.81\nHigh Confidence\nSafe for algorithmic decisions\n\n\nPlacer\n109,375\n1.70\nHigh Confidence\nSafe for algorithmic decisions\n\n\nRiverside\n84,505\n1.26\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSacramento\n84,010\n0.97\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Bernardino\n77,423\n1.04\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Diego\n96,974\n1.02\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Francisco\n136,689\n1.43\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Joaquin\n82,837\n1.75\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Luis Obispo\n90,158\n2.56\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSan Mateo\n149,907\n1.75\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSanta Barbara\n92,332\n2.05\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSanta Clara\n153,792\n1.00\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSanta Cruz\n104,409\n3.04\nHigh Confidence\nSafe for algorithmic decisions\n\n\nShasta\n68,347\n3.63\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSiskiyou\n53,898\n4.90\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSolano\n97,037\n1.78\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSonoma\n99,266\n2.00\nHigh Confidence\nSafe for algorithmic decisions\n\n\nStanislaus\n74,872\n1.83\nHigh Confidence\nSafe for algorithmic decisions\n\n\nSutter\n72,654\n4.71\nHigh Confidence\nSafe for algorithmic decisions\n\n\nTulare\n64,474\n2.31\nHigh Confidence\nSafe for algorithmic decisions\n\n\nVentura\n102,141\n1.50\nHigh Confidence\nSafe for algorithmic decisions\n\n\nYolo\n85,097\n2.74\nHigh Confidence\nSafe for algorithmic decisions\n\n\nYuba\n66,693\n4.19\nHigh Confidence\nSafe for algorithmic decisions\n\n\nAmador\n74,853\n8.08\nModerate Confidence\nUse with caution – monitor outcomes\n\n\nCalaveras\n77,526\n5.00\nModerate Confidence\nUse with caution – monitor outcomes\n\n\nColusa\n69,619\n8.25\nModerate Confidence\nUse with caution – monitor outcomes\n\n\nDel Norte\n61,149\n7.16\nModerate Confidence\nUse with caution – monitor outcomes\n\n\nGlenn\n64,033\n6.19\nModerate Confidence\nUse with caution – monitor outcomes\n\n\nInyo\n63,417\n8.60\nModerate Confidence\nUse with caution – monitor outcomes\n\n\nLassen\n59,515\n5.97\nModerate Confidence\nUse with caution – monitor outcomes\n\n\nMariposa\n60,021\n8.82\nModerate Confidence\nUse with caution – monitor outcomes\n\n\nModoc\n54,962\n9.80\nModerate Confidence\nUse with caution – monitor outcomes\n\n\nSan Benito\n104,451\n5.23\nModerate Confidence\nUse with caution – monitor outcomes\n\n\nTehama\n59,029\n6.95\nModerate Confidence\nUse with caution – monitor outcomes\n\n\nTuolumne\n70,432\n6.66\nModerate Confidence\nUse with caution – monitor outcomes\n\n\n\n\n\nKey Recommendations:\nYour Task: Use your analysis results to provide specific guidance to the department.\n\nCounties suitable for immediate algorithmic implementation: Alameda, Butte, Contra Costa, El Dorado, Fresno, Humboldt, Imperial, Kern, Kings, Lake, Los Angeles, Madera Marin, Mendocino, Merced, Monterey, Napa, Nevada, Orange, Placer, Riverside, Sacramento, San Bernardino, San Diego, San Francisco, San Joaquin, San Luis Obispo, San Mateo, Santa Barbara, Santa Clara, Santa Cruz, Shasta,Siskiyou, Solano, Sonoma, Stainislaus, Sutter, Tulare, Ventura, Yolo, and Yuba\nCounties requiring additional oversight: Amador, Calaveras, Colusa, Del Norte, Glenn, Inyo, Lassen, Mariposa, Modoc, San Benito, Tehama, and Tuolumne\nCounties needing alternative approaches: Alpine, Mono, Plumas, Sierra, and Trinity"
  },
  {
    "objectID": "Assignments/Assignment_1/assignment1_template.html#questions-for-further-investigation",
    "href": "Assignments/Assignment_1/assignment1_template.html#questions-for-further-investigation",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Questions for Further Investigation",
    "text": "Questions for Further Investigation\n\nAre high-MOE tracts clustered spatially (e.g., along rural–urban boundaries or in specific regions of the state), or do they appear evenly dispersed?\nDo MOE patterns persist across ACS releases, or do they improve over time with larger samples? A time-series comparison could reveal whether underrepresentation of minority or rural communities is a persistent structural issue, similar to how you track flood or disaster impacts across years.\nHow do MOE patterns for racial and ethnic groups vary across states? Are high MOEs for Hispanic and Black populations a uniquely California phenomenon, or do they reflect a broader national issue embedded in ACS sampling design?"
  },
  {
    "objectID": "Assignments/Assignment_1/assignment1_template.html#submission-checklist",
    "href": "Assignments/Assignment_1/assignment1_template.html#submission-checklist",
    "title": "Assignment 1: Census Data Quality for Policy Decisions",
    "section": "Submission Checklist",
    "text": "Submission Checklist\nBefore submitting your portfolio link on Canvas:\n\nAll code chunks run without errors\nAll “[Fill this in]” prompts have been completed\nTables are properly formatted and readable\nExecutive summary addresses all four required components\nPortfolio navigation includes this assignment\nCensus API key is properly set\nDocument renders correctly to HTML\n\nRemember: Submit your portfolio URL on Canvas, not the file itself. Your assignment should be accessible at your-portfolio-url/assignments/assignment_1/your_file_name.html"
  },
  {
    "objectID": "Assignments/Assignment_2/AlAbbas_Mohamad_Assignment2.html",
    "href": "Assignments/Assignment_2/AlAbbas_Mohamad_Assignment2.html",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "Assignments/Assignment_2/AlAbbas_Mohamad_Assignment2.html#assignment-overview",
    "href": "Assignments/Assignment_2/AlAbbas_Mohamad_Assignment2.html#assignment-overview",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "",
    "text": "Learning Objectives: - Apply spatial operations to answer policy-relevant research questions - Integrate census demographic data with spatial analysis - Create publication-quality visualizations and maps - Work with spatial data from multiple sources - Communicate findings effectively for policy audiences"
  },
  {
    "objectID": "Assignments/Assignment_2/AlAbbas_Mohamad_Assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "href": "Assignments/Assignment_2/AlAbbas_Mohamad_Assignment2.html#part-1-healthcare-access-for-vulnerable-populations",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 1: Healthcare Access for Vulnerable Populations",
    "text": "Part 1: Healthcare Access for Vulnerable Populations\n\nResearch Question\nWhich Pennsylvania counties have the highest proportion of vulnerable populations (elderly + low-income) living far from hospitals?\n\n\nAnalysis Steps\nChecking & Setting CRS:\n\n# Check that all data loaded correctly\nst_crs(pa_counties)     #WGS 84\nst_crs(hospitals)       #WGS 84\nst_crs(census_tracts)   #NAD 83\n\n#Convert to same projection\nhospitals &lt;- st_transform(hospitals, st_crs(pa_counties))\ncensus_tracts &lt;- st_transform(census_tracts, st_crs(pa_counties))\n\nQuestions to answer:\n\nHow many hospitals are in your dataset?\nAnswer: 223 Hospitals.\nHow many census tracts?\nAnswer: 3,445 census tracts.\nWhat coordinate reference system is each dataset in?\nAnswer: Both PA counties & Hospitals are in WGS 84, whereas the census tracts are in NAD 83.\n\n\n\nStep 2: Get Demographic Data\nAPI Call:\n\nDemographic_variables &lt;- c(\n  total_pop = \"B01003_001\",  # Total population\n  medhhinc  = \"B19013_001\",  # Median household income\n  # Male population by age group (65+)\n  m65_66   = \"B01001_020\",\n  m67_69   = \"B01001_021\",\n  m70_74   = \"B01001_022\",\n  m75_79   = \"B01001_023\",\n  m80_84   = \"B01001_024\",\n  m85plus  = \"B01001_025\",\n  # Female population by age group (65+)\n  f65_66   = \"B01001_044\",\n  f67_69   = \"B01001_045\",\n  f70_74   = \"B01001_046\",\n  f75_79   = \"B01001_047\",\n  f80_84   = \"B01001_048\",\n  f85plus  = \"B01001_049\"\n)\n\nPA_acs &lt;- get_acs(\n  geography = \"tract\",\n  variables = Demographic_variables,\n  state = \"PA\",\n  year = 2022,\n  survey = \"acs5\",\n  output = \"wide\")\n\n# The `mutate()` function sums the estimates for all male and female\n# age groups aged 65 and above to produce a new variable `pop65plus`.\n\nPA_acs &lt;- PA_acs %&gt;%\n  mutate(pop65plus = (\n    m65_66E + m67_69E + m70_74E + m75_79E + m80_84E + m85plusE +\n    f65_66E + f67_69E + f70_74E + f75_79E + f80_84E + f85plusE\n  ))\n\n# Merge the demographic data (`PA_acs`) with the tract shapefile\n# (`census_tracts`) by the unique GEOID identifier.\n\nPA_tracts &lt;- census_tracts %&gt;%\n  left_join(PA_acs, by = \"GEOID\")\n\n# income across tracts:\n\nsummary(PA_tracts$medhhincE)\n\nQuestions to answer:\n\nWhat year of ACS data are you using?\n\nAnswer: 2022\n\nHow many tracts have missing income data?\n\nAnswer: 62\n\nWhat is the median income across all PA census tracts?\n\nAnswer: 70,188 USD\n\n\n\n\n\nStep 3: Vulnerable Populations\nDefining Vulnerability:\n\n# The variable `share65` represents the proportion of the total population\n# aged 65 and older. This will be used to identify tracts with a high\n# concentration of elderly residents.\n\nPA_tracts &lt;- PA_tracts %&gt;%\n  mutate(\n    share65 = pop65plus / total_popE\n  )\n\n\n# Low income: bottom quartile (≤ 25th percentile) of median household income\n# High elderly: top quartile (≥ 75th percentile) of the 65+ share\n\nInc_threshold &lt;- quantile(PA_tracts$medhhincE, probs = 0.25, na.rm = TRUE)\nage_threshold &lt;- quantile(PA_tracts$share65, probs = 0.75, na.rm = TRUE)\n\n\n# A tract is classified as “vulnerable” if BOTH conditions are met:\n# (1) Median household income is below or equal to the 25th percentile, AND\n# (2) Elderly share is above or equal to the 75th percentile.\n\nPA_tracts &lt;- PA_tracts %&gt;%\n  mutate(\n    low_income = medhhincE &lt;= Inc_threshold,  # Low-income indicator\n    older_pop  = share65 &gt;= age_threshold,    # High-elderly indicator\n    vulnerable = low_income & older_pop       # Combined vulnerability flag\n  )\n\n\n# `n_vuln`: total number of tracts meeting both criteria\n# `pct_vuln`: percentage of all tracts that are vulnerable\n\nn_vuln &lt;- sum(PA_tracts$vulnerable, na.rm = TRUE)\npct_vuln &lt;- n_vuln / nrow(PA_tracts)\n\n\n# The table reports the income and elderly thresholds used, the number of\n# vulnerable tracts, and their share of all tracts. `scales::dollar()` and\n# `scales::percent()` are used to format the values for readability.\n\nknitr::kable(\n  tibble::tibble(\n    `Income threshold (25th percentile)`        = scales::dollar(as.numeric(Inc_threshold)),\n    `Elderly share threshold (75th percentile)` = scales::percent(as.numeric(age_threshold), accuracy = 0.1),\n    `# Vulnerable tracts`                        = n_vuln,\n    `% of all tracts that are vulnerable`       = scales::percent(pct_vuln, accuracy = 0.1)\n  ),\n  caption = \"Vulnerability thresholds and counts (low income + high elderly share)\"\n)\n\nQuestions to answer:\n\nWhat income threshold did you choose and why?\n\nAnswer: Ideally, I would have selected the national U.S. poverty line of $32,150 as the income cutoff. However, this figure does not accurately reflect relative poverty across regions. For instance, a household earning $50,000 in New York may still experience financial vulnerability due to local cost-of-living differences.Moreover, because this analysis focuses on elderly populations, income alone may underestimate vulnerability — many older adults have non-income assets or retirement savings that distort what “low income” means within their demographic. Therefore, I use the 25th percentile, $55,923.50, of median household income across Pennsylvania census tracts as the cutoff for “low income.” This approach casts a wider and more context-sensitive net that captures tracts relatively disadvantaged within the state. Theoretically, tracts with a higher concentration of elderly residents and greater socioeconomic vulnerability are more likely to fall in this bottom income quartile, making it an efficient and justifiable criterion in the absence of more granular wealth data.\n\nWhat elderly population threshold did you choose and why?\n\nAnswer: For the elderly population threshold, I use the 75th percentile of the share of residents aged 65 and older, corresponding to the top quartile of tracts with the highest elderly concentrations. This approach identifies communities where aging populations are most prevalent and likely to face greater service and health-related vulnerabilities. In total\n\nHow many tracts meet your vulnerability criteria?\n\nAnswer: 164 tracts are considered vulnerable\n\nWhat percentage of PA census tracts are considered vulnerable by your definition?\n\nAnswer: 4.8%\n\n\n\n\n\nStep 4: Calculate Distance to Hospitals\nDistance Calculations:\n\n# Distances are only meaningful in a projected CRS (not WGS84 lon/lat).\n# Using a feet-based CRS lets st_distance() return feet directly.\n# We use the NAD83, 3365 projection designed for Pennsylvania South (ftUS)\n# as the state plane, 3365 is the ideal projection for distance calculations\npa_proj &lt;- st_transform(pa_counties, 3365)\nhospitals_proj &lt;- st_transform(hospitals, 3365)\ntracts_proj &lt;- st_transform(PA_tracts, 3365)\n\n# Compute polygon centroids AFTER projection. (Safer than doing so in lon/lat.)\n# If you need guaranteed interior points for odd-shaped tracts, consider st_point_on_surface().\ntract_centroids &lt;- tracts_proj %&gt;%\n  st_centroid()\n\n# st_distance() returns an N_tracts x N_hospitals matrix with linear units of the CRS (here: feet).\ndist_matrix &lt;- st_distance(tract_centroids, hospitals_proj)\n\n# Take the minimum across each row (each tract) → nearest hospital distance (feet).\ntract_centroids$nearest_hosp_ft &lt;- apply(dist_matrix, 1, min)\n\n# Also compute miles for easier interpretation.\ntract_centroids &lt;- tract_centroids %&gt;%\n  mutate(nearest_hosp_miles = nearest_hosp_ft / 5280)\n\nPA_tracts &lt;- tracts_proj %&gt;%\n  left_join(\n    tract_centroids %&gt;%\n      st_drop_geometry() %&gt;%\n      select(GEOID, nearest_hosp_miles),\n    by = \"GEOID\"\n  )\n\n# Average distance to nearest hospital (miles)\n# Maximum distance (miles)\n# Count of tracts farther than 15 miles\n\nvuln_summary &lt;- PA_tracts %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(vulnerable == TRUE) %&gt;%\n  summarize(\n    avg_distance_miles = mean(nearest_hosp_miles, na.rm = TRUE),\n    max_distance_miles = max(nearest_hosp_miles, na.rm = TRUE),\n    tracts_over_15mi = sum(nearest_hosp_miles &gt; 15, na.rm = TRUE)\n  )\n\nknitr::kable(vuln_summary, caption = \"Distance to Nearest Hospital (Vulnerable Tracts)\")\n\nQuestions to answer:\n\nWhat is the average distance to the nearest hospital for vulnerable tracts?\n\nAnswer: 4.74 miles.\n\nWhat is the maximum distance?\n\nAnswer: 19.16 miles.\n\nHow many vulnerable tracts are more than 15 miles from the nearest hospital?\n\nAnswer: Nine.\n\n\n\n\n\nStep 5: Identify Underserved Areas\nUnderserved Areas:\n\n# A tract is classified as underserved if it meets BOTH conditions:\n#   (1) It is already marked as \"vulnerable\" (low income + high elderly share)\n#   (2) It lies more than 15 miles from the nearest hospital\n#\n# This combined condition identifies tracts that are both socioeconomically\n# and spatially disadvantaged in terms of healthcare access.\n\nPA_tracts &lt;- PA_tracts %&gt;%\n  mutate(\n    underserved = vulnerable & nearest_hosp_miles &gt; 15\n  )\n\n# `total_vulnerable`: total number of tracts identified as vulnerable\n# `total_underserved`: number of vulnerable tracts that are also &gt;15 miles away\n# `pct_underserved`: share (%) of vulnerable tracts that are underserved\n\nunderserved_summary &lt;- PA_tracts %&gt;%\n  st_drop_geometry() %&gt;%\n  summarize(\n    `Total Vulnerable Tracts`  = sum(vulnerable, na.rm = TRUE),\n    `Total Underserved Tracts` = sum(underserved, na.rm = TRUE),\n    `Percent Underserved (%)`  = round(sum(underserved, na.rm = TRUE) /\n                                       sum(vulnerable, na.rm = TRUE) * 100, 2)\n  )\n\nknitr::kable(\n  underserved_summary,\n  caption = \"Underserved Tracts Summary (Vulnerable + &gt;15 miles from Hospital)\",\n  align = c(\"c\", \"c\", \"c\")\n)\n\n\nUnderserved Tracts Summary (Vulnerable + &gt;15 miles from Hospital)\n\n\n\n\n\n\n\nTotal Vulnerable Tracts\nTotal Underserved Tracts\nPercent Underserved (%)\n\n\n\n\n165\n9\n5.45\n\n\n\n\n\nQuestions to answer:\n\nHow many tracts are underserved?\n\nAnswer: A total of nine census tracts are classified as underserved, that is, they meet both criteria of being socioeconomically vulnerable and located more than 15 miles from the nearest hospital.\n\nWhat percentage of vulnerable tracts are underserved?\n\nAnswer: These nine tracts represent approximately 5.45 % of all 165 vulnerable tracts identified in the analysis.\n\nDoes this surprise you? Why or why not?\n\nAnswer: Not particularly. Pennsylvania is known for its dense healthcare network, so it is unsurprising that only a small percentage (5.45%) of vulnerable tracts are geographically isolated. Moreover, elderly individuals tend to gravitate toward areas with better healthcare access, while healthcare conglomerates and hospitals are likewise inclined to locate in regions where their primary patient base already resides. However, even this small cluster of underserved tracts may represent significant service gaps, particularly if these areas contain a high concentration of older residents. It would be worthwhile to examine the racial and age composition of these tracts to assess whether patterns of racial segregation or elderly disenfranchisement contribute to disparities in healthcare accessibility.\n\n\n\n\n\nStep 6: Aggregate to County Level\nDemographics of Underserved:\n\n# Spatial join tracts to counties\n\nPA_sjoin &lt;- PA_tracts %&gt;%\n  st_join(pa_proj)\n\n# For each county, compute:\n# Total number of tracts\n# Mean tract area\n# Count of vulnerable and underserved tracts\n# % of vulnerable tracts that are underserved\n# Average distance (mi) to nearest hospital for vulnerable tracts\n# Total elderly population in vulnerable tracts\n\ncounty_summaries &lt;- PA_sjoin %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(COUNTY_NAM) %&gt;%\n  summarize(\n    `Number of Tracts`                = n(),\n    `Average Tract Area (sq mi)`      = round(mean(AREA_SQ_MI, na.rm = TRUE), 2),\n    `Vulnerable Tracts`               = sum(vulnerable, na.rm = TRUE),\n    `Underserved Tracts`              = sum(underserved, na.rm = TRUE),\n    `Percent Underserved (%)`         = ifelse(\n      sum(vulnerable, na.rm = TRUE) &gt; 0,\n      round(sum(underserved, na.rm = TRUE) / sum(vulnerable, na.rm = TRUE) * 100, 2),\n      0\n    ),\n    `Avg Dist (mi) - Vulnerable`      = ifelse(\n      sum(vulnerable, na.rm = TRUE) &gt; 0,\n      round(mean(nearest_hosp_miles[vulnerable], na.rm = TRUE), 2),\n      NA_real_\n    ),\n    `Total Vulnerable Pop (65+)`      = ifelse(\n      sum(vulnerable, na.rm = TRUE) &gt; 0,\n      sum(pop65plus[vulnerable], na.rm = TRUE),\n      0\n    )\n  )\n\n# Identify top 5 counties by % underserved\n\ntop5_underserved &lt;- county_summaries %&gt;%\n  arrange(desc(`Percent Underserved (%)`),desc(`Total Vulnerable Pop (65+)`)) %&gt;%\n  slice_head(n = 5)\n\n\nknitr::kable(\n  top5_underserved,\n  caption = \"Top 5 Counties with Highest Percentage of Underserved Vulnerable Tracts\",\n  align = \"c\"\n)\n\nQuestions to answer:\n\nWhich 5 counties have the highest percentage of underserved vulnerable tracts?\n\nAnswer: Bradford, Columbia, Juniata, Monroe, and Perry\n\nWhich counties have the most vulnerable people living far from hospitals?\n\nAnswer: Perry, Bradford, Juniata, Monroe, and Columbia , in that order.\n\nAre there any patterns in where underserved counties are located?\n\nAnswer: They appear to be mostly rural counties, and all five counties appear in PA’s MUAs index of underserved areas, so I am confident that the techniques are accurate. Interestingly, all five are white dominant population, but republican which indicates an older generation.\n\n\n\n\n\nStep 7: Create Summary Table\nSummary Tabulation:\n\n# Step 1: Compute a composite priority score\n# Combines percent underserved (severity) × total vulnerable population (scale)\ncounty_priorities &lt;- county_summaries %&gt;%\n  mutate(\n    priority_score      = (`Percent Underserved (%)` * `Total Vulnerable Pop (65+)`) / 1000,\n    priority_percentile = percent_rank(priority_score) * 100\n  )\n\n# Step 2: Select the top 10 counties by priority score\ntop_priority &lt;- county_priorities %&gt;%\n  arrange(desc(priority_score)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  select(\n    County                              = COUNTY_NAM,\n    `# Vulnerable Tracts`               = `Vulnerable Tracts`,\n    `# Underserved Tracts`              = `Underserved Tracts`,\n    `% Underserved`                     = `Percent Underserved (%)`,\n    `Avg. Distance to Hospital (miles)` = `Avg Dist (mi) - Vulnerable`,\n    `Total Vulnerable Population (65+)` = `Total Vulnerable Pop (65+)`,\n    `Priority Score`                    = priority_score,\n    `Percentile Rank`                   = priority_percentile\n  ) %&gt;%\n  mutate(\n    `% Underserved`                     = scales::percent(`% Underserved` / 100, accuracy = 0.1),\n    `Avg. Distance to Hospital (miles)` = round(`Avg. Distance to Hospital (miles)`, 1),\n    `Total Vulnerable Population (65+)` = scales::comma(`Total Vulnerable Population (65+)`, accuracy = 1),\n    `Priority Score`                    = scales::comma(round(`Priority Score`, 0)),\n    `Percentile Rank`                   = scales::percent(`Percentile Rank` / 100, accuracy = 0.1)\n  )\n\n# Step 3: Display formatted table\nknitr::kable(\n  top_priority,\n  caption = \"Top 10 Pennsylvania Counties for Healthcare Investment Priority (Vulnerability × Access)\",\n  align   = c(\"l\",\"c\",\"c\",\"c\",\"c\",\"c\",\"c\",\"c\"),\n  booktabs = TRUE\n)\n\n\nTop 10 Pennsylvania Counties for Healthcare Investment Priority (Vulnerability × Access)\n\n\n\n\n\n\n\n\n\n\n\n\nCounty\n# Vulnerable Tracts\n# Underserved Tracts\n% Underserved\nAvg. Distance to Hospital (miles)\nTotal Vulnerable Population (65+)\nPriority Score\nPercentile Rank\n\n\n\n\nPERRY\n2\n2\n100.0%\n17.5\n1,558\n156\n100.0%\n\n\nBRADFORD\n1\n1\n100.0%\n16.7\n1,545\n154\n98.5%\n\n\nDAUPHIN\n2\n1\n50.0%\n9.9\n2,122\n106\n97.0%\n\n\nCLEARFIELD\n5\n1\n20.0%\n10.1\n4,368\n87\n95.5%\n\n\nFRANKLIN\n3\n1\n33.3%\n7.0\n2,570\n86\n93.9%\n\n\nWARREN\n7\n1\n14.3%\n6.9\n5,963\n85\n92.4%\n\n\nVENANGO\n4\n1\n25.0%\n10.1\n3,344\n84\n90.9%\n\n\nPOTTER\n4\n1\n25.0%\n8.4\n3,219\n80\n89.4%\n\n\nCRAWFORD\n6\n1\n16.7%\n8.4\n4,683\n78\n87.9%\n\n\nLUZERNE\n8\n1\n12.5%\n5.0\n6,125\n77\n86.4%"
  },
  {
    "objectID": "Assignments/Assignment_2/AlAbbas_Mohamad_Assignment2.html#part-2-comprehensive-visualization",
    "href": "Assignments/Assignment_2/AlAbbas_Mohamad_Assignment2.html#part-2-comprehensive-visualization",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 2: Comprehensive Visualization",
    "text": "Part 2: Comprehensive Visualization\n\nMap 1: County-Level Choropleth\nCounty-Level Vulnerability:\n\n# Create county-level access map\ncounties_map &lt;- pa_proj %&gt;%\n  left_join(county_summaries, by = \"COUNTY_NAM\")\n\n# Map: % of vulnerable tracts underserved + hospitals (with shared legend)\nggplot() +\n  # County polygons colored by % underserved\n  geom_sf(\n    data = counties_map,\n    aes(fill = `Percent Underserved (%)`),\n    color = \"white\",\n    size  = 0.2\n  ) +\n  # Hospital points added to legend\n  geom_sf(\n    data  = hospitals_proj,\n    aes(shape = \"Hospitals\"),     # &lt;— adds to legend\n    color = \"darkgreen\",\n    size  = 1.5,\n    alpha = 0.8,\n    inherit.aes = FALSE\n  ) +\n  # Continuous viridis fill scale\n  scale_fill_viridis_c(\n    name   = \"% of vulnerable tracts underserved\",\n    labels = scales::label_number(accuracy = 1, suffix = \"%\"),\n    limits = c(0, 100),\n    option = \"plasma\",\n    na.value = \"grey90\"\n  ) +\n  # Add a manual shape legend for hospitals\n  scale_shape_manual(\n    name   = NULL,\n    values = c(\"Hospitals\" = 16)  # solid circle\n  ) +\n  # Combine legends neatly (fill first, shape second)\n  guides(\n    fill  = guide_colorbar(order = 1, barwidth = 0.7, barheight = 8),\n    shape = guide_legend(order = 2, override.aes = list(size = 3, color = \"darkgreen\"))\n  ) +\n  labs(\n    title    = \"Healthcare Access Challenges Across Pennsylvania\",\n    subtitle = \"% of vulnerable tracts (low-income & elderly) &gt;15 miles from the nearest hospital\",\n    caption  = \"Sources: ACS 2018–2022 (tidycensus); PA Counties & Hospitals (course data). Projection: EPSG:3365\"\n  ) +\n  coord_sf(datum = NA) +\n  theme_void() +\n  theme(\n    legend.position = \"right\",\n    legend.title    = element_text(size = 10, face = \"bold\"),\n    legend.text     = element_text(size = 9),\n    plot.title      = element_text(face = \"bold\", size = 14),\n    plot.subtitle   = element_text(size = 11),\n    plot.caption    = element_text(size = 9)\n  )\n\n\n\n\n\n\n\n\n\n\n\nMap 2: Detailed Vulnerability Map\nVulnerability Maps:\n\n# 1) Prepare tract status for mapping\ntracts_for_map &lt;- PA_tracts %&gt;%\n  sf::st_transform(3365) %&gt;%\n  dplyr::mutate(\n    tract_status = dplyr::case_when(\n      underserved ~ \"Underserved vulnerable\",\n      vulnerable  ~ \"Vulnerable (≤ 15 mi)\",\n      TRUE        ~ \"Other tracts\"\n    ),\n    tract_status = factor(\n      tract_status,\n      levels = c(\"Underserved vulnerable\", \"Vulnerable (≤ 15 mi)\", \"Other tracts\")\n    )\n  )\n\n# 2) Plot with hospitals symbol integrated into legend\nggplot() +\n  # Background tracts\n  geom_sf(\n    data  = dplyr::filter(tracts_for_map, tract_status == \"Other tracts\"),\n    fill  = \"#F2F2F2\", color = NA\n  ) +\n  # Vulnerable but near hospitals\n  geom_sf(\n    data  = dplyr::filter(tracts_for_map, tract_status == \"Vulnerable (≤ 15 mi)\"),\n    aes(fill = tract_status),\n    color = NA,\n    alpha = 0.9\n  ) +\n  # Underserved vulnerable\n  geom_sf(\n    data  = dplyr::filter(tracts_for_map, tract_status == \"Underserved vulnerable\"),\n    aes(fill = tract_status),\n    color = NA\n  ) +\n  # County outlines\n  geom_sf(\n    data  = pa_proj,\n    fill  = NA, color = \"white\", linewidth = 0.4\n  ) +\n  # Hospital locations (added to legend)\n  geom_sf(\n    data = hospitals_proj,\n    aes(shape = \"Hospitals\"),  # &lt;—— This makes a new legend key for hospitals\n    color = \"black\",\n    size  = 1.1,\n    alpha = 0.8,\n    inherit.aes = FALSE\n  ) +\n  # Manual fill colors\n  scale_fill_manual(\n    name   = NULL,\n    values = c(\n      \"Underserved vulnerable\" = \"#C0392B\",\n      \"Vulnerable (≤ 15 mi)\"   = \"#7F8C8D\"\n    ),\n    breaks = c(\"Underserved vulnerable\", \"Vulnerable (≤ 15 mi)\"),\n    drop   = FALSE\n  ) +\n  # Add legend symbol for hospitals\n  scale_shape_manual(\n    name   = NULL,\n    values = c(\"Hospitals\" = 16)  # solid circle\n  ) +\n  guides(\n    fill  = guide_legend(order = 1, override.aes = list(alpha = 1)),\n    shape = guide_legend(order = 2, override.aes = list(size = 3, color = \"black\"))\n  ) +\n  labs(\n    title    = \"Underserved Vulnerable Census Tracts in Pennsylvania\",\n    subtitle = \"Tracts with low income & high elderly share located &gt;15 miles from the nearest hospital\",\n    caption  = \"Sources: ACS 2018–2022 (tidycensus); Course hospitals & PA counties. Projection: EPSG:3365\"\n  ) +\n  coord_sf(datum = NA) +\n  theme_void() +\n  theme(\n    legend.position = \"right\",\n    legend.title    = element_blank(),\n    legend.text     = element_text(size = 10),\n    plot.title      = element_text(face = \"bold\", size = 14),\n    plot.subtitle   = element_text(size = 11),\n    plot.caption    = element_text(size = 9)\n  )\n\n\n\n\n\n\n\n\n\n\n\nChart: Distribution Analysis\nFigure 1:\n\n# --- Prep: tract-level data (vulnerable only) ---\ntract_vuln &lt;- PA_tracts %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(vulnerable == TRUE, !is.na(nearest_hosp_miles))\n\n# --- Histogram: distance distribution among vulnerable tracts ---\np_hist &lt;- ggplot(tract_vuln, aes(x = nearest_hosp_miles)) +\n  geom_histogram(bins = 30, alpha = 0.8, fill = \"gray40\") +\n  geom_density(aes(y = after_stat(..count..)), linewidth = 0.8, alpha = 0.4) +\n  geom_vline(xintercept = 15, linewidth = 0.7, linetype = \"dashed\") +\n  labs(\n    title = \"Distance to Nearest Hospital (Vulnerable Tracts)\",\n    subtitle = \"Dashed line at 15 miles (underserved threshold)\",\n    x = \"Distance to nearest hospital (miles)\",\n    y = \"Number of tracts\",\n    caption = \"Sample: PA tracts flagged as vulnerable (low income & high elderly share).\"\n  ) +\n  theme_minimal(base_size = 11)\n\np_hist\n\n\n\n\nThe distribution is heavily left-skewed, with most vulnerable tracts located close to hospitals. Only a few exceed the 15-mile threshold, confirming that long-distance access barriers are rare but geographically distinct.\n\n\n\n\nFigure 2:\n\np_scatter &lt;- ggplot(tract_vuln, aes(x = nearest_hosp_miles, y = pop65plus)) +\n  geom_point(alpha = 0.4) +\n  geom_vline(xintercept = 15, linewidth = 0.7, linetype = \"dashed\") +\n  scale_y_continuous(labels = scales::comma) +\n  labs(\n    title = \"Distance vs. Size of Vulnerable Population (Tract-Level)\",\n    subtitle = \"Each point is a vulnerable tract; vertical line marks 15 miles\",\n    x = \"Distance to nearest hospital (miles)\",\n    y = \"Vulnerable population (65+)\"\n  ) +\n  theme_minimal(base_size = 11)\n\np_scatter\n\n\n\n\nMost vulnerable tracts lie within 10 miles of a hospital, though a few outliers exceed the 15-mile cutoff. Larger elderly populations are not systematically farther away, suggesting that isolation is primarily geographic rather than demographic."
  },
  {
    "objectID": "Assignments/Assignment_2/AlAbbas_Mohamad_Assignment2.html#part-3-bring-your-own-data-analysis",
    "href": "Assignments/Assignment_2/AlAbbas_Mohamad_Assignment2.html#part-3-bring-your-own-data-analysis",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Part 3: Bring Your Own Data Analysis",
    "text": "Part 3: Bring Your Own Data Analysis\n\nYour Analysis\n\nLoading and Synchronziing Fire/EMS Data with Census Tract\n\n\nlibrary(sf)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(units)\nlibrary(ggplot2)\nlibrary(knitr)\nlibrary(scales)\nlibrary(tidycensus)\n\n# ---- Load data ----\n# Fire/EMS stations (single shapefile)\nstations_raw &lt;- st_read(\"FIRE_EMS/Fire_and_Emergency_Medical_Service__EMS__Stations_pt.shp\", quiet = TRUE)\n\n# Project counties and tracts you already have (from earlier parts)\nstations_proj &lt;- st_transform(stations_raw, 3365)\npa_proj       &lt;- st_transform(pa_counties, 3365)   # county polygons\ntracts_proj   &lt;- st_transform(PA_tracts, 3365)     # tracts with total_popE\n\n# ---- 1) Dataset description (answers: source/date, features, CRS) ----\ndataset_info &lt;- tibble::tibble(\n  `Dataset`        = \"Fire & Emergency Medical Service (EMS) Stations (points)\",\n  `Source`         = \"Pennsylvania Spatial Data Access (PASDA), Homeland Infrastructure Foundation Level Data HIFLD\",\n  `Data vintage`   = \"2025\",\n  `Feature count`  = nrow(stations_raw),\n  `Geometry type`  = paste(unique(as.character(st_geometry_type(stations_raw))), collapse = \", \"),\n  `Original CRS`   = paste0(st_crs(stations_raw)$epsg, \" — \", st_crs(stations_raw)$Name),\n  `Analysis CRS`   = paste0(\"EPSG:3365 — \", st_crs(stations_proj)$Name)\n)\n\nknitr::kable(dataset_info, caption = \"Fire/EMS dataset: source, features, and CRS\")\n\n\nFire/EMS dataset: source, features, and CRS\n\n\n\n\n\n\n\n\n\n\n\nDataset\nSource\nData vintage\nFeature count\nGeometry type\nOriginal CRS\nAnalysis CRS\n\n\n\n\nFire & Emergency Medical Service (EMS) Stations (points)\nPennsylvania Spatial Data Access (PASDA), Homeland Infrastructure Foundation Level Data HIFLD\n2025\n2591\nPOINT\n3857 — WGS 84 / Pseudo-Mercator\nEPSG:3365 — NAD83(HARN) / Pennsylvania South (ftUS)\n\n\n\n\n\nQuestions to answer:\n\nWhat dataset did you choose and why?\n\nAnswer: The Fire & Emergency Medical Service (EMS) Stations point layer for Pennsylvania (single shapefile). It’s the right basis to test response coverage—we can buffer stations, apportion population coverage by tract, and compare coverage against population density and minority share.\n\nWhat is the data source and date?\n\nAnswer: Pennsylvania Spatial Data Access (PASDA), Homeland Infrastructure Foundation Level Data HIFLD, for the year 2025.\n\nHow many features does it contain?\n\nAnswer: 2,591 station points.\n\nWhat CRS is it in? Did you need to transform it?\n\nAnswer: WGS84, transformed to 3365 for spatial calculations.\n\n\n\n\nResearch question\n\nWhich Pennsylvania tracts are high-priority for Fire/EMS expansion, and where do these gaps overlap with high-minority communities?\n\n\nSpatial analysis\n\nCoverage and Equity:\n\n# What this chunk does:\n#  1) Pulls ACS race (B03002) to compute minority share per tract\n#  2) Builds 2-mile service areas around Fire/EMS stations (EPSG:3365, ftUS)\n#  3) Area-weights tract populations to estimate % population covered vs. uncovered\n#  4) Aggregates key metrics to counties\n#  5) Outputs:\n#     - Statewide table (incl. \"% Uncovered that is Minority\")\n#     - Equity Table A (Top 10 counties by estimated minority uncovered)\n#     - Tract-level Coverage vs. Density Map (deficit index)\n#     - Tract-level Coverage Equity Map (high-minority + low-coverage)\n#\n# Notes:\n#  - Distance proxy: 2 miles ≈ ~5-min drive (network service areas would be ideal)\n#  - Population coverage is area-weighted at the tract level\n#  - EPSG:3365 (PA South, ftUS) is used for accurate buffering/distances\n\n\n# Pull total population and non-Hispanic White counts at the tract level\nrace_vars &lt;- c(total = \"B03002_001\", white_nonhisp = \"B03002_003\")\n\npa_race &lt;- get_acs(\n  geography = \"tract\",\n  variables = race_vars,\n  state     = \"PA\",\n  year      = 2022,\n  survey    = \"acs5\",\n  output    = \"wide\"\n) %&gt;%\n  transmute(\n    GEOID,\n    total         = totalE,\n    white_nonhisp = white_nonhispE\n  ) %&gt;%\n  mutate(\n    # Count of residents who are NOT non-Hispanic White\n    minority     = pmax(total - white_nonhisp, 0),\n    pct_minority = if_else(total &gt; 0, minority / total * 100, NA_real_)\n  )\n\n# Project counties and tracts to EPSG:3365 and attach race\npa_proj &lt;- st_transform(pa_counties, 3365)\ntracts_proj &lt;- st_transform(PA_tracts, 3365) %&gt;%\n  left_join(pa_race, by = \"GEOID\")\n\n\n# Build 2-mile (= 10,560 ft) coverage\ntwo_miles_ft &lt;- 10560\n\n# Buffer Fire/EMS stations and dissolve into one coverage polygon\nstns_buffer    &lt;- st_buffer(stations_proj, dist = two_miles_ft)\ncoverage_union &lt;- st_union(stns_buffer) |&gt; st_as_sf()\n\n# Clip to Pennsylvania extent for clean intersections/plots\ncoverage_union &lt;- st_intersection(coverage_union, st_union(pa_proj) |&gt; st_as_sf())\n\n# Compute tract areas in square miles (for density + apportionment)\ntracts_proj$area_mi2 &lt;- as.numeric(set_units(st_area(tracts_proj), \"mi^2\"))\n\n# Intersect coverage with tracts to get covered sub-areas\nt_cov_parts &lt;- st_intersection(tracts_proj, coverage_union)\n\n# Sum covered area per tract (sq mi)\nt_cov_area &lt;- t_cov_parts %&gt;%\n  mutate(area_cov_mi2 = as.numeric(set_units(st_area(geometry), \"mi^2\"))) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(GEOID) %&gt;%\n  summarise(area_cov_mi2 = sum(area_cov_mi2, na.rm = TRUE), .groups = \"drop\")\n\n# Combine: density, coverage fraction, covered/uncovered population\ntracts_cov &lt;- tracts_proj %&gt;%\n  left_join(t_cov_area, by = \"GEOID\") %&gt;%\n  mutate(\n    area_cov_mi2  = replace_na(area_cov_mi2, 0),\n    pop           = total_popE,                                # from your earlier ACS pull\n    pop_density   = if_else(area_mi2 &gt; 0, pop / area_mi2, NA_real_),\n    cover_frac    = pmin(1, if_else(area_mi2 &gt; 0, area_cov_mi2 / area_mi2, 0)),\n    pop_cov       = pop * cover_frac,\n    pop_uncovered = pmax(0, pop - pop_cov)\n  ) %&gt;%\n  # Recompute minority fields on THIS object to guarantee availability downstream\n  mutate(\n    minority     = if_else(!is.na(total) & !is.na(white_nonhisp),\n                           pmax(total - white_nonhisp, 0), NA_real_),\n    pct_minority = if_else(!is.na(total) & total &gt; 0,\n                           minority / total * 100, NA_real_)\n  )\n\n\n# Estimate minority share within the uncovered population at the tract level\ntracts_equity &lt;- tracts_cov %&gt;%\n  mutate(\n    minority_share_tr   = if_else(!is.na(total) & total &gt; 0, minority / total, NA_real_),\n    est_uncovered_minor = pop_uncovered * minority_share_tr\n  )\n\n# Compute the statewide row (raw)\nstatewide_tbl_raw &lt;- tracts_equity %&gt;%\n  st_drop_geometry() %&gt;%\n  summarise(\n    Total_Population             = sum(pop, na.rm = TRUE),\n    Covered_Population           = sum(pop_cov, na.rm = TRUE),\n    Uncovered_Population         = sum(pop_uncovered, na.rm = TRUE),\n    Estimated_Uncovered_Minority = sum(est_uncovered_minor, na.rm = TRUE),\n    Stations                     = nrow(stations_proj)\n  ) %&gt;%\n  mutate(\n    Pct_Covered                    = Covered_Population   / Total_Population,\n    Pct_Uncovered                  = Uncovered_Population / Total_Population,\n    Pct_Uncovered_That_Is_Minority = if_else(\n      Uncovered_Population &gt; 0,\n      Estimated_Uncovered_Minority / Uncovered_Population,\n      NA_real_\n    )\n  )\n\n# Make a display-friendly, formatted one-row table\nstatewide_tbl_display &lt;- statewide_tbl_raw %&gt;%\n  transmute(\n    `Total population`                     = scales::comma(Total_Population),\n    `Covered population`                   = scales::comma(Covered_Population),\n    `Uncovered population`                 = scales::comma(Uncovered_Population),\n    `Estimated uncovered minority`         = scales::comma(Estimated_Uncovered_Minority),\n    `Stations (count)`                     = scales::comma(Stations),\n    `Percent covered`                      = scales::percent(Pct_Covered, accuracy = 0.01),\n    `Percent uncovered`                    = scales::percent(Pct_Uncovered, accuracy = 0.01),\n    `Uncovered that is minority`           = scales::percent(Pct_Uncovered_That_Is_Minority, accuracy = 0.01)\n  )\n\nknitr::kable(\n  statewide_tbl_display,\n  caption = \"Statewide coverage & equity (area-weighted, 2-mile buffers)\",\n  align   = \"c\",\n  booktabs = TRUE\n)\n\n\nStatewide coverage & equity (area-weighted, 2-mile buffers)\n\n\n\n\n\n\n\n\n\n\n\n\nTotal population\nCovered population\nUncovered population\nEstimated uncovered minority\nStations (count)\nPercent covered\nPercent uncovered\nUncovered that is minority\n\n\n\n\n12,989,208\n10,854,031\n2,135,177\n197,764\n2,591\n83.56%\n16.44%\n9.26%\n\n\n\n\n# Count stations per county for context (optional columns if you want to show later)\nstns_in_county &lt;- st_join(stations_proj, pa_proj, join = st_within)\ncounty_station_counts &lt;- stns_in_county %&gt;%\n  st_drop_geometry() %&gt;%\n  count(COUNTY_NAM, name = \"Stations\")\n\n# Aggregate tract metrics to counties\nt2c &lt;- st_join(tracts_equity, pa_proj) %&gt;% st_drop_geometry()\n\ncounty_equity &lt;- t2c %&gt;%\n  group_by(COUNTY_NAM) %&gt;%\n  summarise(\n    Total_Population             = sum(pop, na.rm = TRUE),\n    Covered_Population           = sum(pop_cov, na.rm = TRUE),\n    Uncovered_Population         = sum(pop_uncovered, na.rm = TRUE),\n    Minority_Population          = sum(minority, na.rm = TRUE),\n    Estimated_Uncovered_Minority = sum(est_uncovered_minor, na.rm = TRUE),\n    .groups = \"drop\"\n  ) %&gt;%\n  left_join(county_station_counts, by = \"COUNTY_NAM\") %&gt;%\n  mutate(\n    `Percent Covered` = if_else(Total_Population &gt; 0,\n                                Covered_Population / Total_Population * 100, NA_real_)\n  )\n\n# extracting top ten counties with high unocvered minority levels. \nequity_top_abs &lt;- county_equity %&gt;%\n  arrange(desc(Estimated_Uncovered_Minority)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  select(\n    County                          = COUNTY_NAM,\n    `Estimated Minority Uncovered`  = Estimated_Uncovered_Minority,\n    `Total Uncovered`               = Uncovered_Population,\n    `Percent Covered`               = `Percent Covered`,\n    Stations\n  ) %&gt;%\n  mutate(\n    across(c(`Estimated Minority Uncovered`, `Total Uncovered`, Stations), comma),\n    across(c(`Percent Covered`), ~round(.x, 1))\n  )\n\nknitr::kable(\n  equity_top_abs,\n  caption = \"Equity Table: Top 10 Counties by Estimated Minority Uncovered Population\",\n  align   = \"c\"\n)\n\n\nEquity Table: Top 10 Counties by Estimated Minority Uncovered Population\n\n\n\n\n\n\n\n\n\nCounty\nEstimated Minority Uncovered\nTotal Uncovered\nPercent Covered\nStations\n\n\n\n\nMONROE\n31,484\n104,348\n56.9\n28\n\n\nCHESTER\n27,528\n167,305\n75.6\n47\n\n\nPIKE\n15,027\n53,880\n42.0\n22\n\n\nLANCASTER\n12,138\n127,043\n80.2\n80\n\n\nYORK\n11,069\n112,851\n80.4\n63\n\n\nFRANKLIN\n10,763\n118,915\n44.0\n19\n\n\nCUMBERLAND\n9,550\n104,646\n72.2\n39\n\n\nWAYNE\n8,598\n50,818\n50.1\n25\n\n\nCENTRE\n8,418\n100,669\n54.5\n23\n\n\nADAMS\n8,250\n83,701\n54.8\n22\n\n\n\n\n# Z-score standardization and the “deficiency index”\n# --------------------------------------------------\n# We compare two quantities on different scales:\n#   (1) pop_density_i          = population per square mile in tract i\n#   (2) pct_pop_covered_i      = % of tract i’s population within 2 miles of a station\n#\n# To make them comparable, we standardize each with a z-score:\n#   z(x_i) = (x_i − mean(x)) / sd(x), computed across all Pennsylvania tracts.\n# This rescales both variables to unitless, mean-0, sd-1 measures so differences\n# reflect relative position within the statewide distribution (not raw units).\n#\n# Deficiency index (higher = denser but under-covered):\n#   deficiency_i = z(pop_density_i) − z(pct_pop_covered_i)\n# Interpretation:\n#   - Positive : tract is relatively dense given its relatively low coverage\n#                  (potential service gap / higher priority).\n#   - 0        : density and coverage are in balance relative to peers.\n#   - Negative : tract is relatively sparse and/or relatively well covered.\n\ntracts_deficit &lt;- tracts_cov %&gt;%\n  mutate(\n    pct_pop_covered = if_else(pop &gt; 0, (pop_cov / pop) * 100, NA_real_),\n    z_density       = as.numeric(scale(pop_density)),\n    z_coverage      = as.numeric(scale(pct_pop_covered)),\n    deficit_index   = z_density - z_coverage  # higher = dense but poorly covered\n  )\n\nggplot() +\n  geom_sf(\n    data  = tracts_deficit,\n    aes(fill = deficit_index),\n    color = NA\n  ) +\n  geom_sf(\n    data  = pa_proj,\n    fill  = NA, color = \"white\", linewidth = 0.25\n  ) +\n  geom_sf(\n    data  = stations_proj,\n    aes(shape = \"Fire/EMS stations\"),\n    color = \"darkgreen\", size = 0.9, alpha = 0.85, inherit.aes = FALSE\n  ) +\n  scale_fill_gradient2(\n    name     = \"Coverage Deficit Index\\n(+ = dense but low coverage)\",\n    low      = \"#2b83ba\", mid = \"white\", high = \"#d7191c\",\n    midpoint = 0\n  ) +\n  scale_shape_manual(name = NULL, values = c(\"Fire/EMS stations\" = 16)) +\n  guides(\n    fill  = guide_colorbar(order = 1, barheight = 8),\n    shape = guide_legend(order = 2, override.aes = list(size = 2.2, color = \"darkgreen\"))\n  ) +\n  labs(\n    title    = \"Coverage vs. Density (Tract-Level): Potential Fire/EMS Service Gaps\",\n    subtitle = \"Index = z(population density) − z(% of tract population within 2 miles of a station)\",\n    caption  = \"Buffers ≈ 2 miles (EPSG:3365). Coverage apportioned by tract area overlap.\"\n  ) +\n  coord_sf(datum = NA) +\n  theme_void() +\n  theme(\n    legend.position = \"right\",\n    plot.title      = element_text(face = \"bold\", size = 14),\n    plot.subtitle   = element_text(size = 11),\n    plot.caption    = element_text(size = 9)\n  )\n\n\n\n\n\n\n\n# Thresholds: top quartile minority share, bottom quartile coverage\nthr_minority_tr &lt;- quantile(tracts_deficit$pct_minority,     0.75, na.rm = TRUE)\nthr_cover_tr    &lt;- quantile(tracts_deficit$pct_pop_covered, 0.25, na.rm = TRUE)\n\ntracts_equity_tr &lt;- tracts_deficit %&gt;%\n  mutate(\n    high_minority_tr = pct_minority     &gt;= thr_minority_tr,\n    low_cover_tr     = pct_pop_covered  &lt;= thr_cover_tr,\n    flag_both_tr     = high_minority_tr & low_cover_tr\n  )\n\nggplot() +\n  # Fill by % of tract population covered (easier to interpret than the index)\n  geom_sf(\n    data  = tracts_equity_tr,\n    aes(fill = pct_pop_covered),\n    color = NA\n  ) +\n  # Red outline for tracts meeting BOTH: high-minority AND low-coverage\n  geom_sf(\n    data      = dplyr::filter(tracts_equity_tr, flag_both_tr),\n    fill      = NA,\n    color     = \"cyan\",\n    linewidth = 0.35\n  ) +\n  geom_sf(\n    data  = pa_proj,\n    fill  = NA, color = \"white\", linewidth = 0.25\n  ) +\n  geom_sf(\n    data  = stations_proj,\n    aes(shape = \"Fire/EMS stations\"),\n    color = \"darkgreen\", size = 0.9, alpha = 0.85, inherit.aes = FALSE\n  ) +\n  scale_fill_viridis_c(\n    name   = \"% of tract population within 2 miles\",\n    labels = label_number(accuracy = 1, suffix = \"%\"),\n    option = \"plasma\",\n    na.value = \"grey90\"\n  ) +\n  scale_shape_manual(name = NULL, values = c(\"Fire/EMS stations\" = 16)) +\n  guides(\n    fill  = guide_colorbar(order = 1, barheight = 8),\n    shape = guide_legend(order = 2, override.aes = list(size = 2.2, color = \"darkgreen\"))\n  ) +\n  labs(\n    title    = \"Coverage Equity (Tract-Level): Fire/EMS Access in Pennsylvania\",\n    subtitle = paste0(\n      \"Cyan outline = top 25% minority share (≥ \", round(thr_minority_tr, 1),\n      \"%) AND bottom 25% coverage (≤ \", round(thr_cover_tr, 1), \"%)\"\n    ),\n    caption  = \"Minority share: ACS B03002 (2018–2022). Coverage: 2-mile buffers, area-weighted to tracts.\"\n  ) +\n  coord_sf(datum = NA) +\n  theme_void() +\n  theme(\n    legend.position = \"right\",\n    plot.title      = element_text(face = \"bold\", size = 14),\n    plot.subtitle   = element_text(size = 11),\n    plot.caption    = element_text(size = 9)\n  )\n\n\n\n\n\n\n\n\nYour interpretation:\nOverall access is high. About 83.6% of residents live within 2 miles of a station; 16.4% (≈2.14M) are outside. Roughly 9.3% of the uncovered statewide are minority (≈198k). Where the gaps concentrate (density vs. coverage map). The largest clusters of dense-but-under-covered tracts appear along the Poconos/Route-80 corridor (Monroe, Pike, parts of Wayne) and in the south-central/exurban belt around Chester, Franklin, Adams, Cumberland, and York. Much of the rural interior is low density with low coverage—but that’s not a “deficit” by the index. Equity overlay (tract-level). Cyan-outlined tracts (high-minority + low coverage) are scattered but not widespread. They appear mainly in the Monroe–Pike area and selected exurban tracts in Chester/Lancaster/York. This matches the Top-10 equity table, where Monroe and Chester lead in the absolute number of minority residents outside the 2-mile catchment. Counties to watch. From the equity table: Monroe, Chester, Pike, Lancaster, York (plus Franklin/Cumberland/Adams/Wayne/Centre) combine sizable uncovered totals with notable minority counts—consistent with the map hot spots."
  },
  {
    "objectID": "Assignments/Assignment_2/AlAbbas_Mohamad_Assignment2.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "href": "Assignments/Assignment_2/AlAbbas_Mohamad_Assignment2.html#finally---a-few-comments-about-your-incorporation-of-feedback",
    "title": "Assignment 2: Spatial Analysis and Visualization",
    "section": "Finally - A few comments about your incorporation of feedback!",
    "text": "Finally - A few comments about your incorporation of feedback!\nI recieved two comments in the last assignment:\n\nHide the setup code block. The Census API key is not supposed to be shared publicly.\n\nAction: Setup code blocks were hidden using the quarto functionality.\n\nIf you want to print something, provide a brief explanation. If just for personal reference, print in console (don’t need them in markdown)\n\nAction: Using the quarto functionality, I prevented auxillary tabulations required for answering certain questions, but not requested by instructions from appear in the markdown, but they remain in the code blocks."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "This portfolio documents my learning journey in Public Policy Analytics (MUSA 5080).\n\n\nAdvanced spatial analysis and data science for urban planning and public policy.\n\n\n\n\nWeekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge\n\n\n\n\nMohamad Al Abbas was born in Beirut, Lebanon. He holds a Bachelor’s degree in Electrical and Computer Engineering and a Master of Arts in International Affairs. He is currently pursuing a joint PhD in Demography and Sociology, though he identifies more closely as an environmental demographer and sociologist. His research focuses on climate change and population health, particularly among children under five and pregnant women. Mohamad’s most recent work argues that using administrative borders as controls in climate–health research is problematic in low- and middle-income countries. Instead, he suggests that researchers adopt definitions based on environmental rather than political boundaries.\nView my full CV | View my Publication list\n\n\n\n\nEmail: ma96@upenn.edu\nGitHub: @MohamadAlAbbas-PhD\n\n\n\n\nMohamad is enrolled in this course to gain a stronger understanding of public policy in relation to urban spatial design, as well as to develop a more streamlined and polished pipeline for coding and research article preparation. He hopes to use this repository as a template for sharing code in publications and scholarly work."
  },
  {
    "objectID": "index.html#about-this-course",
    "href": "index.html#about-this-course",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Advanced spatial analysis and data science for urban planning and public policy."
  },
  {
    "objectID": "index.html#portfolio-structure",
    "href": "index.html#portfolio-structure",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Weekly Notes: My learning reflections and key concepts\nLabs: Completed assignments and analyses\nFinal Project: Capstone modeling challenge"
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Mohamad Al Abbas was born in Beirut, Lebanon. He holds a Bachelor’s degree in Electrical and Computer Engineering and a Master of Arts in International Affairs. He is currently pursuing a joint PhD in Demography and Sociology, though he identifies more closely as an environmental demographer and sociologist. His research focuses on climate change and population health, particularly among children under five and pregnant women. Mohamad’s most recent work argues that using administrative borders as controls in climate–health research is problematic in low- and middle-income countries. Instead, he suggests that researchers adopt definitions based on environmental rather than political boundaries.\nView my full CV | View my Publication list"
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Email: ma96@upenn.edu\nGitHub: @MohamadAlAbbas-PhD"
  },
  {
    "objectID": "index.html#why-this-course",
    "href": "index.html#why-this-course",
    "title": "MUSA 5080 Portfolio",
    "section": "",
    "text": "Mohamad is enrolled in this course to gain a stronger understanding of public policy in relation to urban spatial design, as well as to develop a more streamlined and polished pipeline for coding and research article preparation. He hopes to use this repository as a template for sharing code in publications and scholarly work."
  },
  {
    "objectID": "labs/lab_0/lab0_template.html",
    "href": "labs/lab_0/lab0_template.html",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "",
    "text": "Welcome to your first lab! In this (not graded) assignment, you’ll practice the fundamental dplyr operations I overviewed in class using car sales data. This lab will help you get comfortable with:\n\nBasic data exploration\nColumn selection and manipulation\n\nCreating new variables\nFiltering data\nGrouping and summarizing\n\nInstructions: Copy this template into your portfolio repository under a lab_0/ folder, then complete each section with your code and answers. You will write the code under the comment section in each chunk. Be sure to also copy the data folder into your lab_0 folder."
  },
  {
    "objectID": "labs/lab_0/lab0_template.html#data-structure-exploration",
    "href": "labs/lab_0/lab0_template.html#data-structure-exploration",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "1.1 Data Structure Exploration",
    "text": "1.1 Data Structure Exploration\nExplore the structure of your data and answer these questions:\n\n# Use glimpse() to see the data structure\n\n\n# Check the column names\nglimpse(car_data)\n\nRows: 50,000\nColumns: 7\n$ Manufacturer          &lt;chr&gt; \"Ford\", \"Porsche\", \"Ford\", \"Toyota\", \"VW\", \"Ford…\n$ Model                 &lt;chr&gt; \"Fiesta\", \"718 Cayman\", \"Mondeo\", \"RAV4\", \"Polo\"…\n$ `Engine size`         &lt;dbl&gt; 1.0, 4.0, 1.6, 1.8, 1.0, 1.4, 1.8, 1.4, 1.2, 2.0…\n$ `Fuel type`           &lt;chr&gt; \"Petrol\", \"Petrol\", \"Diesel\", \"Hybrid\", \"Petrol\"…\n$ `Year of manufacture` &lt;dbl&gt; 2002, 2016, 2014, 1988, 2006, 2018, 2010, 2015, …\n$ Mileage               &lt;dbl&gt; 127300, 57850, 39190, 210814, 127869, 33603, 866…\n$ Price                 &lt;dbl&gt; 3074, 49704, 24072, 1705, 4101, 29204, 14350, 30…\n\n# Look at the first few rows\nhead(car_data)\n\n# A tibble: 6 × 7\n  Manufacturer Model     `Engine size` `Fuel type` `Year of manufacture` Mileage\n  &lt;chr&gt;        &lt;chr&gt;             &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt;\n1 Ford         Fiesta              1   Petrol                       2002  127300\n2 Porsche      718 Caym…           4   Petrol                       2016   57850\n3 Ford         Mondeo              1.6 Diesel                       2014   39190\n4 Toyota       RAV4                1.8 Hybrid                       1988  210814\n5 VW           Polo                1   Petrol                       2006  127869\n6 Ford         Focus               1.4 Petrol                       2018   33603\n# ℹ 1 more variable: Price &lt;dbl&gt;\n\n\nQuestions to answer: - How many rows and columns does the dataset have? - What types of variables do you see (numeric, character, etc.)? - Are there any column names that might cause problems? Why?\nYour answers: - Rows: 50,000 - Columns: 7\n- Variable types: 4x doubles, 3xchr types - Problematic names: Engine size, Fuel type, Year of manufacture"
  },
  {
    "objectID": "labs/lab_0/lab0_template.html#tibble-vs-data-frame",
    "href": "labs/lab_0/lab0_template.html#tibble-vs-data-frame",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "1.2 Tibble vs Data Frame",
    "text": "1.2 Tibble vs Data Frame\nCompare how tibbles and data frames display:\n\n# Look at the tibble version (what we have)\ncar_data\n\n# A tibble: 50,000 × 7\n   Manufacturer Model    `Engine size` `Fuel type` `Year of manufacture` Mileage\n   &lt;chr&gt;        &lt;chr&gt;            &lt;dbl&gt; &lt;chr&gt;                       &lt;dbl&gt;   &lt;dbl&gt;\n 1 Ford         Fiesta             1   Petrol                       2002  127300\n 2 Porsche      718 Cay…           4   Petrol                       2016   57850\n 3 Ford         Mondeo             1.6 Diesel                       2014   39190\n 4 Toyota       RAV4               1.8 Hybrid                       1988  210814\n 5 VW           Polo               1   Petrol                       2006  127869\n 6 Ford         Focus              1.4 Petrol                       2018   33603\n 7 Ford         Mondeo             1.8 Diesel                       2010   86686\n 8 Toyota       Prius              1.4 Hybrid                       2015   30663\n 9 VW           Polo               1.2 Petrol                       2012   73470\n10 Ford         Focus              2   Diesel                       1992  262514\n# ℹ 49,990 more rows\n# ℹ 1 more variable: Price &lt;dbl&gt;\n\n# Convert to regular data frame and display\n# car_df &lt;- as.data.frame(car_data)\n# car_df\n\nQuestion: What differences do you notice in how they print?\nYour answer: data frame will render every single row within the dataset."
  },
  {
    "objectID": "labs/lab_0/lab0_template.html#selecting-columns",
    "href": "labs/lab_0/lab0_template.html#selecting-columns",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "2.1 Selecting Columns",
    "text": "2.1 Selecting Columns\nPractice selecting different combinations of columns:\n\n# Select just Model and Mileage columns\n\nModel_Mileage &lt;- select(car_data, Model, Mileage)\n\n# Select Manufacturer, Price, and Fuel type\n\nManu_price_Fuel &lt;- select(car_data,`Year of manufacture`, Price, `Fuel type`)\n\n# Challenge: Select all columns EXCEPT Engine Size\n\nno_engine &lt;- select(car_data, -`Engine size`)"
  },
  {
    "objectID": "labs/lab_0/lab0_template.html#renaming-columns",
    "href": "labs/lab_0/lab0_template.html#renaming-columns",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "2.2 Renaming Columns",
    "text": "2.2 Renaming Columns\nLet’s fix a problematic column name:\n\n# Rename 'Year of manufacture' to year\n\ncar_data &lt;- rename(car_data, year = `Year of manufacture`)\n\n\n# Check that it worked\nnames(car_data)\n\n[1] \"Manufacturer\" \"Model\"        \"Engine size\"  \"Fuel type\"    \"year\"        \n[6] \"Mileage\"      \"Price\"       \n\n\nQuestion: Why did we need backticks around Year of manufacture but not around year?\nYour answer: spaces"
  },
  {
    "objectID": "labs/lab_0/lab0_template.html#calculate-car-age",
    "href": "labs/lab_0/lab0_template.html#calculate-car-age",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "3.1 Calculate Car Age",
    "text": "3.1 Calculate Car Age\n\n# Create an 'age' column (2025 minus year of manufacture)\n\ncar_data &lt;- car_data %&gt;%\n  mutate(\n    age = 2025 - year,   # compute age\n  )\n\n# Create a mileage_per_year column\ncar_data &lt;- car_data %&gt;%\n  mutate(\n    mileage_per_year = Mileage / age       # compute mileage per year\n  )\n\n\n# Look at your new columns\nselect(car_data, Model, year, age, Mileage, mileage_per_year)\n\n# A tibble: 50,000 × 5\n   Model       year   age Mileage mileage_per_year\n   &lt;chr&gt;      &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;            &lt;dbl&gt;\n 1 Fiesta      2002    23  127300            5535.\n 2 718 Cayman  2016     9   57850            6428.\n 3 Mondeo      2014    11   39190            3563.\n 4 RAV4        1988    37  210814            5698.\n 5 Polo        2006    19  127869            6730.\n 6 Focus       2018     7   33603            4800.\n 7 Mondeo      2010    15   86686            5779.\n 8 Prius       2015    10   30663            3066.\n 9 Polo        2012    13   73470            5652.\n10 Focus       1992    33  262514            7955.\n# ℹ 49,990 more rows"
  },
  {
    "objectID": "labs/lab_0/lab0_template.html#categorize-cars",
    "href": "labs/lab_0/lab0_template.html#categorize-cars",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "3.2 Categorize Cars",
    "text": "3.2 Categorize Cars\n\n# Create a price_category column where if price is &lt; 15000, its is coded as budget, between 15000 and 30000 is midrange and greater than 30000 is mid-range (use case_when)\n\ncar_data &lt;- car_data %&gt;%\n  mutate(\n    price_category = case_when(\n      Price &lt; 15000 ~ \"budget\",\n      Price &gt;= 15000 & Price &lt;= 30000 ~ \"midrange\",\n      Price &gt; 30000 ~ \"luxury\"\n    )\n  )\n\n# Check your categories select the new column and show it\n\nselect(car_data, Model, Price, price_category)\n\n# A tibble: 50,000 × 3\n   Model      Price price_category\n   &lt;chr&gt;      &lt;dbl&gt; &lt;chr&gt;         \n 1 Fiesta      3074 budget        \n 2 718 Cayman 49704 luxury        \n 3 Mondeo     24072 midrange      \n 4 RAV4        1705 budget        \n 5 Polo        4101 budget        \n 6 Focus      29204 midrange      \n 7 Mondeo     14350 budget        \n 8 Prius      30297 luxury        \n 9 Polo        9977 budget        \n10 Focus       1049 budget        \n# ℹ 49,990 more rows"
  },
  {
    "objectID": "labs/lab_0/lab0_template.html#basic-filtering",
    "href": "labs/lab_0/lab0_template.html#basic-filtering",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "4.1 Basic Filtering",
    "text": "4.1 Basic Filtering\n\n# Find all Toyota cars\n\ntoyota_cars &lt;- car_data %&gt;%\n  filter(Manufacturer == \"Toyota\")\n\n# Find cars with mileage less than 30,000\n\nlow_mileage &lt;- car_data %&gt;%\n  filter(Mileage &lt; 30000)\n\n# Find luxury cars (from price category) with low mileage\n\nluxury_cars &lt;- car_data %&gt;%\n  filter(price_category == \"luxury\")\n\ntoyota_cars\n\n# A tibble: 12,554 × 10\n   Manufacturer Model `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;         &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Toyota       RAV4            1.8 Hybrid       1988  210814  1705    37\n 2 Toyota       Prius           1.4 Hybrid       2015   30663 30297    10\n 3 Toyota       RAV4            2.2 Petrol       2007   79393 16026    18\n 4 Toyota       Yaris           1.4 Petrol       1998   97286  4046    27\n 5 Toyota       RAV4            2.4 Hybrid       2003  117425 11667    22\n 6 Toyota       Yaris           1.2 Petrol       1992  245990   720    33\n 7 Toyota       RAV4            2   Hybrid       2018   28381 52671     7\n 8 Toyota       Prius           1   Hybrid       2003  115291  6512    22\n 9 Toyota       Prius           1   Hybrid       1990  238571   961    35\n10 Toyota       Prius           1.8 Hybrid       2017   31958 38961     8\n# ℹ 12,544 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\nlow_mileage\n\n# A tibble: 5,402 × 10\n   Manufacturer Model      `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Toyota       RAV4                 2   Hybrid       2018   28381 52671     7\n 2 VW           Golf                 2   Petrol       2020   18985 36387     5\n 3 BMW          M5                   4   Petrol       2017   22759 97758     8\n 4 Toyota       RAV4                 2.4 Petrol       2018   24588 49125     7\n 5 VW           Golf                 2   Hybrid       2018   25017 36957     7\n 6 Porsche      718 Cayman           2.4 Petrol       2021   14070 69526     4\n 7 Ford         Focus                1.8 Petrol       2020   22371 40336     5\n 8 Ford         Mondeo               1.6 Diesel       2015   21834 28435    10\n 9 VW           Passat               1.6 Diesel       2018   22122 36634     7\n10 VW           Passat               1.4 Diesel       2020   21413 39310     5\n# ℹ 5,392 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\nluxury_cars\n\n# A tibble: 6,178 × 10\n   Manufacturer Model      `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Porsche      718 Cayman           4   Petrol       2016   57850 49704     9\n 2 Toyota       Prius                1.4 Hybrid       2015   30663 30297    10\n 3 Toyota       RAV4                 2   Hybrid       2018   28381 52671     7\n 4 Porsche      911                  2.6 Petrol       2009   66273 41963    16\n 5 Toyota       Prius                1.8 Hybrid       2017   31958 38961     8\n 6 VW           Golf                 2   Petrol       2020   18985 36387     5\n 7 BMW          M5                   4   Petrol       2017   22759 97758     8\n 8 Toyota       RAV4                 2.4 Petrol       2018   24588 49125     7\n 9 Porsche      Cayenne              2.6 Diesel       2015   33693 54037    10\n10 VW           Golf                 2   Hybrid       2018   25017 36957     7\n# ℹ 6,168 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;"
  },
  {
    "objectID": "labs/lab_0/lab0_template.html#multiple-conditions",
    "href": "labs/lab_0/lab0_template.html#multiple-conditions",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "4.2 Multiple Conditions",
    "text": "4.2 Multiple Conditions\n\n# Find cars that are EITHER Honda OR Nissan\n\nhonda_nissan &lt;- car_data %&gt;%\n  filter(Manufacturer == \"Honda\" | Manufacturer == \"Nissan\")\n\n# Find cars with price between $20,000 and $35,000\n\nprice_range &lt;- car_data %&gt;%\n  filter(Price &gt;= 20000, Price &lt;= 35000)\n\n# Find diesel cars less than 10 years old\n\ndiesel_recent &lt;- car_data %&gt;%\n  filter(`Fuel type` == \"Diesel\", age &lt; 10)\n\nhonda_nissan\n\n# A tibble: 0 × 10\n# ℹ 10 variables: Manufacturer &lt;chr&gt;, Model &lt;chr&gt;, Engine size &lt;dbl&gt;,\n#   Fuel type &lt;chr&gt;, year &lt;dbl&gt;, Mileage &lt;dbl&gt;, Price &lt;dbl&gt;, age &lt;dbl&gt;,\n#   mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\nprice_range\n\n# A tibble: 7,301 × 10\n   Manufacturer Model  `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Ford         Mondeo           1.6 Diesel       2014   39190 24072    11\n 2 Ford         Focus            1.4 Petrol       2018   33603 29204     7\n 3 Toyota       Prius            1.4 Hybrid       2015   30663 30297    10\n 4 Toyota       Prius            1.4 Hybrid       2016   43893 29946     9\n 5 Toyota       Prius            1.4 Hybrid       2016   43130 30085     9\n 6 VW           Passat           1.6 Petrol       2016   64344 23641     9\n 7 Ford         Mondeo           1.6 Diesel       2015   21834 28435    10\n 8 BMW          M5               4.4 Petrol       2008  109941 31711    17\n 9 BMW          Z4               2.2 Petrol       2014   61332 26084    11\n10 Porsche      911              3.5 Petrol       2003  107705 24378    22\n# ℹ 7,291 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\ndiesel_recent\n\n# A tibble: 2,040 × 10\n   Manufacturer Model   `Engine size` `Fuel type`  year Mileage Price   age\n   &lt;chr&gt;        &lt;chr&gt;           &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1 Ford         Fiesta            1   Diesel       2017   38370 16257     8\n 2 VW           Passat            1.6 Diesel       2018   22122 36634     7\n 3 VW           Passat            1.4 Diesel       2020   21413 39310     5\n 4 BMW          X3                2   Diesel       2018   27389 44018     7\n 5 Ford         Mondeo            2   Diesel       2016   51724 28482     9\n 6 Porsche      Cayenne           2.6 Diesel       2019   20147 76182     6\n 7 VW           Polo              1.2 Diesel       2018   37411 19649     7\n 8 Ford         Mondeo            1.8 Diesel       2016   29439 30886     9\n 9 Ford         Mondeo            1.4 Diesel       2020   18929 37720     5\n10 Ford         Mondeo            1.4 Diesel       2018   42017 28904     7\n# ℹ 2,030 more rows\n# ℹ 2 more variables: mileage_per_year &lt;dbl&gt;, price_category &lt;chr&gt;\n\n\nQuestion: How many diesel cars are less than 10 years old?\nYour answer: 2,040"
  },
  {
    "objectID": "labs/lab_0/lab0_template.html#basic-summaries",
    "href": "labs/lab_0/lab0_template.html#basic-summaries",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "5.1 Basic Summaries",
    "text": "5.1 Basic Summaries\n\n# Calculate average price by manufacturer\navg_price_by_brand &lt;- car_data %&gt;%\n  group_by(Manufacturer) %&gt;%\n  summarize(avg_price = mean(Price, na.rm = TRUE))\n\navg_price_by_brand\n\n# A tibble: 5 × 2\n  Manufacturer avg_price\n  &lt;chr&gt;            &lt;dbl&gt;\n1 BMW             24429.\n2 Ford            10672.\n3 Porsche         29104.\n4 Toyota          14340.\n5 VW              10363.\n\n# Calculate average mileage by fuel type\n\navg_price_by_brand &lt;- car_data %&gt;%\n  group_by(`Fuel type`) %&gt;%\n  summarize(avg_mileage = mean(Mileage, na.rm = TRUE))\n\navg_price_by_brand\n\n# A tibble: 3 × 2\n  `Fuel type` avg_mileage\n  &lt;chr&gt;             &lt;dbl&gt;\n1 Diesel          112667.\n2 Hybrid          111622.\n3 Petrol          112795.\n\n# Count cars by manufacturer\n\ncar_counts &lt;- car_data %&gt;%\n  count(Manufacturer)\n\ncar_counts\n\n# A tibble: 5 × 2\n  Manufacturer     n\n  &lt;chr&gt;        &lt;int&gt;\n1 BMW           4965\n2 Ford         14959\n3 Porsche       2609\n4 Toyota       12554\n5 VW           14913"
  },
  {
    "objectID": "labs/lab_0/lab0_template.html#categorical-summaries",
    "href": "labs/lab_0/lab0_template.html#categorical-summaries",
    "title": "Lab 0: Getting Started with dplyr",
    "section": "5.2 Categorical Summaries",
    "text": "5.2 Categorical Summaries\n\n# Frequency table for price categories\n\nfreq_cat &lt;- car_data %&gt;%\n  count(price_category, name = \"frequency\") %&gt;%\n  mutate(proportion = frequency / sum(frequency))\n\nfreq_cat\n\n# A tibble: 3 × 3\n  price_category frequency proportion\n  &lt;chr&gt;              &lt;int&gt;      &lt;dbl&gt;\n1 budget             34040      0.681\n2 luxury              6178      0.124\n3 midrange            9782      0.196"
  },
  {
    "objectID": "labs/week-03/scrips/week3_lab_exercise.html",
    "href": "labs/week-03/scrips/week3_lab_exercise.html",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "",
    "text": "# Load required packages\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(scales)\nlibrary(RColorBrewer)\n# Set your Census API key if you haven't already\ncensus_api_key(Sys.getenv(\"807ea1c0820a3e1e46dde3c53438622057fcc1ba\"))\n\n# We'll use Pennsylvania data for consistency with previous weeks\nstate_choice &lt;- \"PA\""
  },
  {
    "objectID": "labs/week-03/scrips/week3_lab_exercise.html#setup-and-data-loading",
    "href": "labs/week-03/scrips/week3_lab_exercise.html#setup-and-data-loading",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "",
    "text": "# Load required packages\nlibrary(tidyverse)\nlibrary(tidycensus)\nlibrary(scales)\nlibrary(RColorBrewer)\n# Set your Census API key if you haven't already\ncensus_api_key(Sys.getenv(\"807ea1c0820a3e1e46dde3c53438622057fcc1ba\"))\n\n# We'll use Pennsylvania data for consistency with previous weeks\nstate_choice &lt;- \"PA\""
  },
  {
    "objectID": "labs/week-03/scrips/week3_lab_exercise.html#exercise-0-finding-census-variable-codes",
    "href": "labs/week-03/scrips/week3_lab_exercise.html#exercise-0-finding-census-variable-codes",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 0: Finding Census Variable Codes",
    "text": "Exercise 0: Finding Census Variable Codes\nThe Challenge: You know you want data on total population, median income, and median age, but you don’t know the specific Census variable codes. How do you find them?\n\n0.1 Load the Variable Dictionary\n\n# Load all available variables for ACS 5-year 2022\nacs_vars_2022 &lt;- load_variables(2022, \"acs5\", cache = TRUE)\n\n# Look at the structure\nglimpse(acs_vars_2022)\n\nRows: 28,152\nColumns: 4\n$ name      &lt;chr&gt; \"B01001A_001\", \"B01001A_002\", \"B01001A_003\", \"B01001A_004\", …\n$ label     &lt;chr&gt; \"Estimate!!Total:\", \"Estimate!!Total:!!Male:\", \"Estimate!!To…\n$ concept   &lt;chr&gt; \"Sex by Age (White Alone)\", \"Sex by Age (White Alone)\", \"Sex…\n$ geography &lt;chr&gt; \"tract\", \"tract\", \"tract\", \"tract\", \"tract\", \"tract\", \"tract…\n\nhead(acs_vars_2022)\n\n# A tibble: 6 × 4\n  name        label                                   concept          geography\n  &lt;chr&gt;       &lt;chr&gt;                                   &lt;chr&gt;            &lt;chr&gt;    \n1 B01001A_001 Estimate!!Total:                        Sex by Age (Whi… tract    \n2 B01001A_002 Estimate!!Total:!!Male:                 Sex by Age (Whi… tract    \n3 B01001A_003 Estimate!!Total:!!Male:!!Under 5 years  Sex by Age (Whi… tract    \n4 B01001A_004 Estimate!!Total:!!Male:!!5 to 9 years   Sex by Age (Whi… tract    \n5 B01001A_005 Estimate!!Total:!!Male:!!10 to 14 years Sex by Age (Whi… tract    \n6 B01001A_006 Estimate!!Total:!!Male:!!15 to 17 years Sex by Age (Whi… tract    \n\n\nWhat you see:\n\nname: The variable code (e.g., “B01003_001”)\nlabel: Human-readable description\nconcept: The broader table this variable belongs to\n\n\n\n0.2 Search for Population Variables\nYour Task: Find the variable code for total population.\n\n# Search for population-related variables\npopulation_vars &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(label, \"Total.*population\"))\n\n# Look at the results\nhead(population_vars, 10)\n\n# A tibble: 10 × 4\n   name       label                                            concept geography\n   &lt;chr&gt;      &lt;chr&gt;                                            &lt;chr&gt;   &lt;chr&gt;    \n 1 B16008_002 \"Estimate!!Total:!!Native population:\"           Citize… tract    \n 2 B16008_003 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 3 B16008_004 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 4 B16008_005 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 5 B16008_006 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 6 B16008_007 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 7 B16008_008 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 8 B16008_009 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n 9 B16008_010 \"Estimate!!Total:!!Native population:!!5 to 17 … Citize… tract    \n10 B16008_011 \"Estimate!!Total:!!Native population:!!18 years… Citize… tract    \n\n# Or search in the concept field\npop_concept &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(concept, \"Total Population\"))\n\nhead(pop_concept)\n\n# A tibble: 6 × 4\n  name        label                             concept                geography\n  &lt;chr&gt;       &lt;chr&gt;                             &lt;chr&gt;                  &lt;chr&gt;    \n1 B01003_001  Estimate!!Total                   Total Population       block gr…\n2 B25008A_001 Estimate!!Total:                  Total Population in O… block gr…\n3 B25008A_002 Estimate!!Total:!!Owner occupied  Total Population in O… block gr…\n4 B25008A_003 Estimate!!Total:!!Renter occupied Total Population in O… block gr…\n5 B25008B_001 Estimate!!Total:                  Total Population in O… block gr…\n6 B25008B_002 Estimate!!Total:!!Owner occupied  Total Population in O… block gr…\n\n\nTip: Look for “Total” followed by “population” - usually B01003_001\n\n\n0.3 Search for Income Variables\nYour Task: Find median household income variables.\n\n# Search for median income\nincome_vars &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(label, \"[Mm]edian.*income\"))\n\n# Look specifically for household income\nhousehold_income &lt;- income_vars %&gt;%\n  filter(str_detect(label, \"household\"))\n\nprint(\"Household income variables:\")\n\n[1] \"Household income variables:\"\n\nhead(household_income)\n\n# A tibble: 6 × 4\n  name        label                                            concept geography\n  &lt;chr&gt;       &lt;chr&gt;                                            &lt;chr&gt;   &lt;chr&gt;    \n1 B10010_002  Estimate!!Median family income in the past 12 m… Median… tract    \n2 B10010_003  Estimate!!Median family income in the past 12 m… Median… tract    \n3 B19013A_001 Estimate!!Median household income in the past 1… Median… tract    \n4 B19013B_001 Estimate!!Median household income in the past 1… Median… tract    \n5 B19013C_001 Estimate!!Median household income in the past 1… Median… tract    \n6 B19013D_001 Estimate!!Median household income in the past 1… Median… tract    \n\n# Alternative: search by concept\nincome_concept &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(concept, \"Median Household Income\"))\n\nhead(income_concept)\n\n# A tibble: 6 × 4\n  name        label                                            concept geography\n  &lt;chr&gt;       &lt;chr&gt;                                            &lt;chr&gt;   &lt;chr&gt;    \n1 B19013A_001 Estimate!!Median household income in the past 1… Median… tract    \n2 B19013B_001 Estimate!!Median household income in the past 1… Median… tract    \n3 B19013C_001 Estimate!!Median household income in the past 1… Median… tract    \n4 B19013D_001 Estimate!!Median household income in the past 1… Median… tract    \n5 B19013E_001 Estimate!!Median household income in the past 1… Median… county   \n6 B19013F_001 Estimate!!Median household income in the past 1… Median… tract    \n\n\nPattern Recognition: Median household income is typically B19013_001\n\n\n0.4 Search for Age Variables\nYour Task: Find median age variables.\n[write the code below - first add a code chunk]\n\n\n0.5 Advanced Search Techniques\nYour Task: Learn more sophisticated search methods.\n\n# Search for multiple terms at once\nhousing_vars &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(label, \"[Mm]edian.*(rent|value)\"))\n\nprint(\"Housing cost variables:\")\n\n[1] \"Housing cost variables:\"\n\nhead(housing_vars, 10)\n\n# A tibble: 10 × 4\n   name         label                                          concept geography\n   &lt;chr&gt;        &lt;chr&gt;                                          &lt;chr&gt;   &lt;chr&gt;    \n 1 B07002PR_004 Estimate!!Median age --!!Total:!!Moved from d… Median… &lt;NA&gt;     \n 2 B07002_004   Estimate!!Median age --!!Total:!!Moved from d… Median… tract    \n 3 B07002_005   Estimate!!Median age --!!Total:!!Moved from d… Median… tract    \n 4 B07011PR_004 Estimate!!Median income in the past 12 months… Median… &lt;NA&gt;     \n 5 B07011_004   Estimate!!Median income in the past 12 months… Median… tract    \n 6 B07011_005   Estimate!!Median income in the past 12 months… Median… tract    \n 7 B07402PR_004 Estimate!!Median age --!!Total living in area… Median… &lt;NA&gt;     \n 8 B07402_004   Estimate!!Median age --!!Total living in area… Median… county   \n 9 B07402_005   Estimate!!Median age --!!Total living in area… Median… county   \n10 B07411PR_004 Estimate!!Median income in the past 12 months… Median… &lt;NA&gt;     \n\n# Search excluding certain terms\nincome_not_family &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(label, \"[Mm]edian.*income\") & \n         !str_detect(label, \"family\"))\n\nprint(\"Income variables (not family income):\")\n\n[1] \"Income variables (not family income):\"\n\nhead(income_not_family)\n\n# A tibble: 6 × 4\n  name         label                                           concept geography\n  &lt;chr&gt;        &lt;chr&gt;                                           &lt;chr&gt;   &lt;chr&gt;    \n1 B06011PR_001 Estimate!!Median income in the past 12 months … Median… &lt;NA&gt;     \n2 B06011PR_002 Estimate!!Median income in the past 12 months … Median… &lt;NA&gt;     \n3 B06011PR_003 Estimate!!Median income in the past 12 months … Median… &lt;NA&gt;     \n4 B06011PR_004 Estimate!!Median income in the past 12 months … Median… &lt;NA&gt;     \n5 B06011PR_005 Estimate!!Median income in the past 12 months … Median… &lt;NA&gt;     \n6 B06011_001   Estimate!!Median income in the past 12 months … Median… tract    \n\n# Case-insensitive search using regex\neducation_vars &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(label, regex(\"bachelor\", ignore_case = TRUE)))\n\nprint(\"Education variables:\")\n\n[1] \"Education variables:\"\n\nhead(education_vars, 5)\n\n# A tibble: 5 × 4\n  name         label                                           concept geography\n  &lt;chr&gt;        &lt;chr&gt;                                           &lt;chr&gt;   &lt;chr&gt;    \n1 B06009PR_005 Estimate!!Total:!!Bachelor's degree             Place … &lt;NA&gt;     \n2 B06009PR_011 Estimate!!Total:!!Born in Puerto Rico:!!Bachel… Place … &lt;NA&gt;     \n3 B06009PR_017 Estimate!!Total:!!Born in the United States:!!… Place … &lt;NA&gt;     \n4 B06009PR_023 Estimate!!Total:!!Native; born elsewhere:!!Bac… Place … &lt;NA&gt;     \n5 B06009PR_029 Estimate!!Total:!!Foreign born:!!Bachelor's de… Place … &lt;NA&gt;     \n\n\n\n\n0.6 Interactive Exploration\nYour Task: Use RStudio’s viewer for easier searching.\n\n# Open the full variable list in RStudio viewer\n# This opens a searchable data table\nView(acs_vars_2022)\n\n# Pro tip: You can also search specific table groups\n# B01 = Age and Sex\n# B19 = Income  \n# B25 = Housing\ntable_b19 &lt;- acs_vars_2022 %&gt;%\n  filter(str_detect(name, \"^B19\"))  # ^ means \"starts with\"\n\nprint(\"All B19 (Income) table variables:\")\n\n[1] \"All B19 (Income) table variables:\"\n\nhead(table_b19, 10)\n\n# A tibble: 10 × 4\n   name        label                                concept            geography\n   &lt;chr&gt;       &lt;chr&gt;                                &lt;chr&gt;              &lt;chr&gt;    \n 1 B19001A_001 Estimate!!Total:                     Household Income … tract    \n 2 B19001A_002 Estimate!!Total:!!Less than $10,000  Household Income … tract    \n 3 B19001A_003 Estimate!!Total:!!$10,000 to $14,999 Household Income … tract    \n 4 B19001A_004 Estimate!!Total:!!$15,000 to $19,999 Household Income … tract    \n 5 B19001A_005 Estimate!!Total:!!$20,000 to $24,999 Household Income … tract    \n 6 B19001A_006 Estimate!!Total:!!$25,000 to $29,999 Household Income … tract    \n 7 B19001A_007 Estimate!!Total:!!$30,000 to $34,999 Household Income … tract    \n 8 B19001A_008 Estimate!!Total:!!$35,000 to $39,999 Household Income … tract    \n 9 B19001A_009 Estimate!!Total:!!$40,000 to $44,999 Household Income … tract    \n10 B19001A_010 Estimate!!Total:!!$45,000 to $49,999 Household Income … tract    \n\n\n\n\n0.7 Verify Your Variable Choices\nYour Task: Test your variables by getting a small sample of data.\n\n# Test the variables you found\ntest_vars &lt;- c(\n  total_pop = \"B01003_001\",      # Total population\n  median_income = \"B19013_001\",  # Median household income\n  median_age = \"B01002_001\"      # Median age\n)\n\n# Get data for just one state to test\ntest_data &lt;- get_acs(\n  geography = \"state\",\n  variables = test_vars,\n  state = \"PA\",\n  year = 2022\n)\n\n# Check that you got what you expected\ntest_data\n\n# A tibble: 3 × 5\n  GEOID NAME         variable        estimate   moe\n  &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;              &lt;dbl&gt; &lt;dbl&gt;\n1 42    Pennsylvania median_age          40.8   0.1\n2 42    Pennsylvania total_pop     12989208    NA  \n3 42    Pennsylvania median_income    73170   347  \n\n\n\n\n0.8 Common Variable Patterns\nReference guide for future use:\n\n# Common patterns to remember:\ncommon_variables &lt;- tribble(\n  ~concept, ~typical_code, ~description,\n  \"Total Population\", \"B01003_001\", \"Total population\",\n  \"Median Age\", \"B01002_001\", \"Median age of population\", \n  \"Median HH Income\", \"B19013_001\", \"Median household income\",\n  \"White Population\", \"B03002_003\", \"White alone population\",\n  \"Black Population\", \"B03002_004\", \"Black/African American alone\",\n  \"Hispanic Population\", \"B03002_012\", \"Hispanic or Latino population\",\n  \"Bachelor's Degree\", \"B15003_022\", \"Bachelor's degree or higher\",\n  \"Median Rent\", \"B25058_001\", \"Median contract rent\",\n  \"Median Home Value\", \"B25077_001\", \"Median value owner-occupied\"\n)\n\nprint(\"Common Census Variables:\")\n\n[1] \"Common Census Variables:\"\n\ncommon_variables\n\n# A tibble: 9 × 3\n  concept             typical_code description                  \n  &lt;chr&gt;               &lt;chr&gt;        &lt;chr&gt;                        \n1 Total Population    B01003_001   Total population             \n2 Median Age          B01002_001   Median age of population     \n3 Median HH Income    B19013_001   Median household income      \n4 White Population    B03002_003   White alone population       \n5 Black Population    B03002_004   Black/African American alone \n6 Hispanic Population B03002_012   Hispanic or Latino population\n7 Bachelor's Degree   B15003_022   Bachelor's degree or higher  \n8 Median Rent         B25058_001   Median contract rent         \n9 Median Home Value   B25077_001   Median value owner-occupied  \n\n\nKey Tips for Variable Hunting:\n\nStart with concepts - search for the topic you want (income, age, housing)\nLook for “Median” vs “Mean” - median is usually more policy-relevant\nCheck the universe - some variables are for “households,” others for “population”\nTest with small data before running large queries\nBookmark useful variables for future projects (type them in your weekly notes!)"
  },
  {
    "objectID": "labs/week-03/scrips/week3_lab_exercise.html#exercise-1-single-variable-eda",
    "href": "labs/week-03/scrips/week3_lab_exercise.html#exercise-1-single-variable-eda",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 1: Single Variable EDA",
    "text": "Exercise 1: Single Variable EDA\n\n1.1 Load and Inspect Data\n\n# Get county-level data for your state\ncounty_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    total_pop = \"B01003_001\",       # Total population\n    median_income = \"B19013_001\",   # Median household income\n    median_age = \"B01002_001\"       # Median age\n  ),\n  state = state_choice,\n  year = 2022,\n  output = \"wide\"\n)\n\n# Clean county names\ncounty_data &lt;- county_data %&gt;%\n  mutate(county_name = str_remove(NAME, paste0(\", \", state_choice)))\n\n# Basic inspection\nglimpse(county_data)\n\nRows: 67\nColumns: 9\n$ GEOID          &lt;chr&gt; \"42001\", \"42003\", \"42005\", \"42007\", \"42009\", \"42011\", \"…\n$ NAME           &lt;chr&gt; \"Adams County, Pennsylvania\", \"Allegheny County, Pennsy…\n$ total_popE     &lt;dbl&gt; 104604, 1245310, 65538, 167629, 47613, 428483, 122640, …\n$ total_popM     &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,…\n$ median_incomeE &lt;dbl&gt; 78975, 72537, 61011, 67194, 58337, 74617, 59386, 60650,…\n$ median_incomeM &lt;dbl&gt; 3334, 869, 2202, 1531, 2606, 1191, 2058, 2167, 1516, 21…\n$ median_ageE    &lt;dbl&gt; 43.8, 40.6, 47.0, 44.9, 47.3, 39.9, 42.9, 43.9, 44.0, 4…\n$ median_ageM    &lt;dbl&gt; 0.2, 0.1, 0.2, 0.1, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, …\n$ county_name    &lt;chr&gt; \"Adams County, Pennsylvania\", \"Allegheny County, Pennsy…\n\n\n\n\n1.2 Explore Income Distribution\nYour Task: Create a histogram of median household income and describe what you see.\n\n# Create histogram of median income\nggplot(county_data) +\n  aes(x = median_incomeE) +\n  geom_histogram(bins = 15, fill = \"steelblue\", alpha = 0.7) +\n  labs(\n    title = \"Distribution of Median Household Income\",\n    x = \"Median Household Income ($)\",\n    y = \"Number of Counties\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = dollar)\n\n\n\n\n\n\n\n\n\n\n1.3 Box Plot for Outlier Detection\nYour Task: Create a boxplot to identify specific outlier counties.\n\n# Box plot to see outliers clearly\nggplot(county_data) +\n  aes(y = median_incomeE) +\n  geom_boxplot(fill = \"lightblue\", width = 0.5) +\n  labs(\n    title = \"Median Income Distribution with Outliers\",\n    y = \"Median Household Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n# Identify the outlier counties\nincome_outliers &lt;- county_data %&gt;%\n  mutate(\n    Q1 = quantile(median_incomeE, 0.25, na.rm = TRUE),\n    Q3 = quantile(median_incomeE, 0.75, na.rm = TRUE),\n    IQR = Q3 - Q1,\n    outlier = median_incomeE &lt; (Q1 - 1.5 * IQR) | median_incomeE &gt; (Q3 + 1.5 * IQR)\n  ) %&gt;%\n  filter(outlier) %&gt;%\n  select(county_name, median_incomeE)\n\nprint(\"Outlier counties:\")\n\n[1] \"Outlier counties:\"\n\nincome_outliers\n\n# A tibble: 3 × 2\n  county_name                     median_incomeE\n  &lt;chr&gt;                                    &lt;dbl&gt;\n1 Bucks County, Pennsylvania              107826\n2 Chester County, Pennsylvania            118574\n3 Montgomery County, Pennsylvania         107441\n\n\n\n\n1.4 Challenge Exercise: Population Distribution\nYour Task: Create your own visualization of population distribution and identify outliers.\nRequirements:\n\nCreate a histogram of total population (total_popE)\nUse a different color than the income example (try “darkgreen” or “purple”)\nAdd appropriate labels and title\nCreate a boxplot to identify population outliers\nFind and list the 3 most populous and 3 least populous counties\n\n\n# Create histogram of median income\nggplot(county_data) +\n  aes(x = total_popE) +\n  geom_histogram(bins = 15, fill = \"darkgreen\", alpha = 0.7) +\n  labs(\n    title = \"Distribution of Total Population\",\n    x = \"Total Population\",\n    y = \"Number of Counties\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma)"
  },
  {
    "objectID": "labs/week-03/scrips/week3_lab_exercise.html#exercise-2-two-variable-relationships",
    "href": "labs/week-03/scrips/week3_lab_exercise.html#exercise-2-two-variable-relationships",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 2: Two Variable Relationships",
    "text": "Exercise 2: Two Variable Relationships\n\n2.1 Population vs Income Scatter Plot\nYour Task: Explore the relationship between population size and median income.\n\n# Basic scatter plot\nggplot(county_data) +\n  aes(x = total_popE, y = median_incomeE) +\n  geom_point() +\n  labs(\n    title = \"Population vs Median Income\",\n    x = \"Total Population\",\n    y = \"Median Household Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n\n\n\n2.2 Add Trend Line and Labels\nYour Task: Improve the plot by adding a trend line and labeling interesting points.\n\n# Enhanced scatter plot with trend line\nggplot(county_data) +\n  aes(x = total_popE, y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = TRUE, color = \"red\") +\n  labs(\n    title = \"Population vs Median Income in Pennsylvania Counties\",\n    subtitle = \"2018-2022 ACS 5-Year Estimates\",\n    x = \"Total Population\",\n    y = \"Median Household Income ($)\",\n    caption = \"Source: U.S. Census Bureau\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n# Calculate correlation\ncorrelation &lt;- cor(county_data$total_popE, county_data$median_incomeE, use = \"complete.obs\")\nprint(paste(\"Correlation coefficient:\", round(correlation, 3)))\n\n[1] \"Correlation coefficient: 0.457\"\n\n\n\n\n2.3 Deal with Skewed Data\nYour Task: The population data is highly skewed. Try a log transformation.\n\n# Log-transformed scatter plot\nggplot(county_data) +\n  aes(x = log(total_popE), y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  labs(\n    title = \"Log(Population) vs Median Income\",\n    x = \"Log(Total Population)\",\n    y = \"Median Household Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n\nQuestion: Does the log transformation reveal a clearer relationship?\n\n\n2.4 Challenge Exercise: Age vs Income Relationship\nYour Task: Explore the relationship between median age and median income using different visualization techniques.\nRequirements:\n\nCreate a scatter plot with median age on x-axis and median income on y-axis\nUse red points (color = \"red\") with 50% transparency (alpha = 0.5)\nAdd a smooth trend line using method = \"loess\" instead of “lm”\nUse the “dark” theme (theme_dark())\nFormat the y-axis with dollar signs\nAdd a title that mentions both variables\n\n\n# Log-transformed scatter plot\nggplot(county_data) +\n  aes(x = median_ageE, y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"loess\", se = TRUE, color =\"red\") +\n  labs(\n    title = \"Median Age vs Median Income\",\n    x = \"Median Age\",\n    y = \"Median Household Income ($)\"\n  ) +\n  theme_dark() +\n  scale_y_continuous(labels = dollar)"
  },
  {
    "objectID": "labs/week-03/scrips/week3_lab_exercise.html#exercise-3-data-quality-visualization",
    "href": "labs/week-03/scrips/week3_lab_exercise.html#exercise-3-data-quality-visualization",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 3: Data Quality Visualization",
    "text": "Exercise 3: Data Quality Visualization\n\n3.1 Visualize Margins of Error\nYour Task: Create a visualization showing how data reliability varies across counties.\n\n# Calculate MOE percentages\ncounty_reliability &lt;- county_data %&gt;%\n  mutate(\n    income_moe_pct = (median_incomeM / median_incomeE) * 100,\n    pop_category = case_when(\n      total_popE &lt; 50000 ~ \"Small (&lt;50K)\",\n      total_popE &lt; 200000 ~ \"Medium (50K-200K)\",\n      TRUE ~ \"Large (200K+)\"\n    )\n  )\n\n# MOE by population size\nggplot(county_reliability) +\n  aes(x = total_popE, y = income_moe_pct) +\n  geom_point(alpha = 0.7) +\n  geom_hline(yintercept = 10, color = \"red\", linetype = \"dashed\") +\n  labs(\n    title = \"Data Reliability Decreases with Population Size\",\n    x = \"Total Population\",\n    y = \"Margin of Error (%)\",\n    caption = \"Red line = 10% reliability threshold\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma)\n\n\n\n\n\n\n\n\n\n\n3.2 Compare Reliability by County Size\nYour Task: Use box plots to compare MOE across county size categories.\n\n# Box plots by population category\nggplot(county_reliability) +\n  aes(x = pop_category, y = income_moe_pct, fill = pop_category) +\n  geom_boxplot() +\n  labs(\n    title = \"Data Reliability by County Size Category\",\n    x = \"Population Category\",\n    y = \"Margin of Error (%)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")  # Remove legend since x-axis is clear\n\n\n\n\n\n\n\n\n\n\n3.3 Challenge Exercise: Age Data Reliability\nYour Task: Analyze the reliability of median age data across counties.\nRequirements:\n\nCalculate MOE percentage for median age (median_ageM / median_ageE * 100)\nCreate a scatter plot showing population vs age MOE percentage\nUse purple points (color = \"purple\") with size = 2\nAdd a horizontal line at 5% MOE using geom_hline() with a blue dashed line\nUse theme_classic()instead of theme_minimal()\nCreate a boxplot comparing age MOE across the three population categories\n\n\n# Box plots by population category\ncounty_reliability &lt;- county_reliability %&gt;%\n  mutate(\n    age_moe_pct = (median_ageM / median_ageE * 100)\n    )\n\n# MOE by population size\nggplot(county_reliability) +\n  aes(x = total_popE, y = age_moe_pct) +\n  geom_point(alpha = 0.7, color=\"purple\", size = 2) +\n  geom_hline(yintercept = 5, color = \"blue\", linetype = \"dashed\") +\n  labs(\n    title = \"Data Reliability Decreases with Population Size\",\n    x = \"Total Population\",\n    y = \"Margin of Error (%)\",\n    caption = \"Blue line = 5% reliability threshold\"\n  ) +\n  theme_classic() +\n  scale_x_continuous(labels = comma)\n\n\n\n\n\n\n\nggplot(county_reliability) +\n  aes(x = pop_category, y = age_moe_pct, fill = pop_category) +\n  geom_boxplot() +\n  labs(\n    title = \"Data Reliability by County Size Category\",\n    x = \"Population Category\",\n    y = \"Age Margin of Error (%)\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"none\")  # Remove legend since x-axis is clear"
  },
  {
    "objectID": "labs/week-03/scrips/week3_lab_exercise.html#exercise-4-multiple-variables-with-color-and-faceting",
    "href": "labs/week-03/scrips/week3_lab_exercise.html#exercise-4-multiple-variables-with-color-and-faceting",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 4: Multiple Variables with Color and Faceting",
    "text": "Exercise 4: Multiple Variables with Color and Faceting\n\n4.1 Three-Variable Scatter Plot\nYour Task: Add median age as a color dimension to the population-income relationship.\n\n# Three-variable scatter plot\nggplot(county_data) +\n  aes(x = total_popE, y = median_incomeE, color = median_ageE) +\n  geom_point(size = 2, alpha = 0.7) +\n  scale_color_viridis_c(name = \"Median\\nAge\") +\n  labs(\n    title = \"Population, Income, and Age Patterns\",\n    x = \"Total Population\",\n    y = \"Median Household Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n\n\n\n4.2 Create Categories for Faceting\nYour Task: Create age categories and use faceting to compare patterns.\n\n# Create age categories and faceted plot\ncounty_faceted &lt;- county_data %&gt;%\n  mutate(\n    age_category = case_when(\n      median_ageE &lt; 40 ~ \"Young (&lt; 40)\",\n      median_ageE &lt; 45 ~ \"Middle-aged (40-45)\",\n      TRUE ~ \"Older (45+)\"\n    )\n  )\n\nggplot(county_faceted) +\n  aes(x = total_popE, y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~age_category) +\n  labs(\n    title = \"Population-Income Relationship by Age Profile\",\n    x = \"Total Population\",\n    y = \"Median Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n\nQuestion: Do the relationships between population and income differ by age profile?\nYour Task: Create a visualization using income categories and multiple aesthetic mappings.\nRequirements:\n\nCreate income categories: “Low” (&lt;$50k), “Middle” ($50k-$80k), “High” (&gt;$80k)\nMake a scatter plot with population (x) vs median age (y) - Color points by income category\nSize points by the margin of error for income (median_incomeM)\nUse the “Set2” color palette: scale_color_brewer(palette = \"Set2\") **note: you’ll need to load the RColorBrewer package for this`\nFacet by income category using facet_wrap()\nUse theme_bw() theme"
  },
  {
    "objectID": "labs/week-03/scrips/week3_lab_exercise.html#exercise-5-data-joins-and-integration",
    "href": "labs/week-03/scrips/week3_lab_exercise.html#exercise-5-data-joins-and-integration",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 5: Data Joins and Integration",
    "text": "Exercise 5: Data Joins and Integration\n\n5.1 Get Additional Census Data\nYour Task: Load educational attainment data and join it with our existing data.\n\n# Get educational attainment data\neducation_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    total_25plus = \"B15003_001\",    # Total population 25 years and over\n    bachelor_plus = \"B15003_022\"    # Bachelor's degree or higher\n  ),\n  state = state_choice,\n  year = 2022,\n  output = \"wide\"\n) %&gt;%\n  mutate(\n    pct_college = (bachelor_plusE / total_25plusE) * 100,\n    county_name = str_remove(NAME, paste0(\", \", state_choice))\n  ) %&gt;%\n  select(GEOID, county_name, pct_college)\n\n# Check the data\nhead(education_data)\n\n# A tibble: 6 × 3\n  GEOID county_name                    pct_college\n  &lt;chr&gt; &lt;chr&gt;                                &lt;dbl&gt;\n1 42001 Adams County, Pennsylvania           13.9 \n2 42003 Allegheny County, Pennsylvania       25.4 \n3 42005 Armstrong County, Pennsylvania       12.7 \n4 42007 Beaver County, Pennsylvania          18.3 \n5 42009 Bedford County, Pennsylvania          9.73\n6 42011 Berks County, Pennsylvania           17.2 \n\n\n\n\n5.2 Join the Datasets\nYour Task: Join the education data with our main county dataset.\n\n# Perform the join\ncombined_data &lt;- county_data %&gt;%\n  left_join(education_data, by = \"GEOID\")\n\n# Check the join worked\ncat(\"Original data rows:\", nrow(county_data), \"\\n\")\n\nOriginal data rows: 67 \n\ncat(\"Combined data rows:\", nrow(combined_data), \"\\n\")\n\nCombined data rows: 67 \n\ncat(\"Missing education data:\", sum(is.na(combined_data$pct_college)), \"\\n\")\n\nMissing education data: 0 \n\n# View the combined data\nhead(combined_data)\n\n# A tibble: 6 × 11\n  GEOID NAME     total_popE total_popM median_incomeE median_incomeM median_ageE\n  &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;\n1 42001 Adams C…     104604         NA          78975           3334        43.8\n2 42003 Alleghe…    1245310         NA          72537            869        40.6\n3 42005 Armstro…      65538         NA          61011           2202        47  \n4 42007 Beaver …     167629         NA          67194           1531        44.9\n5 42009 Bedford…      47613         NA          58337           2606        47.3\n6 42011 Berks C…     428483         NA          74617           1191        39.9\n# ℹ 4 more variables: median_ageM &lt;dbl&gt;, county_name.x &lt;chr&gt;,\n#   county_name.y &lt;chr&gt;, pct_college &lt;dbl&gt;\n\n\n\n\n5.3 Analyze the New Relationship\nYour Task: Explore the relationship between education and income.\n\n# Education vs Income scatter plot\nggplot(combined_data) +\n  aes(x = pct_college, y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = TRUE) +\n  labs(\n    title = \"Education vs Income Across Counties\",\n    x = \"Percent with Bachelor's Degree or Higher\",\n    y = \"Median Household Income ($)\"\n  ) +\n  theme_minimal() +\n  scale_y_continuous(labels = dollar)\n\n\n\n\n\n\n\n# Calculate correlation\nedu_income_cor &lt;- cor(combined_data$pct_college, combined_data$median_incomeE, use = \"complete.obs\")\nprint(paste(\"Education-Income Correlation:\", round(edu_income_cor, 3)))\n\n[1] \"Education-Income Correlation: 0.811\"\n\n\n\n\n5.4 Get Housing Data and Triple Join\nYour Task: Add housing cost data to create a three-way analysis.\n\n# Get housing cost data\nhousing_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    median_rent = \"B25058_001\",     # Median contract rent\n    median_home_value = \"B25077_001\" # Median value of owner-occupied units\n  ),\n  state = state_choice,\n  year = 2022,\n  output = \"wide\"\n) %&gt;%\n  select(GEOID, median_rent = median_rentE, median_home_value = median_home_valueE)\n\n# Join all three datasets\nfull_data &lt;- combined_data %&gt;%\n  left_join(housing_data, by = \"GEOID\")\n\n# Create a housing affordability measure\nfull_data &lt;- full_data %&gt;%\n  mutate(\n    rent_to_income = (median_rent * 12) / median_incomeE * 100,\n    income_category = case_when(\n      median_incomeE &lt; 50000 ~ \"Low Income\",\n      median_incomeE &lt; 80000 ~ \"Middle Income\",\n      TRUE ~ \"High Income\"\n    )\n  )\n\nhead(full_data)\n\n# A tibble: 6 × 15\n  GEOID NAME     total_popE total_popM median_incomeE median_incomeM median_ageE\n  &lt;chr&gt; &lt;chr&gt;         &lt;dbl&gt;      &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;\n1 42001 Adams C…     104604         NA          78975           3334        43.8\n2 42003 Alleghe…    1245310         NA          72537            869        40.6\n3 42005 Armstro…      65538         NA          61011           2202        47  \n4 42007 Beaver …     167629         NA          67194           1531        44.9\n5 42009 Bedford…      47613         NA          58337           2606        47.3\n6 42011 Berks C…     428483         NA          74617           1191        39.9\n# ℹ 8 more variables: median_ageM &lt;dbl&gt;, county_name.x &lt;chr&gt;,\n#   county_name.y &lt;chr&gt;, pct_college &lt;dbl&gt;, median_rent &lt;dbl&gt;,\n#   median_home_value &lt;dbl&gt;, rent_to_income &lt;dbl&gt;, income_category &lt;chr&gt;\n\n\n\n\n5.5 Advanced Multi-Variable Analysis\nYour Task: Create a comprehensive visualization showing multiple relationships.\n\n# Complex multi-variable plot\nggplot(full_data) +\n  aes(x = pct_college, y = rent_to_income, \n      color = income_category, size = total_popE) +\n  geom_point(alpha = 0.7) +\n  labs(\n    title = \"Education, Housing Affordability, and Income Patterns\",\n    subtitle = \"Larger points = larger population\",\n    x = \"Percent with Bachelor's Degree or Higher\",\n    y = \"Annual Rent as % of Median Income\",\n    color = \"Income Category\",\n    size = \"Population\"\n  ) +\n  theme_minimal() +\n  guides(size = guide_legend(override.aes = list(alpha = 1)))"
  },
  {
    "objectID": "labs/week-03/scrips/week3_lab_exercise.html#exercise-6-publication-ready-visualization",
    "href": "labs/week-03/scrips/week3_lab_exercise.html#exercise-6-publication-ready-visualization",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 6: Publication-Ready Visualization",
    "text": "Exercise 6: Publication-Ready Visualization\n\n6.1 Create a Policy-Focused Visualization\nYour Task: Combine multiple visualizations to tell a more complete story about county characteristics.\n\n# Create a multi-panel figure\nlibrary(patchwork)  # For combining plots\n\n# Plot 1: Income distribution\np1 &lt;- ggplot(full_data) +\n  aes(x = median_incomeE) +\n  geom_histogram(bins = 15, fill = \"steelblue\", alpha = 0.7) +\n  labs(title = \"A) Income Distribution\", \n       x = \"Median Income ($)\", y = \"Counties\") +\n  scale_x_continuous(labels = dollar) +\n  theme_minimal()\n\n# Plot 2: Education vs Income\np2 &lt;- ggplot(full_data) +\n  aes(x = pct_college, y = median_incomeE) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(title = \"B) Education vs Income\",\n       x = \"% College Educated\", y = \"Median Income ($)\") +\n  scale_y_continuous(labels = dollar) +\n  theme_minimal()\n\n# Plot 3: Housing affordability by income category\np3 &lt;- ggplot(full_data) +\n  aes(x = income_category, y = rent_to_income, fill = income_category) +\n  geom_boxplot() +\n  labs(title = \"C) Housing Affordability by Income\",\n       x = \"Income Category\", y = \"Rent as % of Income\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n# Plot 4: Data reliability by population\np4 &lt;- ggplot(county_reliability) +\n  aes(x = total_popE, y = income_moe_pct) +\n  geom_point(alpha = 0.7) +\n  geom_hline(yintercept = 10, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"D) Data Reliability\",\n       x = \"Population\", y = \"MOE (%)\") +\n  scale_x_continuous(labels = comma) +\n  theme_minimal()\n\n# Combine all plots\ncombined_plot &lt;- (p1 | p2) / (p3 | p4)\ncombined_plot + plot_annotation(\n  title = \"Pennsylvania County Analysis: Income, Education, and Housing Patterns\",\n  caption = \"Source: American Community Survey 2018-2022\"\n)"
  },
  {
    "objectID": "labs/week-03/scrips/week3_lab_exercise.html#exercise-7-ethical-data-communication---implementing-research-recommendations",
    "href": "labs/week-03/scrips/week3_lab_exercise.html#exercise-7-ethical-data-communication---implementing-research-recommendations",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Exercise 7: Ethical Data Communication - Implementing Research Recommendations",
    "text": "Exercise 7: Ethical Data Communication - Implementing Research Recommendations\nBackground: Research by Jurjevich et al. (2018) found that only 27% of planners warn users about unreliable ACS data, violating AICP ethical standards. In this exercise, you’ll practice the five research-based guidelines for ethical ACS data communication.\n\n7.1 Create Professional Data Tables with Uncertainty\nYour Task: Follow the Jurjevich et al. guidelines to create an ethical data presentation.\n\n# Get comprehensive data for ethical analysis\nethical_demo_data &lt;- get_acs(\n  geography = \"county\",\n  variables = c(\n    median_income = \"B19013_001\",   # Median household income\n    total_25plus = \"B15003_001\",    # Total population 25 years and over\n    bachelor_plus = \"B15003_022\",   # Bachelor's degree or higher\n    total_pop = \"B01003_001\"        # Total population\n  ),\n  state = state_choice,\n  year = 2022,\n  output = \"wide\"\n) %&gt;%\n  mutate(\n    # Calculate derived statistics\n    pct_college = (bachelor_plusE / total_25plusE) * 100,\n    \n    # Calculate MOE for percentage using error propagation\n    pct_college_moe = pct_college * sqrt((bachelor_plusM/bachelor_plusE)^2 + (total_25plusM/total_25plusE)^2),\n    \n    # Calculate coefficient of variation for all key variables\n    income_cv = (median_incomeM / median_incomeE) * 100,\n    education_cv = (pct_college_moe / pct_college) * 100,\n    \n    # Create reliability categories based on CV\n    income_reliability = case_when(\n      income_cv &lt; 12 ~ \"High\",\n      income_cv &lt;= 40 ~ \"Moderate\", \n      TRUE ~ \"Low\"\n    ),\n    \n    education_reliability = case_when(\n      education_cv &lt; 12 ~ \"High\",\n      education_cv &lt;= 40 ~ \"Moderate\",\n      TRUE ~ \"Low\"\n    ),\n    \n    # Create color coding for reliability\n    income_color = case_when(\n      income_reliability == \"High\" ~ \"🟢\",\n      income_reliability == \"Moderate\" ~ \"🟡\",\n      TRUE ~ \"🔴\"\n    ),\n    \n    education_color = case_when(\n      education_reliability == \"High\" ~ \"🟢\",\n      education_reliability == \"Moderate\" ~ \"🟡\", \n      TRUE ~ \"🔴\"\n    ),\n    \n    # Clean county names\n    county_name = str_remove(NAME, paste0(\", \", state_choice))\n  )\n\n# Create ethical data table focusing on least reliable estimates\nethical_data_table &lt;- ethical_demo_data %&gt;%\n  select(county_name, median_incomeE, median_incomeM, income_cv, income_color,\n         pct_college, pct_college_moe, education_cv, education_color) %&gt;%\n  arrange(desc(income_cv)) %&gt;%  # Show least reliable first\n  slice_head(n = 10)\n\n# Create professional table following guidelines\nlibrary(knitr)\nlibrary(kableExtra)\n\nethical_data_table %&gt;%\n  select(county_name, median_incomeE, median_incomeM, income_cv, income_color) %&gt;%\n  kable(\n    col.names = c(\"County\", \"Median Income\", \"Margin of Error\", \n                  \"CV (%)\", \"Reliability\"),\n    caption = \"Pennsylvania Counties: Median Household Income with Statistical Uncertainty\",\n    format.args = list(big.mark = \",\")\n  ) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\")) %&gt;%\n  footnote(\n    general = c(\"Coefficient of Variation (CV) indicates reliability:\",\n                \"🟢 High reliability (CV &lt; 12%)\",\n                \"🟡 Moderate reliability (CV 12-40%)\", \n                \"🔴 Low reliability (CV &gt; 40%)\",\n                \"Following Jurjevich et al. (2018) research recommendations\",\n                \"Source: American Community Survey 2018-2022 5-Year Estimates\"),\n    general_title = \"Notes:\"\n  )\n\n\nPennsylvania Counties: Median Household Income with Statistical Uncertainty\n\n\nCounty\nMedian Income\nMargin of Error\nCV (%)\nReliability\n\n\n\n\nForest County, Pennsylvania\n46,188\n4,612\n9.985278\n🟢 |\n\n\nSullivan County, Pennsylvania\n62,910\n5,821\n9.252901\n🟢 |\n\n\nUnion County, Pennsylvania\n64,914\n4,753\n7.321995\n🟢 |\n\n\nMontour County, Pennsylvania\n72,626\n5,146\n7.085617\n🟢 |\n\n\nElk County, Pennsylvania\n61,672\n4,091\n6.633480\n🟢 |\n\n\nGreene County, Pennsylvania\n66,283\n4,247\n6.407374\n🟢 |\n\n\nCameron County, Pennsylvania\n46,186\n2,605\n5.640237\n🟢 |\n\n\nSnyder County, Pennsylvania\n65,914\n3,666\n5.561793\n🟢 |\n\n\nCarbon County, Pennsylvania\n64,538\n3,424\n5.305402\n🟢 |\n\n\nWarren County, Pennsylvania\n57,925\n3,005\n5.187743\n🟢 |\n\n\n\nNotes:\n\n\n\n\n\n\n Coefficient of Variation (CV) indicates reliability:\n\n\n\n\n\n\n 🟢 High reliability (CV &lt; 12%)\n\n\n\n\n\n\n 🟡 Moderate reliability (CV 12-40%)\n\n\n\n\n\n\n 🔴 Low reliability (CV &gt; 40%)\n\n\n\n\n\n\n Following Jurjevich et al. (2018) research recommendations\n\n\n\n\n\n\n Source: American Community Survey 2018-2022 5-Year Estimates\n\n\n\n\n\n\n\n\n\n\n\n\n7.3 Now try Census Tracts\n\n# Get census tract poverty data for Philadelphia\nphilly_poverty &lt;- get_acs(\n    geography = \"tract\",\n    variables = c(\n      poverty_pop = \"B17001_001\",     \n      poverty_below = \"B17001_002\"    \n    ),\n    state = \"PA\",\n    county = \"101\",\n    year = 2022,\n    output = \"wide\"\n  ) %&gt;%\n  filter(poverty_popE &gt; 0) %&gt;%  # Remove tracts with no poverty data\n  mutate(\n    # Calculate poverty rate and its MOE\n    poverty_rate = (poverty_belowE / poverty_popE) * 100,\n    \n    # MOE for derived percentage using error propagation\n    poverty_rate_moe = poverty_rate * sqrt((poverty_belowM/poverty_belowE)^2 + (poverty_popM/poverty_popE)^2),\n    \n    # Coefficient of variation\n    poverty_cv = (poverty_rate_moe / poverty_rate) * 100,\n    \n    # Reliability assessment\n    reliability = case_when(\n      poverty_cv &lt; 12 ~ \"High\",\n      poverty_cv &lt;= 40 ~ \"Moderate\",\n      poverty_cv &lt;= 75 ~ \"Low\",\n      TRUE ~ \"Very Low\"\n    ),\n    \n    # Color coding\n    reliability_color = case_when(\n      reliability == \"High\" ~ \"🟢\",\n      reliability == \"Moderate\" ~ \"🟡\",\n      reliability == \"Low\" ~ \"🟠\",\n      TRUE ~ \"🔴\"\n    ),\n    \n    # Population size categories\n    pop_category = case_when(\n      poverty_popE &lt; 500 ~ \"Very Small (&lt;500)\",\n      poverty_popE &lt; 1000 ~ \"Small (500-1000)\",\n      poverty_popE &lt; 1500 ~ \"Medium (1000-1500)\",\n      TRUE ~ \"Large (1500+)\"\n    )\n  )\n\n# Check the data quality crisis at tracts\nreliability_summary &lt;- philly_poverty %&gt;%\n  count(reliability) %&gt;%\n  mutate(\n    percentage = round(n / sum(n) * 100, 1),\n    total_bg = sum(n)\n  )\n\nprint(\"Philadelphia Census Tract Poverty Data Reliability:\")\n\n[1] \"Philadelphia Census Tract Poverty Data Reliability:\"\n\nreliability_summary %&gt;%\n  kable(\n    col.names = c(\"Data Quality\", \"Number of Tracts\", \"Percentage\", \"Total\"),\n    caption = \"The Data Quality Crisis: Philadelphia Census Tract Poverty Estimates\"\n  ) %&gt;%\n  kable_styling()\n\n\nThe Data Quality Crisis: Philadelphia Census Tract Poverty Estimates\n\n\nData Quality\nNumber of Tracts\nPercentage\nTotal\n\n\n\n\nLow\n295\n75.8\n389\n\n\nModerate\n53\n13.6\n389\n\n\nVery Low\n41\n10.5\n389\n\n\n\n\n\n\n# Show the most problematic estimates (following Guideline 3: provide context)\nworst_estimates &lt;- philly_poverty %&gt;%\n  filter(reliability %in% c(\"Low\", \"Very Low\")) %&gt;%\n  arrange(desc(poverty_cv)) %&gt;%\n  slice_head(n = 10)\n\nworst_estimates %&gt;%\n  select(GEOID, poverty_rate, poverty_rate_moe, poverty_cv, reliability_color, poverty_popE) %&gt;%\n  kable(\n    col.names = c(\"Tract\", \"Poverty Rate (%)\", \"MOE\", \"CV (%)\", \"Quality\", \"Pop Size\"),\n    caption = \"Guideline 3: Tracts with Least Reliable Poverty Estimates\",\n    digits = c(0, 1, 1, 1, 0, 0)\n  ) %&gt;%\n  kable_styling() %&gt;%\n  footnote(\n    general = c(\"These estimates should NOT be used for policy decisions\",\n                \"CV &gt; 75% indicates very low reliability\",\n                \"Recommend aggregation or alternative data sources\")\n  )\n\n\nGuideline 3: Tracts with Least Reliable Poverty Estimates\n\n\nTract\nPoverty Rate (%)\nMOE\nCV (%)\nQuality\nPop Size\n\n\n\n\n42101989100\n15.8\n45.2\n286.1\n🔴 |\n38|\n\n\n42101000101\n0.7\n1.1\n157.9\n🔴 |\n1947|\n\n\n42101980200\n37.9\n45.2\n119.4\n🔴 |\n66|\n\n\n42101023100\n3.8\n4.5\n119.4\n🔴 |\n1573|\n\n\n42101025600\n1.7\n2.0\n114.2\n🔴 |\n2642|\n\n\n42101014202\n1.7\n1.8\n107.0\n🔴 |\n2273|\n\n\n42101000403\n6.6\n6.7\n101.8\n🔴 |\n1047|\n\n\n42101026100\n4.7\n4.4\n95.0\n🔴 |\n2842|\n\n\n42101036502\n4.9\n4.7\n94.9\n🔴 |\n4284|\n\n\n42101032000\n21.8\n20.6\n94.8\n🔴 |\n7873|\n\n\n\nNote: \n\n\n\n\n\n\n\n These estimates should NOT be used for policy decisions\n\n\n\n\n\n\n\n CV &gt; 75% indicates very low reliability\n\n\n\n\n\n\n\n Recommend aggregation or alternative data sources"
  },
  {
    "objectID": "labs/week-03/scrips/week3_lab_exercise.html#key-references-and-acknowledgments",
    "href": "labs/week-03/scrips/week3_lab_exercise.html#key-references-and-acknowledgments",
    "title": "Week 3 In-Class Lab: Data Visualization and EDA",
    "section": "Key References and Acknowledgments",
    "text": "Key References and Acknowledgments\nJurjevich, J. R., Griffin, A. L., Spielman, S. E., Folch, D. C., Merrick, M., & Nagle, N. N. (2018). Navigating statistical uncertainty: How urban and regional planners understand and work with American community survey (ACS) data for guiding policy. Journal of the American Planning Association, 84(2), 112-126.\nWalker, K. (2023). Analyzing US Census Data: Methods, Maps, and Models in R. Available at: https://walker-data.com/census-r/\nAI Acknowledgments: This lab was developed with coding assistance from Claude AI. I have run, reviewed, and edited the final version. Any remaining errors are my own."
  },
  {
    "objectID": "midterm/appendix/Midterm_2025.html",
    "href": "midterm/appendix/Midterm_2025.html",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "",
    "text": "Content: All the technical details:\nAudience: Data scientists and technical reviewers"
  },
  {
    "objectID": "midterm/appendix/Midterm_2025.html#data-sources",
    "href": "midterm/appendix/Midterm_2025.html#data-sources",
    "title": "Philadelphia Housing Model - Technical Appendix",
    "section": "Data Sources",
    "text": "Data Sources\n\nPrimary Dataset: Philadelphia Property Sales\nSource: Philadelphia Property Sales\nWe sourced the following variables from the property sales dataset:\n\nSale price\nSale date\nProperty characteristics:\n\nTotal Area\nTotal Livable Area\nYear Built\nCensus Tract\nExterior Condition\nInterior Condition\nFireplaces\nGarage Spaces\nMarket Value\nBasements\nNumber of Bedrooms\nNumber of Bathrooms\nNumber of Stories\nShape\n\n\n\n\nSecondary Datasets: Spatial Amenities\nSource: OpenDataPhilly\nWe sourced the following environmental and neighborhood characteristics:\n\nCrime Incidents:\nNeighborood Boundaries\nSEPTA\nBike Network\nHospital\nPhiladelphia Park and Recreation\nSchools\nNeighborhood Food Markets\nPolice Station\nFire Department\n\n\n\n\nPart 1: Data Preparation\nLoad and clean Philadelphia sales data:\n\n########################\n# REMOTE DATA SOURCES\n########################\n\nNEIGH_URL   &lt;- \"https://raw.githubusercontent.com/opendataphilly/open-geo-data/refs/heads/master/philadelphia-neighborhoods/philadelphia-neighborhoods.geojson\"\nSTOPS_URL   &lt;- \"https://hub.arcgis.com/api/v3/datasets/4f827cdbf84d4a53983cf43b8d9fd4df_0/downloads/data?format=geojson&spatialRefId=4326&where=1%3D1\"\nBIKE_URL    &lt;- \"https://hub.arcgis.com/api/v3/datasets/b5f660b9f0f44ced915995b6d49f6385_0/downloads/data?format=geojson&spatialRefId=4326&where=1%3D1\"\nHOSP_URL    &lt;- \"https://opendata.arcgis.com/datasets/df8dc18412494e5abbb021e2f33057b2_0.geojson\"\nPARKS_URL   &lt;- \"https://hub.arcgis.com/api/v3/datasets/d52445160ab14380a673e5849203eb64_0/downloads/data?format=geojson&spatialRefId=4326&where=1%3D1\"\nSCHOOLS_URL &lt;- \"https://hub.arcgis.com/api/v3/datasets/d46a7e59e2c246c891fbee778759717e_0/downloads/data?format=geojson&spatialRefId=4326&where=1%3D1\"\nFOOD_URL    &lt;- \"https://opendata.arcgis.com/datasets/53b8a1c653a74c92b2de23a5d7bf04a0_0.geojson\"\nPOLICE_URL  &lt;- \"https://hub.arcgis.com/api/v3/datasets/7e522339d7c24e8ea5f2c7780291c315_0/downloads/data?format=geojson&spatialRefId=4326&where=1%3D1\"\nFIRE_URL    &lt;- \"https://opendata.arcgis.com/datasets/341526186e014aa0aa3ef7e08a394a78_0.geojson\"\nOPA_URL &lt;- \"https://opendata-downloads.s3.amazonaws.com/opa_properties_public.csv\"\nCRIME_2023_URL &lt;- \"https://phl.carto.com/api/v2/sql?filename=incidents_part1_part2&format=csv&q=SELECT%20*%20,%20ST_Y(the_geom)%20AS%20lat,%20ST_X(the_geom)%20AS%20lng%20FROM%20incidents_part1_part2%20WHERE%20dispatch_date_time%20%3E=%20%272024-01-01%27%20AND%20dispatch_date_time%20%3C%20%272025-01-01%27\"\nCRIME_2024_URL &lt;- \"https://phl.carto.com/api/v2/sql?filename=incidents_part1_part2&format=csv&q=SELECT%20*%20,%20ST_Y(the_geom)%20AS%20lat,%20ST_X(the_geom)%20AS%20lng%20FROM%20incidents_part1_part2%20WHERE%20dispatch_date_time%20%3E=%20%272023-01-01%27%20AND%20dispatch_date_time%20%3C%20%272024-01-01%27\"\n\n############################################################\n# HELPERS\n############################################################\n\nnorm_tgc &lt;- function(x) x |&gt; str_squish() |&gt; str_to_lower()\n\nviolent_set &lt;- c(\n  \"homicide - criminal\",\n  \"homicide - justifiable\",\n  \"rape\",\n  \"robbery firearm\",\n  \"robbery no firearm\",\n  \"aggravated assault firearm\",\n  \"aggravated assault no firearm\",\n  \"other assaults\",\n  \"offenses against family and children\"\n)\n\nmake_violence_bins &lt;- function(df) {\n  df %&gt;%\n    mutate(\n      key = norm_tgc(text_general_code),\n      gun_involved = str_detect(key, \"firearm|weapon\"),\n      violence_bin = case_when(\n        gun_involved         ~ \"Violent\",\n        key %in% violent_set ~ \"Violent\",\n        TRUE                 ~ \"Misdemeanor/Non-violent\"\n      ),\n      violence_bin = factor(\n        violence_bin,\n        levels = c(\"Violent\", \"Misdemeanor/Non-violent\")\n      )\n    )\n}\n\nget_block_dists &lt;- function(epsg, one_block_ft = 300) {\n  u &lt;- tryCatch(st_crs(epsg)$units_gdal, error = function(e) NA_character_)\n  if (is.na(u)) u &lt;- \"unknown\"\n  if (u %in% c(\"metre\",\"m\")) {\n    ft_to_m &lt;- 0.3048\n    list(\n      one = one_block_ft * ft_to_m,\n      two = one_block_ft * 2 * ft_to_m\n    )\n  } else {\n    list(\n      one = one_block_ft,\n      two = one_block_ft * 2\n    )\n  }\n}\n\nto_feet &lt;- function(x, units_gdal) {\n  if (isTRUE(units_gdal %in% c(\"metre\",\"m\"))) {\n    as.numeric(x) / 0.3048\n  } else {\n    as.numeric(x)\n  }\n}\n\nto_miles &lt;- function(x, units_gdal) {\n  x_num &lt;- as.numeric(x)\n  if (isTRUE(units_gdal %in% c(\"metre\",\"m\"))) {\n    x_num / 1609.344\n  } else {\n    x_num / 5280\n  }\n}\n\nnearest_dist_ft &lt;- function(from_pts, to_feats, units_gdal) {\n  if (nrow(to_feats) == 0) return(rep(NA_real_, nrow(from_pts)))\n  idx &lt;- st_nearest_feature(from_pts, to_feats)\n  d   &lt;- st_distance(from_pts, to_feats[idx, ], by_element = TRUE)\n  to_feet(d, units_gdal)\n}\n\nnorm_school_type &lt;- function(x) {\n  t &lt;- str_to_lower(str_squish(as.character(x)))\n  case_when(\n    str_detect(t, \"charter\")                 ~ \"charter\",\n    str_detect(t, \"private|independent\")     ~ \"private\",\n    str_detect(t, \"public|district\")         ~ \"public\",\n    TRUE                                     ~ NA_character_\n  )\n}\n\n############################################################\n# 1. LOAD REMOTE DATA\n############################################################\n\nRaw_data &lt;- read_csv(OPA_URL, show_col_types = FALSE)\n\ncrime2023 &lt;- read_csv(CRIME_2023_URL, show_col_types = FALSE)\ncrime2024 &lt;- read_csv(CRIME_2024_URL, show_col_types = FALSE)\n\n# these will now come from URL instead of disk\nneigh_raw  &lt;- read_sf(NEIGH_URL)\nstops_raw  &lt;- read_sf(STOPS_URL)\nbike_raw   &lt;- read_sf(BIKE_URL)\nhosp_raw   &lt;- read_sf(HOSP_URL)\nparks_raw  &lt;- read_sf(PARKS_URL)\nschools_in &lt;- read_sf(SCHOOLS_URL)\nfood_raw   &lt;- read_sf(FOOD_URL)\npol_raw    &lt;- read_sf(POLICE_URL)\nfire_raw   &lt;- read_sf(FIRE_URL)\n\n\n############################################################\n# 2. CLEAN PROPERTY SALES\n############################################################\n\nkeep_vars &lt;- c(\n  \"sale_date\", \"sale_price\",\n  \"total_area\", \"total_livable_area\",\n  \"unfinished\", \"year_built\", \"shape\",\n  \"category_code\", \"category_code_description\",\n  \"census_tract\", \"central_air\",\n  \"exterior_condition\", \"interior_condition\",\n  \"fireplaces\", \"garage_spaces\",\n  \"general_construction\", \"market_value\",\n  \"number_of_bathrooms\", \"number_of_bedrooms\",\n  \"number_stories\", \"quality_grade\",\n  \"basements\"\n)\n\nclean_data &lt;- Raw_data %&gt;%\n  select(any_of(keep_vars)) %&gt;%\n  mutate(\n    # OPA sometimes sticks a leading integer before the date\n    sale_date_chr = sub(\"^\\\\s*\\\\d+\\\\s+\", \"\", as.character(sale_date)),\n    sale_date = suppressWarnings(parse_date_time(\n      sale_date_chr,\n      orders = c(\"Y-m-d H:M:S\", \"Y-m-d\"),\n      tz = \"America/New_York\"\n    ))\n  ) %&gt;%\n  # time filter (2023-01-01 through 2024-12-31 basically)\n  filter(\n    !is.na(sale_date),\n    sale_date &gt;= ymd_hms(\"2023-01-01 00:00:00\", tz = \"America/New_York\"),\n    sale_date &lt;  ymd_hms(\"2025-01-01 00:00:00\", tz = \"America/New_York\")\n  ) %&gt;%\n  # keep only residential code 1\n  mutate(category_code = suppressWarnings(as.integer(category_code))) %&gt;%\n  filter(category_code == 1) %&gt;%\n  select(-sale_date_chr)\n\n# turn WKT parcel geometry into sf polygons; OPA CRS is EPSG:2272\nclean_data &lt;- st_as_sf(clean_data, wkt = \"shape\", crs = 2272)\n\n# NA summary (useful diagnostics)\ndat_no_geom &lt;- st_drop_geometry(clean_data)\nn_rows &lt;- nrow(dat_no_geom)\nna_summary &lt;- dat_no_geom %&gt;%\n  summarise(across(everything(), ~ sum(is.na(.)))) %&gt;%\n  pivot_longer(everything(), names_to = \"variable\", values_to = \"na_count\") %&gt;%\n  mutate(na_pct = round(100 * na_count / n_rows, 2)) %&gt;%\n  arrange(desc(na_count))\n\n# drop some bad cols and then drop rows w/ remaining NA\nclean_data &lt;- clean_data %&gt;%\n  select(-unfinished, -central_air)\n\nkeep_rows &lt;- stats::complete.cases(st_drop_geometry(clean_data))\nclean_data &lt;- clean_data[keep_rows, ]\n\n\n############################################################\n# 3. SPATIAL ENRICHMENT\n############################################################\n\ncrime2023_bins &lt;- make_violence_bins(crime2023)\ncrime2024_bins &lt;- make_violence_bins(crime2024)\n\ncrime_bins &lt;- bind_rows(crime2023_bins, crime2024_bins) %&gt;%\n  filter(!is.na(lng), !is.na(lat)) %&gt;%\n  st_as_sf(coords = c(\"lng\",\"lat\"), crs = 4326, remove = FALSE) %&gt;%\n  st_transform(CRS_TARGET)\n\ncrime_violent &lt;- filter(crime_bins, violence_bin == \"Violent\")\ncrime_petty   &lt;- filter(crime_bins, violence_bin == \"Misdemeanor/Non-violent\")\n\n# project parcels, ensure valid geom, attach an ID\nclean_data &lt;- clean_data %&gt;%\n  st_transform(CRS_TARGET) %&gt;%\n  st_make_valid() %&gt;%\n  mutate(prop_id = row_number())\n\n# create centroids for distance/buffer ops\nprops_centroids &lt;- st_centroid(clean_data)[, \"prop_id\"]\n\n# make ~1 block (~300ft) and ~2 blocks (~600ft) buffers\nBLK &lt;- get_block_dists(CRS_TARGET, one_block_ft = 300)\nbuf_1blk &lt;- st_buffer(props_centroids, BLK$one)[, \"prop_id\"]\nbuf_2blk &lt;- st_buffer(props_centroids, BLK$two)[, \"prop_id\"]\n\n# track all IDs so every parcel gets a value\nprop_id_tbl &lt;- st_drop_geometry(props_centroids) %&gt;% select(prop_id)\n\nviolent_counts &lt;- st_join(buf_2blk, crime_violent, join = st_intersects, left = TRUE) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(prop_id) %&gt;%\n  summarise(violent_2blocks = sum(!is.na(lat)), .groups = \"drop\") %&gt;%\n  right_join(prop_id_tbl, by = \"prop_id\") %&gt;%\n  mutate(violent_2blocks = coalesce(violent_2blocks, 0L))\n\npetty_counts &lt;- st_join(buf_1blk, crime_petty, join = st_intersects, left = TRUE) %&gt;%\n  st_drop_geometry() %&gt;%\n  group_by(prop_id) %&gt;%\n  summarise(petty_1block = sum(!is.na(lat)), .groups = \"drop\") %&gt;%\n  right_join(prop_id_tbl, by = \"prop_id\") %&gt;%\n  mutate(petty_1block = coalesce(petty_1block, 0L))\n\nclean_data &lt;- clean_data %&gt;%\n  left_join(violent_counts, by = \"prop_id\") %&gt;%\n  left_join(petty_counts,  by = \"prop_id\")\n\n\n### 3b. Neighborhood join\n\ncandidate_cols &lt;- c(\n  \"name\",\"neighborhood\",\"neighborhood_name\",\"mapname\",\n  \"map_name\",\"label\",\"area_name\",\"neigh_name\",\"NAME\",\"LABEL\"\n)\nhit &lt;- intersect(candidate_cols, names(neigh_raw))\nNEIGH_NAME_COL &lt;- if (length(hit) &gt;= 1) hit[1] else {\n  stop(\"Couldn't find a neighborhood name column in neigh_raw.\")\n}\n\nneigh &lt;- neigh_raw %&gt;%\n  st_make_valid() %&gt;%\n  st_transform(CRS_TARGET) %&gt;%\n  select(neigh_name = !!sym(NEIGH_NAME_COL))\n\nprops_ctr &lt;- st_centroid(clean_data)[, \"prop_id\"]\nprops_with_neigh &lt;- st_join(props_ctr, neigh, join = st_within, left = TRUE)\n\nclean_data &lt;- clean_data %&gt;%\n  left_join(\n    st_drop_geometry(props_with_neigh) %&gt;% select(prop_id, neigh_name),\n    by = \"prop_id\"\n  )\n\n\n### 3c. Transit access\n\nstops &lt;- stops_raw %&gt;%\n  st_make_valid() %&gt;%\n  st_transform(CRS_TARGET) %&gt;%\n  filter(!st_is_empty(geometry))\n\n# recompute props_ctr just in case\nclean_data &lt;- clean_data %&gt;%\n  st_transform(CRS_TARGET) %&gt;%\n  st_make_valid() %&gt;%\n  mutate(prop_id = coalesce(prop_id, row_number()))\nprops_ctr &lt;- st_centroid(clean_data)[, \"prop_id\"]\n\nnn_idx &lt;- st_nearest_feature(props_ctr, stops)\nnearest_stops &lt;- stops[nn_idx, ]\ndist_vec &lt;- st_distance(props_ctr, nearest_stops, by_element = TRUE)\n\ncrs_units &lt;- tryCatch(st_crs(CRS_TARGET)$units_gdal, error = function(e) NA_character_)\ndist_stop_ft &lt;- to_feet(dist_vec, crs_units)\n\ncand &lt;- c(\"stop_name\",\"name\",\"STOP_NAME\",\"stop_id\",\"id\",\"STOP_ID\")\nhit  &lt;- intersect(cand, names(stops))\nstop_id_col &lt;- if (length(hit) &gt;= 1) hit[1] else NA_character_\n\nnearest_tbl &lt;- st_drop_geometry(nearest_stops) %&gt;%\n  mutate(\n    prop_id = props_ctr$prop_id,\n    dist_stop_ft = dist_stop_ft\n  ) %&gt;%\n  { if (!is.na(stop_id_col)) {\n      select(., prop_id, dist_stop_ft,\n             nearest_stop = !!sym(stop_id_col))\n    } else {\n      select(., prop_id, dist_stop_ft)\n    }\n  }\n\nclean_data &lt;- clean_data %&gt;%\n  left_join(nearest_tbl, by = \"prop_id\")\n\n\n### 3d. Bike network access\n\n# turn polygons into boundaries if needed (so distance-to-line makes sense)\ntypes &lt;- unique(st_geometry_type(bike_raw))\nbike_geom &lt;- st_geometry(bike_raw)\nif (any(grepl(\"POLYGON\", types))) {\n  bike_geom &lt;- st_boundary(bike_geom)\n}\n\nbike &lt;- bike_raw %&gt;%\n  st_set_geometry(bike_geom) %&gt;%\n  st_cast(\"MULTILINESTRING\", warn = FALSE) %&gt;%\n  st_make_valid() %&gt;%\n  st_transform(CRS_TARGET) %&gt;%\n  filter(!st_is_empty(geometry))\n\nclean_data &lt;- clean_data %&gt;%\n  st_transform(CRS_TARGET) %&gt;%\n  st_make_valid()\n\nprops_ctr &lt;- st_centroid(clean_data)[, \"prop_id\"]\n\nnn_idx &lt;- st_nearest_feature(props_ctr, bike)\nnearest_bike &lt;- bike[nn_idx, ]\ndist_vec &lt;- st_distance(props_ctr, nearest_bike, by_element = TRUE)\ndist_bike_ft &lt;- to_feet(dist_vec, crs_units)\n\ncand &lt;- c(\"name\",\"route\",\"route_name\",\"facility\",\"type\",\"street\",\"NAME\",\"ROUTE\")\nhit  &lt;- intersect(cand, names(bike))\nbike_col &lt;- if (length(hit) &gt;= 1) hit[1] else NA_character_\n\nnearest_tbl &lt;- st_drop_geometry(nearest_bike) %&gt;%\n  mutate(\n    prop_id       = props_ctr$prop_id,\n    dist_bike_ft  = dist_bike_ft\n  ) %&gt;%\n  { if (!is.na(bike_col)) {\n      select(., prop_id, dist_bike_ft,\n             nearest_bike_feature = !!sym(bike_col))\n    } else {\n      select(., prop_id, dist_bike_ft)\n    }\n  }\n\nclean_data &lt;- clean_data %&gt;%\n  left_join(nearest_tbl, by = \"prop_id\")\n\n\n### 3e. Hospital access\n\nhosp &lt;- hosp_raw %&gt;%\n  st_make_valid() %&gt;%\n  st_transform(CRS_TARGET) %&gt;%\n  filter(!st_is_empty(geometry))\n\nclean_data &lt;- clean_data %&gt;%\n  st_transform(CRS_TARGET) %&gt;%\n  st_make_valid()\n\nprops_ctr &lt;- st_centroid(clean_data)[, \"prop_id\"]\n\nnn_idx &lt;- st_nearest_feature(props_ctr, hosp)\nnearest_hosp &lt;- hosp[nn_idx, ]\ndist_vec &lt;- st_distance(props_ctr, nearest_hosp, by_element = TRUE)\n\ndist_hosp_mi &lt;- to_miles(dist_vec, crs_units)\n\ncand &lt;- c(\"name\",\"NAME\",\"hospital\",\"Hospital\",\"FACILITY\",\"facility\",\n          \"HOSPITAL\",\"inst_name\",\"INST_NAME\",\"id\",\"ID\")\nhit  &lt;- intersect(cand, names(hosp))\nhosp_col &lt;- if (length(hit) &gt;= 1) hit[1] else NA_character_\n\nnearest_tbl &lt;- st_drop_geometry(nearest_hosp) %&gt;%\n  mutate(\n    prop_id       = props_ctr$prop_id,\n    dist_hosp_mi  = dist_hosp_mi\n  ) %&gt;%\n  { if (!is.na(hosp_col)) {\n      select(., prop_id, dist_hosp_mi,\n             nearest_hospital = !!sym(hosp_col))\n    } else {\n      select(., prop_id, dist_hosp_mi)\n    }\n  } %&gt;%\n  mutate(\n    service_band_code = case_when(\n      dist_hosp_mi &lt; 1                     ~ 1L,\n      dist_hosp_mi &gt;= 1 & dist_hosp_mi &lt; 5 ~ 2L,\n      TRUE                                 ~ 3L\n    ),\n    service_band = factor(\n      service_band_code,\n      levels = c(1L, 2L, 3L),\n      labels = c(\"Too Close\", \"Within Service\", \"Out of Service\"),\n      ordered = TRUE\n    )\n  )\n\nclean_data &lt;- clean_data %&gt;%\n  left_join(nearest_tbl, by = \"prop_id\")\n\n\n### 3f. Park access\n\n# only polygon parks\nis_poly &lt;- st_geometry_type(parks_raw) %in% c(\"POLYGON\",\"MULTIPOLYGON\")\nparks &lt;- parks_raw[is_poly, ] %&gt;%\n  st_make_valid() %&gt;%\n  st_transform(CRS_TARGET) %&gt;%\n  filter(!st_is_empty(geometry))\n\nclean_data &lt;- clean_data %&gt;%\n  st_transform(CRS_TARGET) %&gt;%\n  st_make_valid()\n\nnn_idx &lt;- st_nearest_feature(clean_data, parks)\nnearest_parks &lt;- parks[nn_idx, ]\ndist_vec &lt;- st_distance(clean_data, nearest_parks, by_element = TRUE)\n\nunits_gdal &lt;- tryCatch(st_crs(CRS_TARGET)$units_gdal, error = function(e) NA_character_)\ndist_park_ft &lt;- to_feet(dist_vec, units_gdal)\ndist_park_mi &lt;- to_miles(dist_vec, units_gdal)\n\ncand &lt;- c(\"park_name\",\"name\",\"NAME\",\"asset_name\",\"ASSET_NAME\",\n          \"site_name\",\"SITE_NAME\",\"prop_name\",\"PROP_NAME\")\nhit  &lt;- intersect(cand, names(parks))\npark_col &lt;- if (length(hit) &gt;= 1) hit[1] else NA_character_\n\nnearest_tbl &lt;- st_drop_geometry(nearest_parks) %&gt;%\n  mutate(\n    prop_id      = clean_data$prop_id,\n    dist_park_ft = dist_park_ft,\n    dist_park_mi = dist_park_mi\n  ) %&gt;%\n  { if (!is.na(park_col)) {\n      select(., prop_id, dist_park_ft, dist_park_mi,\n             nearest_park = !!sym(park_col))\n    } else {\n      select(., prop_id, dist_park_ft, dist_park_mi)\n    }\n  }\n\nclean_data &lt;- clean_data %&gt;%\n  left_join(nearest_tbl, by = \"prop_id\")\n\n\n### 3g. School access\n\nschools &lt;- schools_in %&gt;%\n  st_make_valid() %&gt;%\n  mutate(type_norm = norm_school_type(.data[[\"type_specific\"]])) %&gt;%\n  filter(!is.na(type_norm))\n\n# if polygons, collapse to point-on-surface\nif (any(st_geometry_type(schools) %in% c(\"POLYGON\",\"MULTIPOLYGON\"))) {\n  st_geometry(schools) &lt;- st_point_on_surface(st_geometry(schools))\n}\n\nschools &lt;- schools %&gt;%\n  st_transform(CRS_TARGET) %&gt;%\n  filter(!st_is_empty(geometry))\n\nsch_pub  &lt;- filter(schools, type_norm == \"public\")\nsch_char &lt;- filter(schools, type_norm == \"charter\")\nsch_priv &lt;- filter(schools, type_norm == \"private\")\n\nclean_data &lt;- clean_data %&gt;%\n  st_transform(CRS_TARGET) %&gt;%\n  st_make_valid()\n\nprops_ctr &lt;- st_centroid(clean_data)[, \"prop_id\"]\n\nunits_gdal &lt;- tryCatch(st_crs(CRS_TARGET)$units_gdal, error = function(e) NA_character_)\n\ndist_school_public_ft  &lt;- nearest_dist_ft(props_ctr, sch_pub,  units_gdal)\ndist_school_charter_ft &lt;- nearest_dist_ft(props_ctr, sch_char, units_gdal)\ndist_school_private_ft &lt;- nearest_dist_ft(props_ctr, sch_priv, units_gdal)\n\ndist_tbl &lt;- st_drop_geometry(props_ctr) %&gt;%\n  transmute(\n    prop_id,\n    dist_school_public_ft  = dist_school_public_ft,\n    dist_school_charter_ft = dist_school_charter_ft,\n    dist_school_private_ft = dist_school_private_ft\n  )\n\nclean_data &lt;- clean_data %&gt;%\n  left_join(dist_tbl, by = \"prop_id\")\n\n\n### 3h. Food access\n\nclean_data &lt;- clean_data %&gt;%\n  st_transform(CRS_TARGET) %&gt;%\n  st_make_valid()\n\nprops_ctr &lt;- st_centroid(clean_data)[, \"prop_id\"]\n\nfood_proc &lt;- food_raw %&gt;%\n  st_make_valid()\n\nif (any(st_geometry_type(food_proc) %in% c(\"POLYGON\",\"MULTIPOLYGON\"))) {\n  st_geometry(food_proc) &lt;- st_point_on_surface(st_geometry(food_proc))\n}\n\nfood_proc &lt;- food_proc %&gt;%\n  st_transform(CRS_TARGET) %&gt;%\n  filter(!st_is_empty(geometry))\n\nif (nrow(food_proc) == 0L) {\n  clean_data &lt;- clean_data %&gt;%\n    mutate(\n      dist_foodretail_ft   = NA_real_,\n      foodretail_1mi_count = 0L\n    )\n} else {\n  units_gdal &lt;- tryCatch(st_crs(CRS_TARGET)$units_gdal, error = function(e) NA_character_)\n  one_mile &lt;- if (isTRUE(units_gdal %in% c(\"metre\",\"m\"))) 1609.344 else 5280\n\n  nn_idx &lt;- st_nearest_feature(props_ctr, food_proc)\n  nearest_food &lt;- food_proc[nn_idx, ]\n  dist_vec &lt;- st_distance(props_ctr, nearest_food, by_element = TRUE)\n  dist_foodretail_ft &lt;- to_feet(dist_vec, units_gdal)\n\n  buf_1mi &lt;- st_buffer(props_ctr, one_mile)[, \"prop_id\"]\n  cnt_tbl &lt;- st_join(buf_1mi, food_proc, join = st_intersects, left = TRUE) %&gt;%\n    st_drop_geometry() %&gt;%\n    count(prop_id, name = \"foodretail_1mi_count\")\n\n  dist_tbl &lt;- st_drop_geometry(props_ctr) %&gt;%\n    transmute(prop_id, dist_foodretail_ft = dist_foodretail_ft)\n\n  clean_data &lt;- clean_data %&gt;%\n    left_join(dist_tbl, by = \"prop_id\") %&gt;%\n    left_join(cnt_tbl, by = \"prop_id\") %&gt;%\n    mutate(foodretail_1mi_count = coalesce(foodretail_1mi_count, 0L))\n}\n\n\n### 3i. Police access\n\npol_proc &lt;- pol_raw %&gt;%\n  st_make_valid()\n\nif (any(st_geometry_type(pol_proc) %in% c(\"POLYGON\",\"MULTIPOLYGON\"))) {\n  st_geometry(pol_proc) &lt;- st_point_on_surface(st_geometry(pol_proc))\n}\n\npol_proc &lt;- pol_proc %&gt;%\n  st_transform(CRS_TARGET) %&gt;%\n  filter(!st_is_empty(geometry))\n\nif (nrow(pol_proc) == 0L) {\n  clean_data &lt;- clean_data %&gt;%\n    mutate(dist_police_ft = NA_real_)\n} else {\n  clean_data &lt;- clean_data %&gt;%\n    st_transform(CRS_TARGET) %&gt;%\n    st_make_valid()\n\n  props_ctr &lt;- st_centroid(clean_data)[, \"prop_id\"]\n\n  nn_idx &lt;- st_nearest_feature(props_ctr, pol_proc)\n  nearest_pol &lt;- pol_proc[nn_idx, ]\n  dist_vec &lt;- st_distance(props_ctr, nearest_pol, by_element = TRUE)\n  units_gdal &lt;- tryCatch(st_crs(CRS_TARGET)$units_gdal, error = function(e) NA_character_)\n  dist_police_ft &lt;- to_feet(dist_vec, units_gdal)\n\n  cand &lt;- c(\"name\",\"station\",\"precinct\",\"district\",\"NAME\",\"STATION\",\n            \"DISTRICT\",\"FACILITY\",\"facility\",\"id\",\"ID\")\n  hit  &lt;- intersect(cand, names(pol_proc))\n  pol_col &lt;- if (length(hit) &gt;= 1) hit[1] else NA_character_\n\n  nearest_tbl &lt;- st_drop_geometry(nearest_pol) %&gt;%\n    mutate(\n      prop_id        = props_ctr$prop_id,\n      dist_police_ft = dist_police_ft\n    ) %&gt;%\n    { if (!is.na(pol_col)) {\n        select(., prop_id, dist_police_ft,\n               nearest_police = !!sym(pol_col))\n      } else {\n        select(., prop_id, dist_police_ft)\n      }\n    }\n\n  clean_data &lt;- clean_data %&gt;%\n    left_join(nearest_tbl, by = \"prop_id\")\n}\n\n\n### 3j. Fire station access\n\nfire_proc &lt;- fire_raw %&gt;%\n  st_make_valid()\n\nif (any(st_geometry_type(fire_proc) %in% c(\"POLYGON\",\"MULTIPOLYGON\"))) {\n  st_geometry(fire_proc) &lt;- st_point_on_surface(st_geometry(fire_proc))\n}\n\nfire_proc &lt;- fire_proc %&gt;%\n  st_transform(CRS_TARGET) %&gt;%\n  filter(!st_is_empty(geometry))\n\nif (nrow(fire_proc) == 0L) {\n  clean_data &lt;- clean_data %&gt;%\n    mutate(dist_fire_ft = NA_real_)\n} else {\n  clean_data &lt;- clean_data %&gt;%\n    st_transform(CRS_TARGET) %&gt;%\n    st_make_valid()\n\n  props_ctr &lt;- st_centroid(clean_data)[, \"prop_id\"]\n\n  nn_idx &lt;- st_nearest_feature(props_ctr, fire_proc)\n  nearest_fire &lt;- fire_proc[nn_idx, ]\n  dist_vec &lt;- st_distance(props_ctr, nearest_fire, by_element = TRUE)\n  units_gdal &lt;- tryCatch(st_crs(CRS_TARGET)$units_gdal, error = function(e) NA_character_)\n  dist_fire_ft &lt;- to_feet(dist_vec, units_gdal)\n\n  cand &lt;- c(\"name\",\"station\",\"facility\",\"FACILITY\",\"STATION\",\n            \"company\",\"COMPANY\",\"house\",\"HOUSE\",\"id\",\"ID\")\n  hit  &lt;- intersect(cand, names(fire_proc))\n  fire_col &lt;- if (length(hit) &gt;= 1) hit[1] else NA_character_\n\n  nearest_tbl &lt;- st_drop_geometry(nearest_fire) %&gt;%\n    mutate(\n      prop_id      = props_ctr$prop_id,\n      dist_fire_ft = dist_fire_ft\n    ) %&gt;%\n    { if (!is.na(fire_col)) {\n        select(., prop_id, dist_fire_ft,\n               nearest_fire = !!sym(fire_col))\n      } else {\n        select(., prop_id, dist_fire_ft)\n      }\n    }\n\n  clean_data &lt;- clean_data %&gt;%\n    left_join(nearest_tbl, by = \"prop_id\")\n}\n\n\n############################################################\n# 4. ACS (tract-level sociodemographic context)\n############################################################\n\n# race / ethnicity\nrace_vars &lt;- c(\n  total     = \"B03002_001\",\n  white_nh  = \"B03002_003\",\n  black_nh  = \"B03002_004\",\n  asian_nh  = \"B03002_006\",\n  hispanic  = \"B03002_012\"\n)\n\npa_race &lt;- get_acs(\n  geography = GEOG,\n  state = STATE_FIPS,\n  year = ACS_YEAR,\n  survey = \"acs5\",\n  variables = race_vars,\n  output = \"wide\"\n) %&gt;%\n  transmute(\n    GEOID,\n    pct_white    = 100 * white_nhE / totalE,\n    pct_black    = 100 * black_nhE / totalE,\n    pct_hispanic = 100 * hispanicE / totalE,\n    pct_asian    = 100 * asian_nhE / totalE\n  )\n\n# age structure\nb01001  &lt;- get_acs(\n  geography = GEOG,\n  state = STATE_FIPS,\n  year = ACS_YEAR,\n  survey = \"acs5\",\n  table = \"B01001\"\n)\n\nvdict   &lt;- load_variables(ACS_YEAR, \"acs5\", cache = TRUE)\n\nb01001l &lt;- b01001 %&gt;%\n  left_join(vdict %&gt;% select(name, label), by = c(\"variable\" = \"name\"))\n\nsum_ages &lt;- function(df, pats) {\n  df %&gt;%\n    filter(str_detect(label, paste(pats, collapse = \"|\"))) %&gt;%\n    group_by(GEOID) %&gt;%\n    summarize(estimate = sum(estimate, na.rm = TRUE), .groups = \"drop\")\n}\n\nage_total &lt;- b01001l %&gt;%\n  filter(str_detect(label, \"Total:\")) %&gt;%\n  group_by(GEOID) %&gt;%\n  summarize(pop_total = first(estimate), .groups = \"drop\")\n\nage_25_44 &lt;- sum_ages(\n  b01001l,\n  c(\"Male:!!25 to 29 years\",\"Male:!!30 to 34 years\",\n    \"Male:!!35 to 39 years\",\"Male:!!40 to 44 years\",\n    \"Female:!!25 to 29 years\",\"Female:!!30 to 34 years\",\n    \"Female:!!35 to 39 years\",\"Female:!!40 to 44 years\")\n) %&gt;% rename(age_25_44 = estimate)\n\nage_45_64 &lt;- sum_ages(\n  b01001l,\n  c(\"Male:!!45 to 49 years\",\"Male:!!50 to 54 years\",\n    \"Male:!!55 to 59 years\",\"Male:!!60 and 61 years\",\n    \"Male:!!62 to 64 years\",\n    \"Female:!!45 to 49 years\",\"Female:!!50 to 54 years\",\n    \"Female:!!55 to 59 years\",\"Female:!!60 and 61 years\",\n    \"Female:!!62 to 64 years\")\n) %&gt;% rename(age_45_64 = estimate)\n\nage_65p &lt;- sum_ages(\n  b01001l,\n  c(\"Male:!!65 and 66 years\",\"Male:!!67 to 69 years\",\n    \"Male:!!70 to 74 years\",\"Male:!!75 to 79 years\",\n    \"Male:!!80 to 84 years\",\"Male:!!85 years and over\",\n    \"Female:!!65 and 66 years\",\"Female:!!67 to 69 years\",\n    \"Female:!!70 to 74 years\",\"Female:!!75 to 79 years\",\n    \"Female:!!80 to 84 years\",\"Female:!!85 years and over\")\n) %&gt;% rename(age_65plus = estimate)\n\npa_age &lt;- age_total %&gt;%\n  left_join(age_25_44, by = \"GEOID\") %&gt;%\n  left_join(age_45_64, by = \"GEOID\") %&gt;%\n  left_join(age_65p,   by = \"GEOID\") %&gt;%\n  mutate(\n    pct_age_25_44 = 100 * age_25_44 / pop_total,\n    pct_age_45_64 = 100 * age_45_64 / pop_total,\n    pct_age_65plus = 100 * age_65plus / pop_total\n  ) %&gt;%\n  select(GEOID, pct_age_25_44, pct_age_45_64, pct_age_65plus)\n\n# housing + rent\nhousing_vars &lt;- c(\n  hu_total  = \"B25002_001\",\n  hu_vacant = \"B25002_003\",\n  med_rent  = \"B25064_001\"\n)\npa_housing &lt;- get_acs(\n  geography = GEOG,\n  state = STATE_FIPS,\n  year = ACS_YEAR,\n  survey = \"acs5\",\n  variables = housing_vars,\n  output = \"wide\"\n) %&gt;%\n  transmute(\n    GEOID,\n    vacancy_rate   = 100 * hu_vacantE / hu_totalE,\n    med_gross_rent = med_rentE\n  )\n\n# income + poverty\ninc_pov_vars &lt;- c(\n  med_hh_inc = \"B19013_001\",\n  pov_below  = \"B17001_002\",\n  pov_univ   = \"B17001_001\"\n)\npa_inc_pov &lt;- get_acs(\n  geography = GEOG,\n  state = STATE_FIPS,\n  year = ACS_YEAR,\n  survey = \"acs5\",\n  variables = inc_pov_vars,\n  output = \"wide\"\n) %&gt;%\n  transmute(\n    GEOID,\n    med_hh_income = med_hh_incE,\n    poverty_rate  = 100 * pov_belowE / pov_univE\n  )\n\n# education\nb15003 &lt;- get_acs(\n  geography = GEOG,\n  state = STATE_FIPS,\n  year = ACS_YEAR,\n  survey = \"acs5\",\n  table = \"B15003\"\n)\nedu_total &lt;- b15003 %&gt;%\n  filter(variable == \"B15003_001\") %&gt;%\n  select(GEOID, edu_total = estimate)\n\nba_plus_codes &lt;- c(\"B15003_022\",\"B15003_023\",\"B15003_024\",\"B15003_025\")\nedu_bap &lt;- b15003 %&gt;%\n  filter(variable %in% ba_plus_codes) %&gt;%\n  group_by(GEOID) %&gt;%\n  summarize(ba_plus = sum(estimate, na.rm = TRUE), .groups = \"drop\")\n\nhs_only_code &lt;- \"B15003_017\"\nedu_hs &lt;- b15003 %&gt;%\n  filter(variable == hs_only_code) %&gt;%\n  transmute(GEOID, hs_only = estimate)\n\npa_edu &lt;- edu_total %&gt;%\n  left_join(edu_bap, by = \"GEOID\") %&gt;%\n  left_join(edu_hs,  by = \"GEOID\") %&gt;%\n  mutate(\n    pct_ba_plus = 100 * ba_plus / edu_total,\n    pct_hs_only = 100 * hs_only / edu_total\n  ) %&gt;%\n  select(GEOID, pct_ba_plus, pct_hs_only)\n\n# unemployment\nlabor_vars &lt;- c(\n  civ_lf     = \"B23025_003\",\n  unemployed = \"B23025_005\"\n)\npa_labor &lt;- get_acs(\n  geography = GEOG,\n  state = STATE_FIPS,\n  year = ACS_YEAR,\n  survey = \"acs5\",\n  variables = labor_vars,\n  output = \"wide\"\n) %&gt;%\n  transmute(\n    GEOID,\n    unemployment_rate = 100 * unemployedE / civ_lfE\n  )\n\nPA_ACS &lt;- pa_race %&gt;%\n  left_join(pa_age,     by = \"GEOID\") %&gt;%\n  left_join(pa_housing, by = \"GEOID\") %&gt;%\n  left_join(pa_inc_pov, by = \"GEOID\") %&gt;%\n  left_join(pa_edu,     by = \"GEOID\") %&gt;%\n  left_join(pa_labor,   by = \"GEOID\")\n\n\n############################################################\n# 5. ATTACH TRACT GEOID TO PARCELS AND MERGE ACS\n############################################################\n\n# get statewide tracts, project to CRS_TARGET\npa_tracts &lt;- tracts(state = \"PA\", year = ACS_YEAR, class = \"sf\") %&gt;%\n  st_transform(CRS_TARGET) %&gt;%\n  select(GEOID)\n\nprops_ctr &lt;- st_centroid(clean_data)[, \"prop_id\"]\nprops2tract &lt;- st_join(props_ctr, pa_tracts, join = st_within, left = TRUE)\n\nclean_data &lt;- clean_data %&gt;%\n  left_join(st_drop_geometry(props2tract), by = \"prop_id\") %&gt;%\n  left_join(PA_ACS, by = \"GEOID\")\n\n\n############################################################\n# 6. FINAL FILTERS, EXPORT, DERIVED FIELDS\n############################################################\n\nn_before &lt;- nrow(clean_data)\n\nclean_data &lt;- clean_data %&gt;%\n  mutate(\n    sale_price = suppressWarnings(as.numeric(sale_price))\n  ) %&gt;%\n  filter(!is.na(sale_price), sale_price &gt;= 10000)\n\nn_after &lt;- nrow(clean_data)\nmessage(\"Dropped \", n_before - n_after,\n        \" rows with sale_price &lt; $10,000 (or missing).\")\n\n# create non-spatial copy for modeling, plus psf/value_multiple like you had\nsales_data &lt;- clean_data %&gt;%\n  st_drop_geometry() %&gt;%\n  mutate(\n    psf = round(sale_price / total_livable_area, 2),\n    value_multiple = round(market_value / sale_price, 2)\n  ) %&gt;%\n  arrange(psf)\n\ngeom_backup &lt;- clean_data\n\nrm(list = setdiff(ls(), c(\"clean_data\", \"sales_data\", \"geom_backup\" )))\n\n\nLogical Underpinning:\nThe modeling pipeline builds a property-level dataset that doesn’t just look at what the house is, but where the house is — and that “where” part is doing a lot of work. Beyond the standard housing attributes (square footage, year built, condition, number of beds/baths, etc.), we engineered a bunch of spatial features that try to capture a home’s context in Philadelphia. Every parcel in the sales dataset was turned into geometry using its recorded shape, projected into a local coordinate reference system, and then enriched with measurements like “How far is this property from transit?” or “How many food retailers are within a mile?” The point is: we’re not just assuming the structure itself determines sale price. We’re explicitly measuring access, amenities, safety, and neighborhood context as part of value.\nCrime exposure is one of the biggest contextual signals we added. For every sold property, we counted the number of reported “violent” incidents within roughly two blocks (~600 feet) and the number of “non-violent/misdemeanor” incidents within about one block (~300 feet). We defined “violent” broadly — not only homicides and robberies, but any incident where a gun or weapon was mentioned in the police data. The logic is that buyers and appraisers implicitly price in perceived safety. If two houses are physically similar but one sits in an area with more firearm-related incidents, we expect downward pressure on sale price. That becomes a numeric predictor: more nearby violent incidents → potentially lower willingness to pay.\nWe also quantified access to urban infrastructure and services, because convenience is money. For each property we measured the straight-line distance to the nearest transit stop, the nearest marked bike facility, the nearest hospital, the nearest fire station, and the nearest police station. We also measured distance to the nearest park, using parcel polygons against park polygons to approximate “are you on/next to green space or are you park-poor?” These distances were then standardized (feet or miles depending on context) and in some cases bucketed. For hospitals, for example, we didn’t just record distance — we classified properties into service bands: “Too Close” (&lt;1 mile, which might mean sirens/traffic and isn’t always seen as a plus), “Within Service” (1–5 miles, generally good coverage), and “Out of Service” (5+ miles). That turns an abstract geometry calculation into something interpretable as access vs. nuisance.\nFood access is handled a little differently because it’s partly about proximity and partly about density. For each property, we calculated (1) distance to the nearest food retailer (grocery, etc.) and (2) how many food retail locations exist within a one-mile buffer around that property’s centroid. That second feature is important: being near just one corner store is not the same thing as sitting in a commercial corridor with 10+ options. A buyer will pay more, all else equal, for a place where groceries are easy, healthy food is available, and “I don’t need a car for basics” is true. That’s especially relevant in Philadelphia, where car-free living is common and neighborhood retail is a quality-of-life driver.\nWe didn’t stop at amenities and services — we also tied every property to its neighborhood fabric. Each parcel was spatially joined to a named neighborhood (via centroid-in-neighborhood polygons), and to a Census tract. From the tract, we attached socioeconomic context from the American Community Survey: racial/ethnic composition, age structure, vacancy rate, median rent, median household income, poverty rate, unemployment rate, and educational attainment (e.g. share of adults with a bachelor’s or higher). Those variables roughly proxy demand-side pressure and neighborhood stability. High-income, low-vacancy, high-education tracts tend to support higher property prices, even for physically modest homes. That’s not just aesthetics — lenders, buyers, and investors all underwrite neighborhood trajectory, not just individual bricks.\nPut together, the result is a modeling dataset where sale price (or price per square foot) isn’t being predicted just from “bedrooms and bathrooms.” It’s being predicted from “bedrooms and bathrooms, plus how safe the block feels, plus how close you are to the El or a bike lane, plus whether you can walk to food, plus how your surrounding tract looks in terms of income and vacancy.” This is exactly how real-world housing markets behave: value is hyper-local and access-based. By engineering these spatial features, we’re letting the model learn the price premium (or penalty) associated with each of those location advantages — effectively turning geography into numbers the model can reason with.\n\n\n\n\nPart 2: Exploratory Data Analysis\nHistogram: Distribution of sale prices\n\nggplot(sales_data, aes(x = sale_price)) +\n  geom_histogram(fill = \"orange\", bins= 500) +\n  geom_vline(aes(xintercept = median(sale_price, na.rm = TRUE)),\n             color = \"grey40\", size = 0.5) +\n  annotate(\"text\", \n           x = median(sales_data$sale_price, na.rm = TRUE), \n           y = Inf, vjust = 4, hjust = -0.1, \n           label = paste0(\"Median = $\", formatC(median(sales_data$sale_price, na.rm = TRUE), format = \"f\", big.mark = \",\", digits = 0)),\n           color = \"grey40\", size = 3) +\n  labs(title = \"Distribution of Philadelphia Home Sale Price\", x = \"Sale Price (USD)\", y = \"Frequency\") +\n  scale_x_continuous(labels = scales::dollar)\n\n\n\n\n\n\n\n\nInterpretation: This figure shows the distribution of Philadelphia home sale prices in 2023–2024, and the key pattern is that the market is extremely skewed. Most sales are clustered well under 500K USD, and the median sale price — about 241K USD, marked on the plot — is a good summary of what a “typical” home actually sells for in the city. But there’s also a very long tail of high-value transactions that stretches into the millions. Those ultra-expensive deals are rare, but they’re so large that they would pull the average price up, which is why the mean would be misleading here. This shape tells us Philadelphia isn’t one uniform housing market: it’s a mix of lower-cost rowhomes, modest rehabs, and a smaller luxury segment. The result is a city where most buyers transact in the low hundreds of thousands, but a small number of high-end sales coexist on the fringe at several million dollars.\nMap: Geographic distribution (map)\n\n# Re-create the neighborhood layer from the URL\n\nneigh_url &lt;- \"https://raw.githubusercontent.com/opendataphilly/open-geo-data/refs/heads/master/philadelphia-neighborhoods/philadelphia-neighborhoods.geojson\"\n\nneigh &lt;- sf::read_sf(neigh_url)\n\n# 2. Make sure CRS matches parcels\nneigh &lt;- sf::st_transform(neigh, sf::st_crs(clean_data))\n\n# 3. Build the sf of JUST the modeled sales\n#    (this step assumes both data frames still share prop_id;\n#     if prop_id got dropped from sales_data, skip the filter)\nif (\"prop_id\" %in% names(clean_data) &&\n    \"prop_id\" %in% names(sales_data)) {\n  geo_clean &lt;- clean_data %&gt;%\n    dplyr::filter(prop_id %in% sales_data$prop_id)\n} else {\n  geo_clean &lt;- clean_data\n}\n\n# 4. Draw the map\np_price_map &lt;-\n  ggplot2::ggplot() +\n  ggplot2::geom_sf(\n    data = neigh,\n    fill = NA,\n    color = \"grey70\",\n    linewidth = 0.25\n  ) +\n  ggplot2::geom_sf(\n    data = geo_clean,\n    ggplot2::aes(color = sale_price),\n    alpha = 0.7,\n    size  = 0.2\n  ) +\n  scale_color_viridis_c(\n    name      = \"Sale price (USD)\",\n    option    = \"magma\",\n    direction = -1,\n    trans     = \"log10\",\n    breaks    = c(100000, 200000, 300000, 500000, 800000, 1200000, 2000000),\n    labels    = scales::dollar,\n    limits    = c(60000, 2000000),\n    oob       = scales::squish\n  ) +\n  ggplot2::guides(\n    color = ggplot2::guide_colorbar(barheight = grid::unit(4, \"cm\"))\n  ) +\n  ggplot2::coord_sf() +\n  ggplot2::labs(\n    title    = \"Geographical Distribution of Philadelphia Home Sale Prices\",\n    subtitle = \"Each property colored by sale price (log scale)\"\n  ) +\n  ggplot2::theme_void(base_size = 12) +\n  ggplot2::theme(\n    legend.position = c(0.92, 0.25),\n    plot.title      = ggplot2::element_text(face = \"bold\", hjust = 0),\n    plot.subtitle   = ggplot2::element_text(hjust = 0)\n  )\n\np_price_map\n\n\n\n\n\n\n\n\nInterpretation: This map shows that price in Philadelphia is deeply spatial. Each point is a property sale, colored by its sale price (on a log scale so we can see variation across the whole city). You can see clusters of high-value sales in a few tight areas — darker purple concentrations in and around Center City, parts of South Philly near the core, Northern Liberties/Fishtown, and select pockets in Northwest neighborhoods. Those are the submarkets where renovated or newer housing sells at a clear premium. As you move away from those cores, especially into large areas of Southwest, lower North, and parts of far Northeast and river-adjacent industrial fringe, prices shift toward the lighter end of the palette, meaning lower transaction values. The key insight is that this isn’t a smooth gradient from “expensive downtown” to “cheap outskirts.” Instead, it’s patchy: there are block- and neighborhood-scale price islands, even within the same general region of the city. That pattern is exactly why we model price with neighborhood fixed effects — because buyers are really choosing hyperlocal markets, not just “Philadelphia, broadly.”\nScatter Plots: Price vs. structural features\n\n# Build plotting data frame with clean vars\nplot_df &lt;- sales_data %&gt;%\n  mutate(\n    beds     = number_of_bedrooms,\n    baths    = number_of_bathrooms,\n    liv_area = total_livable_area,\n    age      = 2024 - year_built,\n    beds_f   = factor(number_of_bedrooms),\n    baths_f  = factor(number_of_bathrooms)\n  ) %&gt;%\n  filter(!is.na(sale_price),\n         !is.na(liv_area),\n         !is.na(age),\n         !is.na(beds), beds &gt; 0,\n         !is.na(baths), baths &gt; 0)\n\n## Plot A: Price vs. bedrooms (categorical)\np_beds &lt;- ggplot(plot_df, aes(x = beds_f, y = sale_price)) +\n  geom_jitter(width = 0.2, alpha = 0.15, size = 0.6, color = \"steelblue\") +\n  stat_summary(fun = median, geom = \"point\", color = \"red\", size = 2) +\n  scale_y_continuous(labels = scales::dollar) +\n  labs(\n    x = \"Bedrooms (factor)\",\n    y = \"Sale price (USD)\",\n    title = \"Price vs Bedrooms\",\n    subtitle = \"Each dot = a sale; red = median price in that bedroom category\"\n  ) +\n  theme_minimal(base_size = 11)\n\n## Plot B: Price vs. bathrooms (categorical)\np_baths &lt;- ggplot(plot_df, aes(x = baths_f, y = sale_price)) +\n  geom_jitter(width = 0.2, alpha = 0.15, size = 0.6, color = \"darkorange\") +\n  stat_summary(fun = median, geom = \"point\", color = \"red\", size = 2) +\n  scale_y_continuous(labels = scales::dollar) +\n  labs(\n    x = \"Bathrooms (factor)\",\n    y = \"Sale price (USD)\",\n    title = \"Price vs Bathrooms\",\n    subtitle = \"Adding bathrooms tends to move price up in steps, not smoothly\"\n  ) +\n  theme_minimal(base_size = 11)\n\n## Plot C: Price vs. interior living area (continuous)\np_area &lt;- ggplot(plot_df, aes(x = liv_area, y = sale_price)) +\n  geom_point(alpha = 0.15, size = 0.6, color = \"seagreen4\") +\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 0.7, color = \"black\") +\n  scale_y_continuous(labels = scales::dollar) +\n  labs(\n    x = \"Living area (sq ft)\",\n    y = \"Sale price (USD)\",\n    title = \"Price vs Interior Size\",\n    subtitle = \"Larger homes almost always sell for more\"\n  ) +\n  theme_minimal(base_size = 11)\n\n## Plot D: Price vs. building age (continuous)\np_age &lt;- ggplot(plot_df, aes(x = age, y = sale_price)) +\n  geom_point(alpha = 0.15, size = 0.6, color = \"purple4\") +\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 0.7, color = \"black\") +\n  scale_y_continuous(labels = scales::dollar) +\n  labs(\n    x = \"Building age (years)\",\n    y = \"Sale price (USD)\",\n    title = \"Price vs Building Age\",\n    subtitle = \"Newer buildings tend to transact higher, but there's a ton of variance\"\n  ) +\n  theme_minimal(base_size = 11)\n\n## Combine into one figure (2x2 grid)\nfigure_3_structural &lt;- (p_beds | p_baths) / (p_area | p_age)\n\nfigure_3_structural\n\n\n\n\n\n\n\n\nInterpretation:\n\nPrice vs Bedrooms: Each point is an individual sale, grouped by reported bedroom count. The red dots mark the median sale price for that bedroom category. You don’t see a smooth upward climb as beds go up. Instead, most of the market activity — and most of the value — sits in the 2–4 bedroom range, and adding more bedrooms beyond that doesn’t reliably buy you a more expensive house. In fact, once you control for overall square footage in the regression, “more bedrooms in the same box” can actually be a negative signal: it often means the house has been carved up into smaller rooms instead of feeling open. That shows up here as a pretty flat (and sometimes even downward) median past 3–4 beds, rather than a luxury premium for “more bedrooms.”\nPrice vs Bathrooms: Bathrooms behave differently. As you move from 1 bath to 2 to 3+ baths, the red median markers jump upward in more distinct steps. Buyers clearly pay for bathroom count, because bathrooms are basically a livability and privacy amenity. You don’t see as much scatter in the medians as you add baths — the relationship is more monotonic. That lines up with the model result that, holding everything else constant, each additional bathroom adds meaningful sale value. In plain English: a second full bath is treated as an upgrade; a fourth bedroom is not automatically treated as an upgrade.\nPrice vs Interior Size: This panel just shows the classic size premium. Larger homes almost always sell for more. Even though there’s a long tail of huge outliers (multi-million dollar sales and extremely large properties), the main cloud is very tight and upward sloping: once you get past about 1,000–1,500 square feet of living area, prices scale up quickly. The fitted line through that cloud makes that slope obvious. That’s consistent with our regression, where each additional square foot of interior living space is worth on the order of a few dozen USD — real money — and that effect is strong even after controlling for beds and baths.\nPrice vs Building Age: Age is noisy. Newer properties (low age on the x-axis) can sell very high, which matches the story of new construction and high-end rehab commanding premiums. But as age increases, there’s a wide spread: some 100-year-old Philly rowhomes still sell for serious money, and others sell cheaply. The smooth line trends slightly downward with age, but the variance explodes. That tells you age alone isn’t destiny. In Philly’s housing stock — a lot of which is 80–120+ years old — “old” can mean “historic, updated, desirable block,” or it can mean “deferred maintenance and structural risk.” The market doesn’t punish age uniformly; it punishes age plus condition.\n\nScatter Plots: Price vs. spatial features\n\n# Build plotting data frame for spatial/context features\nplot_spatial &lt;- sales_data %&gt;%\n  select(\n    sale_price,\n    dist_park_mi,\n    violent_2blocks,\n    foodretail_1mi_count,\n    med_hh_income\n  ) %&gt;%\n  filter(\n    !is.na(sale_price),\n    sale_price &gt; 0,\n    !is.na(dist_park_mi),\n    !is.na(violent_2blocks),\n    !is.na(foodretail_1mi_count),\n    !is.na(med_hh_income)\n  )\n\n## Plot A: Price vs distance to nearest park (miles)\np_park &lt;- ggplot(plot_spatial,\n                 aes(x = dist_park_mi, y = sale_price)) +\n  geom_point(alpha = 0.15, size = 0.6, color = \"darkgreen\") +\n  geom_smooth(method = \"lm\", se = FALSE,\n              linewidth = 0.7, color = \"black\") +\n  scale_y_continuous(labels = scales::dollar) +\n  labs(\n    x = \"Distance to nearest park (miles)\",\n    y = \"Sale price (USD)\",\n    title = \"Parks & Property Value\",\n    subtitle = \"Homes farther from parks tend to sell for slightly less,\\nthough the effect is noisy\"\n  ) +\n  theme_minimal(base_size = 11)\n\n## Plot B: Price vs violent incidents within ~2 blocks\np_crime &lt;- ggplot(plot_spatial,\n                  aes(x = violent_2blocks, y = sale_price)) +\n  geom_point(alpha = 0.15, size = 0.6, color = \"firebrick\") +\n  geom_smooth(method = \"lm\", se = FALSE,\n              linewidth = 0.7, color = \"black\") +\n  scale_y_continuous(labels = scales::dollar) +\n  labs(\n    x = \"Violent incidents within ~600 ft\",\n    y = \"Sale price (USD)\",\n    title = \"Local Violent Crime\",\n    subtitle = \"Higher recent violent incident counts are associated with\\nlower observed sale prices\"\n  ) +\n  theme_minimal(base_size = 11)\n\n## Plot C: Price vs food access (# of retailers within 1 mile)\np_food &lt;- ggplot(plot_spatial,\n                 aes(x = foodretail_1mi_count, y = sale_price)) +\n  geom_point(alpha = 0.15, size = 0.6, color = \"goldenrod4\") +\n  geom_smooth(method = \"lm\", se = FALSE,\n              linewidth = 0.7, color = \"black\") +\n  scale_y_continuous(labels = scales::dollar) +\n  labs(\n    x = \"Food retailers within 1 mile\",\n    y = \"Sale price (USD)\",\n    title = \"Retail Food Environment\",\n    subtitle = \"More nearby food retail sometimes tracks denser, more affordable areas\"\n  ) +\n  theme_minimal(base_size = 11)\n\n## Plot D: Price vs neighborhood income (ACS)\np_income &lt;- ggplot(plot_spatial,\n                   aes(x = med_hh_income, y = sale_price)) +\n  geom_point(alpha = 0.15, size = 0.6, color = \"navy\") +\n  geom_smooth(method = \"lm\", se = FALSE,\n              linewidth = 0.7, color = \"black\") +\n  scale_x_continuous(labels = scales::dollar) +\n  scale_y_continuous(labels = scales::dollar) +\n  labs(\n    x = \"Median household income (USD, tract)\",\n    y = \"Sale price (USD)\",\n    title = \"Income & Home Prices\",\n    subtitle = \"Higher-income Census tracts show systematically higher sale prices\"\n  ) +\n  theme_minimal(base_size = 11)\n\n## Arrange in a 2x2 panel\nfigure_4_spatial &lt;- (p_park | p_crime) / (p_food | p_income)\n\nfigure_4_spatial\n\n\n\n\n\n\n\n\nInterpretation:\n\nPrice vs Bedrooms: The first panel plots sale price against distance to the nearest park. There’s a weak downward pattern: homes closer to parks tend to reach somewhat higher prices, while homes farther away drift slightly lower. But the cloud is noisy — you can still get expensive sales even 0.5–0.75 miles from a park — which tells us “park adjacency” isn’t a universal price driver. It matters, but not nearly as much as core housing features like condition or size. This also hints that park access is partly a neighborhood-level amenity (already priced into where you’re buying), not always a block-by-block differentiator.\nLocal Violent Crime: Here we see sale price versus the number of violent incidents within roughly 600 feet of the property. This relationship is a lot more directional: higher recent violent counts are associated with sharply lower sale prices. The bottom edge of the market clusters in high-violence blocks, and the very high-dollar sales almost never appear where violence is concentrated. This backs up what we saw in the regression: even within the same nominal neighborhood, buyers are discounting blocks that feel unsafe. Street-level safety is being priced into the deal.\nRetail Food Environment: This panel shows how sale price varies with the number of food retailers within a one-mile radius. There’s basically no clean slope: properties in areas with lots of nearby grocers/delis/bodegas span the full range from cheaper houses to million-dollar-plus sales. If anything, very high counts of food outlets often show up in dense, lower-cost parts of the city — which makes sense in Philly, where corner retail is common in rowhouse neighborhoods. So “more food within a mile” doesn’t automatically equal “higher value.” It’s more of a neighborhood-style signal (urban, walkable, older fabric) than a direct premium within that neighborhood.\nIncome & Home Prices: This last panel compares Census tract median household income to sale price. Here the slope is much clearer and positive: higher-income tracts systematically produce higher sale prices, and you almost never see ultra-high-dollar sales in the very lowest-income tracts. That’s basically the social geography of value. It reflects more than house quality — it’s also about perceived status, lending environment, school expectations, and long-run stability. This is exactly the kind of demographic gradient that still shows up in the model even after we control for physical housing traits.\n\nBubble Plot: One creative visualization\n\nacs_vars &lt;- c(\n  med_income   = \"B19013_001\",\n  poverty_rate = \"S1701_C02_001\"\n)\n\nacs_tracts &lt;- get_acs(\n  geography = \"tract\",\n  state     = \"PA\",\n  county    = \"Philadelphia\",\n  variables = acs_vars,\n  year      = 2023,\n  survey    = \"acs5\",\n  geometry  = TRUE,\n  cache_table = TRUE\n) %&gt;%\n  select(GEOID, variable, estimate, geometry) %&gt;%\n  st_transform(st_crs(geo_clean)) %&gt;%\n  pivot_wider(names_from = variable, values_from = estimate) %&gt;%\n  st_as_sf()\n\nacs_to_neigh &lt;- acs_tracts %&gt;%\n  st_point_on_surface() %&gt;%\n  st_join(neigh %&gt;% select(NAME)) %&gt;%\n  st_drop_geometry() %&gt;%\n  filter(!is.na(NAME)) %&gt;%\n  group_by(NAME) %&gt;%\n  summarise(\n    med_income_acs   = median(med_income, na.rm = TRUE),\n    poverty_rate_acs = median(poverty_rate, na.rm = TRUE)\n  ) %&gt;%\n  ungroup()\n\nsales_neigh &lt;- sales_data %&gt;%\n  filter(sale_price &gt; 0) %&gt;%\n  group_by(neigh_name) %&gt;%\n  summarise(\n    n_sales      = n(),\n    median_price = median(sale_price, na.rm = TRUE)\n  ) %&gt;%\n  ungroup()\n\nneigh_summary_fixed &lt;- sales_neigh %&gt;%\n  left_join(acs_to_neigh, by = c(\"neigh_name\" = \"NAME\")) %&gt;%\n  drop_na(med_income_acs, poverty_rate_acs)\n\np_neigh_bubble_fixed &lt;-\n  ggplot(neigh_summary_fixed,\n         aes(x = med_income_acs, y = median_price,\n             size = n_sales, color = poverty_rate_acs)) +\n  geom_point(alpha = 0.85) +\n  scale_x_continuous(labels = scales::dollar) +\n  scale_y_continuous(labels = scales::dollar) +\n  scale_color_viridis_c(option = \"plasma\", name = \"Poverty rate (%)\") +\n  scale_size_continuous(name = \"Sales count\", range = c(2, 10)) +\n  labs(\n    title = \"Neighborhood Economics and Home Prices\",\n    subtitle = \"Each point = a neighborhood (2023–2024 sales)\\nX: ACS Median household income · Y: Median sale price · Color: ACS poverty rate · Size: sales count\",\n    x = \"Median household income (USD)\",\n    y = \"Median sale price (USD)\"\n  ) +\n  theme_minimal(base_size = 12) +\n  theme(legend.position = \"right\")\n\np_neigh_bubble_fixed\n\n\n\n\n\n\n\n\nInterpretation: This bubble chart is basically the “neighborhood market reality” view. Each dot is a Philadelphia neighborhood, and we’re plotting its median sale price (y-axis) against its median household income from ACS (x-axis). Two things jump out immediately. First, the slope is very positive: richer neighborhoods almost always have higher typical sale prices. You almost never see a low-income neighborhood with a 700K+ median sale price, and you don’t see high-income neighborhoods with 150K medians — so price and local income are tightly linked at the neighborhood scale. Second, the color shading (poverty rate) lines up with that same gradient: neighborhoods with high poverty (yellow) sit in the low-income / low-price corner, while wealthier neighborhoods (purple/blue, lower poverty) show higher sales prices. The dot size is how many homes sold, which tells you where volume is. You can see that most of the activity (the big circles) is happening in the middle of the distribution — not the ultra-rich enclaves and not the most distressed areas, but the working-to-middle income neighborhoods. In plain language: Philadelphia’s housing market is stratified by neighborhood wealth and poverty status, and that stratification is not subtle.\n\n\n\nPart 3: Feature Pruning\n\nselect_data &lt;- sales_data |&gt; \n  select(year_built, number_of_bathrooms, number_of_bedrooms, basements, \n         fireplaces, garage_spaces,\n         general_construction, interior_condition, exterior_condition, \n         census_tract, neigh_name, \n         value_multiple, market_value, sale_price, total_area, total_livable_area, \n         psf, violent_2blocks, petty_1block, dist_park_mi, \n         vacancy_rate, med_gross_rent, med_hh_income, pct_ba_plus, \n         pct_white, pct_black, pct_hispanic, pct_asian,\n         foodretail_1mi_count, \n         dist_school_public_ft, dist_school_charter_ft, \n         pct_age_25_44, pct_age_65plus,\n         poverty_rate, unemployment_rate) |&gt; \n  filter(number_of_bathrooms &gt; 0, number_of_bedrooms &gt; 0) |&gt; \n  mutate(age = 2024 - year_built,\n         basements = dplyr::recode(trimws(as.character(basements)),\n                              \"0\"=\"0\",\"A\"=\"1\",\"B\"=\"2\",\"C\"=\"3\",\"D\"=\"4\",\"E\"=\"5\",\n                              \"F\"=\"6\",\"G\"=\"7\",\"H\"=\"8\",\"I\"=\"9\"),\n         basements = as.integer(basements)) |&gt; \n  rename(\n    baths      = number_of_bathrooms,\n    beds       = number_of_bedrooms,\n    construction = general_construction,\n    interior   = interior_condition,\n    exterior   = exterior_condition,\n    garage     = garage_spaces,\n    tract      = census_tract,\n    neigh      = neigh_name,\n    mkt_value  = market_value,\n    area       = total_area,\n    liv_area   = total_livable_area\n  )\n\nReasoning: For the modeling stage we deliberately narrowed the feature set to variables that are (1) directly interpretable in a housing valuation context, (2) broadly available across observations, and (3) not redundant with each other. The earlier pipeline generated a huge number of spatial and contextual attributes — multiple school distance measures, different types of crime buffers, access to transit, bike lanes, fire/police proximity, hospital service bands, etc. That’s great for exploratory geography-of-equity questions, but it’s overkill (and sometimes harmful) for estimating price. Many of those features are highly collinear with each other or with neighborhood identity, and a lot of them introduce sparsity because they’re missing for certain parcels or depend on fragile spatial joins. Here, we keep classic structural housing characteristics (beds, baths, living area, age, basements, condition, garage, fireplaces), because buyers literally pay for those. We keep a small set of location/amenity signals that plausibly capitalize into price and that we trust methodologically: nearby violent incidents (violent_2blocks), food access density (foodretail_1mi_count), park proximity (dist_park_mi), and distance to schools. We also attach tract-level socioeconomic context from ACS (income, vacancy, education, race composition, poverty, unemployment, and age structure) to proxy neighborhood demand and perceived quality. Finally, we retain sale_price itself plus market_value and ratios like value_multiple and psf, because those let us study how assessment and unit price per square foot behave. In short, we’re trimming down to high-signal, low-missingness, economically meaningful predictors and dropping niche engineering steps that add noise, inflate multicollinearity, or don’t generalize well in a pricing model.\n\n# Intuition: market value should not deviate substantially from the sale price\nvalue_range &lt;- select_data |&gt; \n  summarize(\n    q1    = quantile(value_multiple, 0.25, na.rm = TRUE),\n    q3    = quantile(value_multiple, 0.75, na.rm = TRUE),\n    iqr   = q3 - q1,\n    lower = q1 - 1.5 * iqr,\n    upper = q3 + 1.5 * iqr\n  )\n\n# Kable table for value_range\nvalue_range |&gt; \n  mutate(\n    q1    = round(q1, 2),\n    q3    = round(q3, 2),\n    iqr   = round(iqr, 2),\n    lower = round(lower, 2),\n    upper = round(upper, 2)\n  ) |&gt;\n  kable(\n    caption = \"Flag thresholds for Value Multiple (Assessed Value ÷ Sale Price)\",\n    col.names = c(\"Q1\", \"Q3\", \"IQR\", \"Lower Bound\", \"Upper Bound\"),\n    align = \"c\",\n    digits = 2,\n    format = \"html\"\n  ) |&gt;\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\n    full_width = FALSE,\n    font_size = 13\n  )\n\n\n\nFlag thresholds for Value Multiple (Assessed Value ÷ Sale Price)\n\n\nQ1\nQ3\nIQR\nLower Bound\nUpper Bound\n\n\n\n\n0.87\n1.16\n0.29\n0.44\n1.59\n\n\n\n\n\n# Intuition: psf outliers are likely non-market transactions\npsf_range &lt;- select_data |&gt; \n  summarize(\n    q1    = quantile(psf, 0.25, na.rm = TRUE),\n    q3    = quantile(psf, 0.75, na.rm = TRUE),\n    iqr   = q3 - q1,\n    lower = q1 - 1.5 * iqr,\n    upper = q3 + 1.5 * iqr\n  )\n\n# Kable table for psf_range\npsf_range |&gt;\n  mutate(\n    q1    = round(q1, 2),\n    q3    = round(q3, 2),\n    iqr   = round(iqr, 2),\n    lower = round(lower, 2),\n    upper = round(upper, 2)\n  ) |&gt;\n  kable(\n    caption = \"Flag thresholds for psf (Price per Square Foot)\",\n    col.names = c(\"Q1\", \"Q3\", \"IQR\", \"Lower Bound\", \"Upper Bound\"),\n    align = \"c\",\n    digits = 2,\n    format = \"html\"\n  ) |&gt;\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\n    full_width = FALSE,\n    font_size = 13\n  )\n\n\n\nFlag thresholds for psf (Price per Square Foot)\n\n\nQ1\nQ3\nIQR\nLower Bound\nUpper Bound\n\n\n\n\n131.37\n266.19\n134.82\n-70.86\n468.43\n\n\n\n\n\n# Clean data used for modeling / mapping\nclean_data &lt;- select_data |&gt; \n  filter(\n    liv_area &gt; 500,\n    area &gt; 500,\n    between(psf, 40, 800),\n    between(value_multiple, 0.4, 2.0)\n  ) |&gt; \n  mutate(across(where(is.numeric), ~ round(.x, 2))) |&gt; \n  tidyr::drop_na() |&gt; \n  arrange(liv_area)\n\n# Nicely show the first ~10 rows\nclean_data |&gt; \n  slice_head(n = 10) |&gt;\n  kable(\n    caption = \"Cleaned transaction data (first 10 rows after filters)\",\n    align = \"c\",\n    format = \"html\"\n  ) |&gt;\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\n    full_width = FALSE,\n    font_size = 12\n  )\n\n\n\nCleaned transaction data (first 10 rows after filters)\n\n\nyear_built\nbaths\nbeds\nbasements\nfireplaces\ngarage\nconstruction\ninterior\nexterior\ntract\nneigh\nvalue_multiple\nmkt_value\nsale_price\narea\nliv_area\npsf\nviolent_2blocks\npetty_1block\ndist_park_mi\nvacancy_rate\nmed_gross_rent\nmed_hh_income\npct_ba_plus\npct_white\npct_black\npct_hispanic\npct_asian\nfoodretail_1mi_count\ndist_school_public_ft\ndist_school_charter_ft\npct_age_25_44\npct_age_65plus\npoverty_rate\nunemployment_rate\nage\n\n\n\n\n1920\n1\n1\n4\n0\n0\nA\n2\n3\n180\nRICHMOND\n0.86\n143100\n166000\n583\n550\n301.82\n42\n27\n0.09\n8.86\n1248\n70579\n31.33\n64.16\n1.53\n10.90\n0.95\n43\n572.74\n1384.62\n28.84\n12.59\n4.68\n2.28\n104\n\n\n1920\n1\n2\n3\n0\n0\nA\n4\n4\n87\nSPRUCE_HILL\n0.87\n207300\n239000\n563\n550\n434.55\n25\n54\n0.30\n17.57\n1397\n56181\n82.50\n62.07\n9.78\n6.36\n15.56\n59\n326.22\n2454.58\n52.72\n3.42\n33.30\n5.30\n104\n\n\n1915\n1\n1\n4\n0\n0\nA\n4\n4\n28\nDICKINSON_NARROWS\n0.74\n203100\n275000\n641\n586\n469.28\n28\n29\n0.04\n7.55\n1605\n71027\n34.27\n48.46\n3.19\n25.51\n22.68\n74\n766.55\n2486.78\n45.60\n9.70\n19.95\n1.65\n109\n\n\n1920\n1\n2\n5\n0\n0\nA\n2\n3\n180\nRICHMOND\n0.62\n131000\n210000\n583\n598\n351.17\n46\n29\n0.10\n8.86\n1248\n70579\n31.33\n64.16\n1.53\n10.90\n0.95\n43\n603.29\n1340.15\n28.84\n12.59\n4.68\n2.28\n104\n\n\n1920\n1\n2\n3\n0\n0\nA\n3\n4\n180\nRICHMOND\n0.77\n158100\n205000\n696\n600\n341.67\n30\n22\n0.17\n6.20\n1015\n59221\n30.75\n84.06\n1.84\n10.06\n0.04\n40\n1004.49\n1931.18\n40.41\n14.77\n6.52\n10.81\n104\n\n\n1920\n1\n2\n1\n0\n0\nA\n2\n3\n180\nRICHMOND\n0.93\n232000\n250000\n696\n600\n416.67\n29\n22\n0.17\n6.20\n1015\n59221\n30.75\n84.06\n1.84\n10.06\n0.04\n41\n1034.63\n1946.98\n40.41\n14.77\n6.52\n10.81\n104\n\n\n1920\n1\n2\n1\n0\n0\nA\n2\n2\n180\nRICHMOND\n0.90\n224200\n250000\n736\n600\n416.67\n55\n15\n0.01\n8.86\n1248\n70579\n31.33\n64.16\n1.53\n10.90\n0.95\n51\n534.02\n732.30\n28.84\n12.59\n4.68\n2.28\n104\n\n\n1920\n1\n2\n2\n0\n0\nA\n2\n3\n27\nDICKINSON_NARROWS\n1.00\n255000\n255000\n536\n606\n420.79\n32\n26\n0.07\n5.22\n1450\n95224\n58.10\n57.38\n9.19\n17.09\n11.84\n63\n1040.10\n3819.59\n63.09\n7.65\n15.49\n6.96\n104\n\n\n1875\n1\n2\n3\n0\n0\nA\n4\n4\n378\nFISHTOWN\n1.02\n181700\n179000\n503\n610\n293.44\n20\n33\n0.13\n7.37\n1401\n63772\n40.02\n78.77\n4.59\n6.79\n5.45\n38\n2689.29\n437.90\n56.77\n14.20\n10.76\n4.49\n149\n\n\n1875\n1\n2\n3\n0\n0\nA\n3\n4\n161\nEAST_KENSINGTON\n0.90\n270400\n300000\n566\n610\n491.80\n20\n36\n0.10\n10.33\n1668\n102324\n58.08\n48.57\n5.18\n22.09\n13.67\n53\n1241.57\n2580.40\n52.89\n5.68\n10.30\n1.68\n149\n\n\n\n\n\n\nInterpretation: On top of trimming the feature set, we also had to sanity-check which sales even look like real, arm’s-length residential transactions. Property data always includes junk: family transfers for 1 USD, sheriff sales, partial interest transfers, renovation shells, or clerical weirdness (like a 2,000 sq ft rowhouse supposedly selling for 9 million USD). If we blindly feed that into a price model, the model learns nonsense. So we added a set of “real estate flags” to identify and drop those bad rows.\nWe use two main diagnostics: value_multiple and psf. value_multiple is the ratio of the city’s assessed market value to the actual recorded sale price. In a normal sale, the assessment may be a little high or a little low, but it shouldn’t be wildly off. So we look at the distribution of that ratio, get the interquartile range (IQR), and compute fence cutoffs (Q1 − 1.5×IQR, Q3 + 1.5×IQR). Extreme cases — where assessed value is, say, 5× the sale price or 0.1× the sale price — usually mean something non-market happened (distress sale, title correction, bundled sale, etc.). Those points don’t reflect what a typical buyer would pay for that one house, so we trim them. We do the same idea with psf (price per square foot of livable area). psf is a super useful “unit price” signal in real estate, but absurdly low psf often means interior is trashed/gutted or it wasn’t a real arm’s-length sale, and absurdly high psf often means the record is actually for a multi-parcel package or a luxury outlier that behaves more like commercial than typical housing. By fencing psf with IQR cutoffs (and then enforcing a practical range like 40–800 USD per sq ft), we keep what looks like real open-market housing trades in Philadelphia and drop pathological edge cases.\nWe also add basic physical filters: we require reasonable built form (livable area and lot area both over 500 sq ft, because “houses” with 120 sq ft recorded living area are usually data entry mistakes or partial condo interests). Then we require that all key numeric predictors exist and aren’t missing, and we round numeric values for cleaner interpretation. The result is clean_data: a modeling dataset that’s been denoised both structurally and financially. This step matters because linear models are very sensitive to extreme outliers — a handful of fake 1 USD transfers or impossible 5,000 USD/sf lofts can dominate the fit, wreck cross-validation metrics, and drown out the more subtle spatial and neighborhood effects we actually care about.\n\n\n\nPart 4: Modeling Building\n\nclean_data$neigh &lt;- as.factor(clean_data$neigh)\nclean_data$interior &lt;- as.factor(clean_data$interior)\nclean_data$exterior &lt;- as.factor(clean_data$exterior)\nclean_data$basements &lt;- as.factor(clean_data$basements)\n\nset.seed(5080) # Set seed for reproducibility\nctrl &lt;- trainControl(method = \"cv\", number = 10) #10-Fold CV\n\n# Model 1: Structural\ncv_m1 &lt;- train(\n  sale_price ~ beds + baths + basements + interior + exterior + liv_area + poly(age, 2),\n  data = clean_data, method = \"lm\", trControl = ctrl\n)\n\n# Model 2: + Spatial\ncv_m2 &lt;- train(\n  sale_price ~ beds + baths + basements + interior + exterior + liv_area + poly(age, 2) +\n  dist_park_mi + foodretail_1mi_count + dist_school_public_ft\n  + vacancy_rate + med_gross_rent + med_hh_income + pct_ba_plus + pct_black + pct_white\n  + poverty_rate + unemployment_rate + pct_age_25_44 + pct_age_65plus,\n  data = clean_data, method = \"lm\", trControl = ctrl\n)\n\n# Model 3: + Neighborhood Fixed Effects\ncv_m3 &lt;- train(\n  sale_price ~ beds + baths + basements + interior + exterior + liv_area + poly(age, 2) +\n  dist_park_mi + foodretail_1mi_count + dist_school_public_ft +\n  vacancy_rate + med_gross_rent + med_hh_income + pct_ba_plus + pct_black + pct_white +\n  poverty_rate + unemployment_rate + pct_age_25_44 + pct_age_65plus +\n  neigh, \n  data = clean_data, method = \"lm\", trControl = ctrl\n)\n\n# Model 4: + Small Neighborhoods + Mkt Value\nclean_data &lt;- clean_data |&gt; \n  add_count(neigh) |&gt; \n  mutate(\n    neigh_cv = if_else(\n      n &lt; 10,                       # If fewer than 10 sales\n      \"Small_Neighborhoods\",        # Group them\n      as.character(neigh)           # Keep original\n    ),\n    neigh_cv = as.factor(neigh_cv)\n  )\n\ncv_m4 &lt;- train(\n  sale_price ~ beds + baths + basements + interior + exterior + liv_area + poly(age, 2) +\n  violent_2blocks + dist_park_mi + foodretail_1mi_count + dist_school_public_ft + \n  vacancy_rate + med_gross_rent + med_hh_income + pct_ba_plus + pct_black + pct_white +\n  poverty_rate + unemployment_rate + pct_age_25_44 + pct_age_65plus + \n  mkt_value +\n  neigh_cv, \n  data = clean_data, method = \"lm\", trControl = ctrl\n)\n\n# Model 5: + Simpler + Mkt Value\ncv_m5 &lt;- train(\n  sale_price ~ beds + baths + basements + interior + exterior + liv_area + poly(age, 2) +\n  dist_park_mi + foodretail_1mi_count + dist_school_public_ft +\n  mkt_value +\n  neigh_cv,  \n  data = clean_data, method = \"lm\", trControl = ctrl\n)\n\n# Model 6: Simpler Only\ncv_m6 &lt;- train(\n  sale_price ~ beds + baths + basements + interior + exterior + liv_area + poly(age, 2) +\n  dist_park_mi + foodretail_1mi_count + dist_school_public_ft + \n  neigh_cv,  \n  data = clean_data, method = \"lm\", trControl = ctrl\n)\n\n# Compare\nmodel_compare &lt;- data.frame(\n  Model = c(\n    \"Structural\",\n    \"Spatial\",\n    \"Fixed Effects\",\n    \"Small Neighborhoods + Mkt Value\",\n    \"Simpler + Mkt Value\",\n    \"Simpler Only\"\n  ),\n  Rsquared = c(\n    cv_m1$results$Rsquared,\n    cv_m2$results$Rsquared,\n    cv_m3$results$Rsquared,\n    cv_m4$results$Rsquared,\n    cv_m5$results$Rsquared,\n    cv_m6$results$Rsquared\n  ),\n  RMSE = c(\n    cv_m1$results$RMSE,\n    cv_m2$results$RMSE,\n    cv_m3$results$RMSE,\n    cv_m4$results$RMSE,\n    cv_m5$results$RMSE,\n    cv_m6$results$RMSE\n  ),\n  MAE = c(\n    cv_m1$results$MAE,\n    cv_m2$results$MAE,\n    cv_m3$results$MAE,\n    cv_m4$results$MAE,\n    cv_m5$results$MAE,\n    cv_m6$results$MAE\n  )\n) |&gt;\n  mutate(\n    R2 = round(Rsquared, 3),\n    RMSE = round(RMSE, 0),\n    MAE = round(MAE, 0)\n  ) |&gt;\n  select(\n    Model,\n    `R² (CV)` = R2,\n    `RMSE (CV)` = RMSE,\n    `MAE (CV)` = MAE\n  )\n\nmodel_compare |&gt;\n  kable(\n    caption = \"10-fold CV performance across model specifications\",\n    align = c(\"l\", \"c\", \"c\", \"c\"),\n    format = \"html\"\n  ) |&gt;\n  kable_styling(\n    bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"),\n    full_width = FALSE,\n    font_size = 13\n  ) |&gt;\n  row_spec(1, bold = TRUE) |&gt;\n  column_spec(1, width = \"8cm\") |&gt;\n  add_header_above(\n    c(\" \" = 1, \"Out-of-Sample Fit Metrics\" = 3)\n  )\n\n\n\n10-fold CV performance across model specifications\n\n\n\n\n\n\n\n\n\n\nOut-of-Sample Fit Metrics\n\n\n\nModel\nR² (CV)\nRMSE (CV)\nMAE (CV)\n\n\n\n\nStructural\n0.669\n136251\n89872\n\n\nSpatial\n0.793\n108267\n66237\n\n\nFixed Effects\n0.844\n93491\n55873\n\n\nSmall Neighborhoods + Mkt Value\n0.925\n64888\n42255\n\n\nSimpler + Mkt Value\n0.926\n64899\n42304\n\n\nSimpler Only\n0.838\n95561\n57665\n\n\n\n\n\n\nDiscussion: We trained a sequence of models using 10-fold cross-validation to see how different groups of predictors affect out-of-sample accuracy. The first model (“Structural”) uses only property-level physical characteristics: bedrooms, bathrooms, finished basement indicators, interior/exterior condition, living area, and building age. This baseline already explains about 67% of the variation in sale price (CV R² ≈ 0.67), but it’s still pretty rough: the average out-of-sample error is large, with an RMSE of about $136K and an average absolute error (MAE) of about 90K USD. In plain English, even if you know what the house is and what shape it’s in, you’re still often off by six figures. That’s a sign that location — broadly defined — is doing a lot of work in Philadelphia’s housing market.\nWhen we layer in block-scale spatial features and neighborhood socioeconomic context (“Spatial”), accuracy improves a lot. The R² jumps from ~0.67 to ~0.79, and RMSE drops from ~136K to ~108K. This version adds things like violent incidents within two blocks, distance to parks and schools, local retail access, and census tract indicators such as income, rent, vacancy, and demographics. That improvement tells us two things: (1) buyers absolutely price the surrounding environment — safety, access, social context — not just the building; and (2) those effects are strong enough to generalize out of sample. We’re not just overfitting noise; spatial context is genuinely predictive of what a house will sell for.\nBut “Spatial” is still treating the city as one big market. The third model (“Fixed Effects”) adds neighborhood fixed effects — basically, a separate intercept for each neighborhood — which soaks up persistent price differences between places like Fishtown, Point Breeze, Chestnut Hill, etc. That pushes R² to ~0.85 and drops RMSE to about 93K. This is a big deal. It means two homes with the same size, same condition, and same local crime exposure will still trade at systematically different price levels depending on which named neighborhood they’re in. In other words: neighborhood identity itself, not just measured amenities, is priced.\nFrom there, we refine how we treat neighborhood. The fourth model (“Small Neighborhoods + Mkt Value”) does two important things: (1) it collapses very small neighborhoods (with &lt;10 observed sales) into a pooled “Small_Neighborhoods” bin so that those areas don’t produce unstable fixed effects, and (2) it adds the city’s assessed market value (mkt_value) as a predictor. This changes everything. Performance jumps massively: R² rises to ~0.92 and RMSE falls to about 65K, cutting typical error almost in half compared to the baseline structural model. MAE also drops to about 42K. This model is both high-performing and still interpretable: it blends physical house attributes, micro-spatial exposure (crime, distance to parks, etc.), socioeconomic context, and a stabilized neighborhood effect, plus an institutional pricing signal (assessed market value). That combination is our best-performing “full” model.\nThen we test whether we can simplify without losing accuracy. The fifth model (“Simpler + Mkt Value”) strips out some of the weaker spatial/ACS controls but keeps assessed value and the smoothed neighborhood factor. Its performance is basically identical to the full version: R² ≈ 0.925, RMSE ≈ 65K, MAE ≈ 42K. That tells us there’s some redundancy — once you include assessed market value and a good neighborhood structure, you don’t necessarily need every single contextual variable to hit high predictive accuracy.\nFinally, we try “Simpler Only,” which keeps neighborhood effects but drops assessed value. Performance falls back: R² slips to ~0.84 and RMSE rises to ~95K, more like the plain fixed-effects model. That last comparison is super informative: it shows that the assessed market value term is doing a ton of predictive work. It acts like a summary statistic for block-level desirability, renovation quality, curb appeal, and whatever else the city’s assessment office is “seeing.” In practice, the best cross-validated model is the version that includes (i) structural features of the home, (ii) a stabilized neighborhood factor, and (iii) assessed market value. That model is accurate, relatively compact, and realistic to deploy for valuation or forecasting.\n\nfe_m4 &lt;- feols(\n  sale_price ~ beds + baths + basements + interior + exterior + liv_area + poly(age, 2) +\n    violent_2blocks + dist_park_mi + foodretail_1mi_count + dist_school_public_ft + \n    vacancy_rate + med_gross_rent + med_hh_income + pct_ba_plus + pct_black + pct_white +\n    poverty_rate + unemployment_rate + pct_age_25_44 + pct_age_65plus + \n    mkt_value |                    # left of | = regular covariates\n    neigh_cv,                      # right of | = fixed effect(s) to absorb\n  data = clean_data\n)\n\n# Pretty table (no neighborhood FE dummies will show)\n\nmodelsummary(\n  list(\"Fourth Model (FE absorbed + MKT Value)\" = fe_m4),\n  gof_omit  = 'IC|Adj|Log|F',\n  statistic = \"({std.error})\",     \n  estimate  = \"{estimate}{stars}\",\n  stars     = c('*'=.1,'**'=.05,'***'=.01),\n  output    = \"markdown\"\n)\n\n\n\n\n\n\n\n\n\nFourth Model (FE absorbed + MKT Value)\n\n\n\n\nbeds\n-3873.601\n\n\n\n(3075.977)\n\n\nbaths\n13565.646***\n\n\n\n(1558.329)\n\n\nbasements1\n18549.818***\n\n\n\n(6652.229)\n\n\nbasements2\n14322.276**\n\n\n\n(6977.839)\n\n\nbasements3\n2428.907\n\n\n\n(6735.149)\n\n\nbasements4\n-607.741\n\n\n\n(6511.343)\n\n\nbasements5\n17106.796**\n\n\n\n(6752.570)\n\n\nbasements6\n8289.153\n\n\n\n(6832.074)\n\n\nbasements7\n8806.457\n\n\n\n(6851.678)\n\n\nbasements8\n-2633.918\n\n\n\n(6892.256)\n\n\nbasements9\n20055.280\n\n\n\n(22807.785)\n\n\ninterior2\n92999.193***\n\n\n\n(18336.483)\n\n\ninterior3\n85376.543***\n\n\n\n(19174.523)\n\n\ninterior4\n61383.677***\n\n\n\n(18959.641)\n\n\ninterior5\n57123.712***\n\n\n\n(20302.702)\n\n\ninterior6\n62503.514***\n\n\n\n(22971.111)\n\n\ninterior7\n49145.991*\n\n\n\n(28333.278)\n\n\nexterior2\n-66717.967***\n\n\n\n(17368.339)\n\n\nexterior3\n-58862.313***\n\n\n\n(16423.230)\n\n\nexterior4\n-62025.095***\n\n\n\n(16427.168)\n\n\nexterior5\n-81058.115***\n\n\n\n(16847.816)\n\n\nexterior6\n-64382.533**\n\n\n\n(26436.873)\n\n\nexterior7\n-52815.400*\n\n\n\n(26795.724)\n\n\nliv_area\n36.637***\n\n\n\n(6.821)\n\n\npoly(age, 2)1\n138614.935\n\n\n\n(190811.678)\n\n\npoly(age, 2)2\n106759.188\n\n\n\n(179196.584)\n\n\nviolent_2blocks\n-66.507***\n\n\n\n(22.897)\n\n\ndist_park_mi\n-2828.573\n\n\n\n(6599.210)\n\n\nfoodretail_1mi_count\n-7.701\n\n\n\n(106.478)\n\n\ndist_school_public_ft\n-0.044\n\n\n\n(1.030)\n\n\nvacancy_rate\n56.772\n\n\n\n(179.426)\n\n\nmed_gross_rent\n-2.047\n\n\n\n(6.010)\n\n\nmed_hh_income\n0.110\n\n\n\n(0.106)\n\n\npct_ba_plus\n-30.859\n\n\n\n(155.880)\n\n\npct_black\n-120.592\n\n\n\n(90.127)\n\n\npct_white\n-48.847\n\n\n\n(120.549)\n\n\npoverty_rate\n125.862\n\n\n\n(132.408)\n\n\nunemployment_rate\n-186.127\n\n\n\n(147.202)\n\n\npct_age_25_44\n22.016\n\n\n\n(225.754)\n\n\npct_age_65plus\n420.441*\n\n\n\n(232.392)\n\n\nmkt_value\n0.853***\n\n\n\n(0.036)\n\n\nNum.Obs.\n17181\n\n\nR2\n0.927\n\n\nR2 Within\n0.824\n\n\nRMSE\n64002.47\n\n\nStd.Errors\nby: neigh_cv\n\n\n\n\nlm_model4 &lt;- cv_m4$finalModel\n\nCoefficient Interpretation:\nThis model is estimated with neighborhood fixed effects (via neigh_cv), which means we’re not comparing a 1M USD rowhome in Center City to a 90K USD shell in Kensington. We’re comparing houses within the same local market bucket. So every coefficient should be read as: “holding the neighborhood constant, and holding everything else constant, how does this feature change sale price?” That matters because Philadelphia has a lot of lower- and middle-income housing stock with smaller square footage, aging structures, and incremental rehab. In that environment, buyers trade off things like layout, livable condition, block safety, and perceived stability more than just raw square footage. The fixed effects soak up the broad neighborhood premium/discount (schools, reputation, amenities, hype), so what’s left is what explains why two houses on similar turf sell for different prices.\nOn the physical side of the house, three patterns dominate. First, interior and exterior condition are extremely valuable. Compared to houses in visibly poor shape, houses in better interior condition sell for about 60K–90K USD more, and houses in worse exterior condition lose about 50K–80K USD in value, even after controlling for size, beds, and baths. That tells you what really clears in Philly’s for-sale market: buyers are paying for “move-in ready and looks solid from the street,” not just square footage. Bathrooms also carry a strong premium — roughly 13K USD per additional bath — which fits a story about livability and privacy. Bedrooms are trickier. Once you control for total living area, an extra bedroom actually correlates with slightly lower sale price. The way to read that is not “buyers hate bedrooms,” but “for two houses of the same size, the one chopped into more/smaller bedrooms can actually feel tighter and cheaper.” In Philadelphia’s rowhome housing stock — especially entry-level rehab product — that’s common: developers carve an extra bedroom into the same envelope, but buyers still value openness. Living area itself is priced at about 37 USD per additional square foot, which is a clean, intuitive slope: more usable space is still worth more money, but not all square footage is created equal if the layout feels cramped.\nWe also see localized social and environmental factors priced in, even within the same neighborhood. More violent incidents within roughly two blocks (about 600 feet) are associated with lower sale prices, on the order of tens of USD per additional incident. That might sound small per incident, but block-to-block differences add up, and the key point is this survives neighborhood fixed effects. In other words, even inside a neighborhood that’s broadly considered “high crime,” buyers still distinguish between a quieter block and a hotter corner. Meanwhile, access-style amenities like distance to parks, distance to schools, and nearby food retailers don’t come through as statistically meaningful once you control for neighborhood. That suggests those amenities are already baked into which neighborhood you’re in; they’re not the reason one house sells higher than the nearly identical house down the street. Said differently: once you’re in that neighborhood market, hyperlocal safety matters more than marginal proximity to a park.\nFinally, the model captures social context at the tract level — income, race, age structure — and those patterns are uncomfortable but real. Houses in tracts with higher median household income tend to sell for more, even within the same neighborhood bucket, which says that micro-areas of relative affluence inside a neighborhood command a premium. The share of Black residents in the tract is associated with lower sale prices, holding physical quality, crime, and everything else constant. That is not a story about “house quality”; that’s a story about how the market (buyers, lenders, appraisers) continues to discount majority-Black areas. This is consistent with persistent racialized valuation in U.S. housing markets. We also see a positive association with the share of seniors (65+): blocks with more older residents tend to have slightly higher sale prices, which likely proxies for stability — long-term owner occupancy, less churn, and “quiet” blocks. These demographic effects are all still happening inside neighborhoods, which means buyers are sensitive to the social micro-geography of a block or tract, not just its ZIP code.\nOne last anchor in the model is assessed market value. The city’s assessed value is extremely predictive of the actual sale price, at about 0.85 USD on the dollar. That does two things. First, it soaks up a lot of residual variation in quality — finishes, block vibe, unseen mechanical upgrades — that isn’t fully captured by our other variables. Second, it explains why the model fit is so strong (R² ≈ 0.93 overall, ~0.82 within neighborhoods): once you hold neighborhood fixed effects and you include what the city thinks the house is worth, you’ve basically captured both the structural story and the local market story. So the way to interpret this model is: inside a given Philadelphia neighborhood market, buyers pay a lot for finish and condition, they reward livable layouts and bathrooms, they discount safety risk on your immediate block, and they still respond to coded neighborhood status signals — income, racial composition, and perceived block stability — even after we’ve neutralized broad “which neighborhood are you in?” differences.\n\n\n\nPart 5: Model Diagnostics\nDiagnostic 1: Residual Plot\n\n# Residual Plot\nclean_data$residuals &lt;- residuals(lm_model4)\nclean_data$fitted &lt;- fitted(lm_model4)\n\nggplot(clean_data, aes(x = fitted, y = residuals)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"red\", linetype = \"dashed\") +\n  labs(title = \"Residual Plot\", x = \"Fitted Values\", y = \"Residuals\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nInterpretation: random scatter in the residual plot indicates that the model is not missing anything systematic (i.e. biased predictions in predictable ways)\n\n# Breusch-Pagan Test for Heteroskedacity\nlibrary(lmtest)\nbptest(lm_model4)\n\n\n    studentized Breusch-Pagan test\n\ndata:  lm_model4\nBP = 2755.5, df = 173, p-value &lt; 2.2e-16\n\n\nInterpretation: while the small p-value is evidence of heteroskedacity, we have already added a large number of hedonic variables + spatial variables + neighborhood fixed effects. Hence, we acknowledge heteroskedacity as a limitation of our model because we are focused on point prediction rather than inference or confidence intervals.\nDiagnostic 2: Q-Q Plot\n\n# Plot Q-Q Plot to test Normality of Residuals\nq &lt;- ggplot(clean_data, aes(sample = residuals)) +\n  stat_qq() +\n  stat_qq_line(color = \"red\") +\n  labs(title = \"Q-Q Plot of Residuals in House Price Prediction\",\n       x = \"Theoretical Quantiles\",\n       y = \"Sample Quantiles\") +\n  theme_minimal()\n\nprint(q)\n\n\n\n\n\n\n\n\nInterpretation: the Q-Q plot above shows that points generally fall close to the diagonal line. However, the ends (both left and right tails) curve sharply away from the diagonal line. This suggests that residuals have heavy tails — i.e., there are more extreme outliers than what would be expected under a normal distribution, which makes sense for home prices given that luxury home sales could skesw the model.\nDiagnostic 3: Cook’s Distance\n\n# Add diagnostic measures to tibble\ndiagnostic_data &lt;- augment(lm_model4, data = clean_data) |&gt; \n  mutate(\n    cooks_d = .cooksd,\n    leverage = .hat,\n    is_influential = cooks_d &gt; 4/nrow(clean_data)\n  )\n\n# Plot Cook's distance\nggplot(diagnostic_data, aes(x = 1:nrow(diagnostic_data), y = cooks_d)) +\n  geom_point(aes(color = is_influential), size = 2) +\n  geom_hline(yintercept = 4/nrow(diagnostic_data), \n             linetype = \"dashed\", color = \"red\") +\n  scale_color_manual(values = c(\"grey60\", \"red\")) +\n  labs(title = \"Cook's Distance\",\n       x = \"Observation\", y = \"Cook's D\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n# Rule of thumb: Cook's D &gt; 4/n\nthreshold &lt;- 4/nrow(clean_data)\n\ninfluential &lt;- diagnostic_data |&gt;\n  filter(cooks_d &gt; threshold)  |&gt;\n  select(age, baths, beds, mkt_value, liv_area, sale_price, neigh, cooks_d) |&gt;\n  arrange(desc(cooks_d))\n\nhead(influential, 10)\n\n# A tibble: 10 × 8\n     age baths  beds mkt_value liv_area sale_price neigh            cooks_d\n   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt; &lt;fct&gt;              &lt;dbl&gt;\n 1    95     5     5   4952500    14150    6325000 CHESTNUT_HILL     0.282 \n 2    22     5     5   1202700     8055    2650000 UPPER_ROXBOROUGH  0.0600\n 3   109     3     5   1135400     3495    1535000 GARDEN_COURT      0.0258\n 4   224     3     4    639400     2050    1310000 FITLER_SQUARE     0.0233\n 5   224     4     5   1600000     4096    2455650 RITTENHOUSE       0.0167\n 6     6     4     5   2620600     4748    3125000 RITTENHOUSE       0.0159\n 7    74     3     5    910100     3264    1850000 EAST_FALLS        0.0151\n 8    54     2     3   1022100     2400    1825000 SOCIETY_HILL      0.0143\n 9    54     4     4   2033600     4000    1350000 SOCIETY_HILL      0.0133\n10    99     4     6   1251000     5340     850000 MOUNT_AIRY_EAST   0.0124\n\n\nInterpretation: Overall, the vast majority of observations have very low Cook’s D values and are clustered close to 0. However, there a small number of outliers that lie above the horizontal line (i.e. the rule-of-thumb threshold). As shown in the table above, these outliers are legitimate sale transactions because they have large livable areas and are located in premium neighborhoods such as Rittenhouse, Old City, and Chestnut Hill. It is still important to note that these outliers are influential and can potentially skew coefficients.\n\n\n\nPart 6: Conclusions & Recommendations\nWhat is your final model’s accuracy?\nWe tested four models and watched how prediction quality improved as we layered in more information.\n\nStructural only (beds, baths, square footage, age, condition): RMSE ≈ 136,000 USD.\n\nSpatial context (parks, schools, food access, neighborhood socioeconomic stats, crime): RMSE ≈ 108,000 USD.\n\n\nNeighborhood fixed effects (absorbing neighborhood differences directly): RMSE ≈ 93,000 USD.\n\n\nCollapsing small neighborhoods + adding assessed market value: RMSE ≈ 65,000 USD.\n\n\nSo the final model — the “Small Neighborhoods + Mkt Value” spec — cuts average error almost in half compared to the basic structural model (136K → 65K USD). That’s a huge jump. This tells us two things. First, house-level physical traits are not enough to explain price in Philly. Second, you get massive gains in accuracy from (a) knowing where in the city the house is, down to the neighborhood and even the block, and (b) using the city’s own market assessment as a kind of summary of unobserved quality. The final model is accurate enough to be useful for price benchmarking and valuation discussions, not just academic pattern-finding.\nAlso look at R²: it climbs from about 0.67 in the structural model to ~0.92 in the final model. That means we’re capturing ~92% of the variation in sale prices with publicly available data plus neighborhood controls.\nWhich features matter most for Philadelphia prices?\n\nPhysical livability: Interior condition has a massive effect: “better interior” categories add on the order of tens of thousands of USD relative to poor interior condition. Exterior condition works the same direction but in reverse: worse exterior knocks off large chunks of value (often 50K–80K USD less). Bathrooms are also very valuable — adding one more bathroom is associated with roughly +13K USD, holding other things constant. Living area matters too: each additional square foot of livable space sells for ~37 USD.\nLayout quality, not just bedroom count: More bedrooms is not automatically “more expensive.” After controlling for total livable area, additional bedrooms are actually associated with lower prices. The story there: if two houses are the same size, the one chopped into more (smaller) bedrooms is viewed as less desirable. Buyers want usable, open-feeling space, not just “4BR” on paper. Basements in usable form, on the other hand, usually add value, and in some categories that bump is &gt;10K USD.\nImmediate block conditions (social environment): Violent incidents within roughly two blocks (≈600 ft) are priced in. Every additional violent incident nearby lowers the sale price. That’s not just “bad neighborhood vs good neighborhood,” because we absorb neighborhood fixed effects — this is within-neighborhood, block-level safety perception. So buyers are discounting homes on hotter corners, even if they’re still in an otherwise desirable area.\nNeighborhood market position / wealth signals: Median household income in the tract is positively associated with higher sale prices, and assessed market value from the city is extremely predictive (roughly 0.85 USD of sale price per 1 USD of assessed value). That basically means: if the assessor thinks you’re a high-value property, the market mostly agrees. A decent chunk of “finish quality,” “renovation level,” and “this is a block that’s turning” is being captured through that single variable.\n\nWhich neighborhoods are hardest to predict?\n\nneigh_url &lt;- \"https://raw.githubusercontent.com/opendataphilly/open-geo-data/refs/heads/master/philadelphia-neighborhoods/philadelphia-neighborhoods.geojson\"\nneigh &lt;- sf::read_sf(neigh_url)\n\n# make sure neigh has a character neighborhood name column we can join on\n# most Philly neighborhood layers use NAME\nif (!\"NAME\" %in% names(neigh)) {\n  stop(\"Neighborhood layer is missing NAME column; inspect `names(neigh)`.\")\n}\n\n############################################################\n# 1. Add per-model predictions & residuals to clean_data\n############################################################\n\nclean_data &lt;- clean_data %&gt;%\n  mutate(\n    pred_m1  = predict(cv_m1, newdata = clean_data),\n    resid_m1 = sale_price - pred_m1,\n    pred_m2  = predict(cv_m2, newdata = clean_data),\n    resid_m2 = sale_price - pred_m2,\n    pred_m4  = predict(cv_m4, newdata = clean_data),\n    resid_m4 = sale_price - pred_m4,\n    pred_m5  = predict(cv_m5, newdata = clean_data),\n    resid_m5 = sale_price - pred_m5,\n    pred_m6  = predict(cv_m6, newdata = clean_data),\n    resid_m6 = sale_price - pred_m6\n  )\n\n# standardize neighborhood name text in the sales rows\nclean_data_std &lt;- clean_data %&gt;%\n  mutate(\n    neigh = neigh %&gt;%\n      as.character() %&gt;%\n      str_trim()\n  )\n\n############################################################\n# 2. Collapse residuals to neighborhood-level stats\n############################################################\n\nneigh_resid_long &lt;- clean_data_std %&gt;%\n  select(neigh, starts_with(\"resid_\")) %&gt;%\n  pivot_longer(\n    cols = starts_with(\"resid_\"),\n    names_to = \"model\",\n    values_to = \"resid\"\n  ) %&gt;%\n  mutate(\n    # keep only the models we care about\n    model = factor(\n      model,\n      levels = c(\"resid_m1\", \"resid_m2\", \"resid_m4\", \"resid_m5\", \"resid_m6\"),\n      labels = c(\"Model 1\", \"Model 2\", \"Model 4\", \"Model 5\", \"Model 6\")\n    )\n  ) %&gt;%\n  filter(!is.na(model)) %&gt;%  # drop any others\n  group_by(neigh, model) %&gt;%\n  summarise(\n    n_sales    = n(),\n    mean_res   = mean(resid, na.rm = TRUE),                     # avg under/over\n    median_res = median(resid, na.rm = TRUE),\n    rmse_res   = sqrt(mean(resid^2, na.rm = TRUE)),\n    .groups    = \"drop\"\n  )\n\n############################################################\n# 3. Join neighborhood stats back to neighborhood polygons\n############################################################\n\nneigh_map_all &lt;- neigh %&gt;%\n  mutate(\n    neigh = NAME %&gt;%\n      as.character() %&gt;%\n      str_trim()\n  ) %&gt;%\n  left_join(neigh_resid_long, by = \"neigh\")\n\n# neigh_map_all is now sf with columns:\n# neigh, model, mean_res, etc.\n\n############################################################\n# 4. Helper: map residuals for ONE model\n#    - dynamic symmetric color scale per model\n#    - simple title (Model 1, etc), no subtitle in panels\n############################################################\n\nplot_neigh_residual_map &lt;- function(model_label, na_col = \"grey90\") {\n\n  df &lt;- neigh_map_all %&gt;% filter(model == model_label)\n\n  # symmetric color limit per model\n  lim_sym &lt;- max(abs(df$mean_res), na.rm = TRUE)\n  if (!is.finite(lim_sym) || lim_sym &lt;= 0) {\n    lim_sym &lt;- NA_real_\n  }\n\n  ggplot(df) +\n    geom_sf(\n      aes(fill = mean_res),\n      color = \"grey40\",\n      linewidth = 0.2\n    ) +\n    scale_fill_gradient2(\n      name      = \"Mean residual (USD)\",\n      low       = \"#a4133c\",\n      mid       = \"white\",\n      high      = \"#1a759f\",\n      midpoint  = 0,\n      limits    = if (is.finite(lim_sym)) c(-lim_sym, lim_sym) else waiver(),\n      oob       = squish,\n      na.value  = na_col,\n      labels    = dollar\n    ) +\n    labs(\n      title = model_label,\n      x = NULL,\n      y = NULL\n    ) +\n    theme_minimal(base_size = 11) +\n    theme(\n      panel.grid.major = element_blank(),\n      panel.grid.minor = element_blank(),\n      plot.title       = element_text(face = \"bold\", size = 12, hjust = 0),\n      axis.text        = element_blank(),\n      axis.ticks       = element_blank(),\n      legend.position  = \"right\"\n    )\n}\n\n############################################################\n# 5. Build panels for each model (dropping Model 3)\n############################################################\n\np_model1 &lt;- plot_neigh_residual_map(\"Model 1\")\np_model2 &lt;- plot_neigh_residual_map(\"Model 2\")\np_model4 &lt;- plot_neigh_residual_map(\"Model 4\")\np_model5 &lt;- plot_neigh_residual_map(\"Model 5\")\np_model6 &lt;- plot_neigh_residual_map(\"Model 6\")\n\nblank_panel &lt;- ggplot() + theme_void()\n\n############################################################\n# 6. Assemble patchwork figure (2x3 grid layout)\n############################################################\n\nresid_maps_all &lt;-\n  (p_model1 | p_model2 | p_model4) /\n  (p_model5 | p_model6 | blank_panel) +\n  plot_annotation(\n    title = \"Which Neighborhoods Are Hardest to Predict?\",\n    subtitle = paste(\n      \"Mean model residual (USD) by neighborhood.\",\n      \"Blue = model underpredicts (homes sold for MORE than predicted).\",\n      \"Red = model overpredicts (homes sold for LESS).\",\n      sep = \"\\n\"\n    ),\n    theme = theme(\n      plot.title    = element_text(face = \"bold\", size = 16, hjust = 0),\n      plot.subtitle = element_text(size = 11, hjust = 0)\n    )\n  ) &\n  theme(\n    legend.key.height = unit(0.4, \"in\"),\n    legend.key.width  = unit(0.15, \"in\")\n  )\n\n############################################################\n# 7. Print final figure for the slide\n############################################################\n\nprint(resid_maps_all)\n\n\n\n\n\n\n\n\nEquity concerns?\nYes, and they show up in the coefficients. Even after controlling for interior quality, exterior condition, square footage, bathroom count, crime on the block, and neighborhood fixed effects, properties in areas with a higher share of Black residents tend to sell for less. That is not a physical-quality story — we already held physical quality constant. It’s also not just “this is a cheaper neighborhood,” because neighborhood fixed effects are in there. It’s evidence of structural discounting: the same basic housing form is being valued lower in Blacker areas.\nWe also see that tracts with higher median household income sell for more, and tracts with more older residents (65+) sell for more. You can read that as the market attaching a price premium to “social stability” or “aging, long-tenure homeowners with resources,” and a penalty to neighborhoods racialized as lower-value. That has obvious fair housing implications. If you used a model like this to “objectively” value collateral for lending or tax assessment without adjustments, you’d be baking those disparities directly into policy. In other words, the market is not just pricing bricks and bathrooms; it’s pricing neighborhood identity — including race and class — and we can see it in the data.\nLimitations?\n\nWe model sale price, not willingness to pay in general. Off-market transfers, distressed sales, family transfers, or cash flips at weird prices all exist and can leak into the data. We tried to filter out some non-arm’s-length / broken records using safeguards like price-per-square-foot ranges and value_multiple cutoffs, but it’s not perfect.\nWe don’t observe interiors the way an appraiser or buyer does. “Interior condition” is categorical and crude. It does a lot of work in the model, but it’s still way less detailed than listing photos, finishes, HVAC age, roof age, etc. So some of what shows up in the assessed market value term is just “good renovation quality we didn’t explicitly encode.”\nTiming and direction of causality. Crime is measured during 2023–2024, and we’re relating that to sales in roughly that same window. Buyers react to reputation and perceived safety, not just the literal last 12 months of incidents. So the coefficient on violent_2blocks is “areas that are known to be hotter tend to sell for less,” not “if one assault happens on your block today, you instantly lose X USD tomorrow.”\nDemographic variables are structurally loaded. The fact that pct_black is negatively associated with price is not a statement about intrinsic property value. It’s capturing systemic bias — lending, appraisal, speculative expectations — and we’re treating that as a “feature.” That’s useful for prediction, but dangerous for policy if used naively. You would not want to bake that into automated valuation tools without guardrails, because it would literally encode discrimination.\nFixed effects hide macro gap but don’t solve it. Neighborhood fixed effects let us ask “why does this house sell for more than that house within the same neighborhood?” That’s exactly what we want for local pricing. But fixed effects also absorb neighborhood-level disparities themselves. That means we’re not modeling why one neighborhood as a whole appraises lower than another neighborhood, which is also an equity question — we’re holding that gap constant.\n\nDeclaration ChatGPT was used to amend language errors and allow for more seamless sentence structure/syntax. It was also used to aid in certain repetitive coding tasks. All of which were double checked for accuracy and at times redone by another team member to ensure ownership of both content and code."
  },
  {
    "objectID": "Publications.html",
    "href": "Publications.html",
    "title": "Mohamad Al Abbas - MUSA 5080 Portfolio",
    "section": "",
    "text": "Born in a Haze: Early-life exposure to forest fires and child health in Indonesia\n\nExamines impacts of forest fire–related haze on children’s HAZ/WAZ growth indicators using IFLS data.\n\nDevelops clustering algorithms and k-means quintile regression for fire exposure.\n\nCrude Consequences: The Impact of Oil Spills on Child Health and Environmental Vulnerability in Nigeria\n\nIntegrates MICS data with daily oil spill trackers.\n\nExplores stratified vulnerability and child health outcomes.\n\nClimate–Health across Biomes: Stratified impacts of precipitation and temperature on children’s health\n\nMulti-country analysis of diarrhea, fever, cough, and WHZ outcomes.\n\nExamines biome interactions with climate anomalies.\n\nCross-Border Nutritional Vulnerability in Shared Ecologies (India, Bangladesh, Nepal)\n\nDHS-based spatial analysis of child undernutrition near borders.\n\nTests whether ecological context outweighs political boundaries.\n\nCRAM (Climate-Resilient Access Metric): Healthcare isolation in vulnerable regions\n\nDeveloping an index to measure resilience of healthcare access to climate shocks.\n\nFocus on flood seasonality and maternal health risks.\n\nLong-Term PM2.5 Exposure and Cardiovascular Disease in Indonesia\n\nCohort-based study using IFLS (2000–2014).\n\nModels morbidity-adjusted CVD outcomes under cumulative pollution exposure.\n\nDisaster–Health Mediation: Natural disasters as instruments for water contamination\n\nUses IV regression with MICS surveys to assess diarrheal outcomes.\n\nExplores mediation via healthcare-seeking behaviors and infrastructure."
  },
  {
    "objectID": "Publications.html#ongoing-research-projects",
    "href": "Publications.html#ongoing-research-projects",
    "title": "Mohamad Al Abbas - MUSA 5080 Portfolio",
    "section": "",
    "text": "Born in a Haze: Early-life exposure to forest fires and child health in Indonesia\n\nExamines impacts of forest fire–related haze on children’s HAZ/WAZ growth indicators using IFLS data.\n\nDevelops clustering algorithms and k-means quintile regression for fire exposure.\n\nCrude Consequences: The Impact of Oil Spills on Child Health and Environmental Vulnerability in Nigeria\n\nIntegrates MICS data with daily oil spill trackers.\n\nExplores stratified vulnerability and child health outcomes.\n\nClimate–Health across Biomes: Stratified impacts of precipitation and temperature on children’s health\n\nMulti-country analysis of diarrhea, fever, cough, and WHZ outcomes.\n\nExamines biome interactions with climate anomalies.\n\nCross-Border Nutritional Vulnerability in Shared Ecologies (India, Bangladesh, Nepal)\n\nDHS-based spatial analysis of child undernutrition near borders.\n\nTests whether ecological context outweighs political boundaries.\n\nCRAM (Climate-Resilient Access Metric): Healthcare isolation in vulnerable regions\n\nDeveloping an index to measure resilience of healthcare access to climate shocks.\n\nFocus on flood seasonality and maternal health risks.\n\nLong-Term PM2.5 Exposure and Cardiovascular Disease in Indonesia\n\nCohort-based study using IFLS (2000–2014).\n\nModels morbidity-adjusted CVD outcomes under cumulative pollution exposure.\n\nDisaster–Health Mediation: Natural disasters as instruments for water contamination\n\nUses IV regression with MICS surveys to assess diarrheal outcomes.\n\nExplores mediation via healthcare-seeking behaviors and infrastructure."
  },
  {
    "objectID": "Publications.html#in-progress-methodological-work",
    "href": "Publications.html#in-progress-methodological-work",
    "title": "Mohamad Al Abbas - MUSA 5080 Portfolio",
    "section": "In Progress Methodological Work",
    "text": "In Progress Methodological Work\n\nMultistate Life Table Models\n\nTransition modeling between Healthy–Unhealthy–Dead states in IFLS morbidity data.\n\nReplication of Payne et al. (2013) with microsimulation extensions.\n\nSpatial Climate–Health Pipelines\n\nDynamic extraction of precipitation/temperature anomalies via CHIRPS/NOAA rasters.\n\nIntegrating z-scores and climate windows aligned with DHS/MICS survey interviews."
  },
  {
    "objectID": "Publications.html#publications",
    "href": "Publications.html#publications",
    "title": "Mohamad Al Abbas - MUSA 5080 Portfolio",
    "section": "Publications",
    "text": "Publications\n\nDisparate social impacts by satellite and self-report: Floods in Pernambuco, Brazil — Population & Environment (under review)\n\nRacism, Discrimination, and Scapegoating in the Era of COVID-19 — IMS Policy and Working Paper Series\nEnvironmental Determinants of Migration: Air Pollution in Developing Nations — IMS Blog – Borders and Limitations (2022)\n\nGender Inequity in Engineering Higher Education: A Case Study of an American University in a Middle Eastern Country — ICIET Conference Proceedings (2022)\n\nThe Impact of Faculty Scholarly Collaborations with Non-Doctoral Students — IEEE IRTM Conference (2022)\n\nThe Lebanese Diaspora: From Passive to Retaliatory — IMS Blog – Borders and Limitations (2022)\n\nQuantifying Publisher’s Competence Through Scholarly Engagement — Publishing Research Quarterly (2021)\n\nA Case Study of Gender Diversity in a Middle Eastern University — IEEE IT-Based Higher Education & Training Conference (2021)\n\nSetting the Boundaries of COVID-19 Lockdown Relaxation Measures — Library Hi Tech (2021)\n\nSustainable Gender Equity Influenced by an Americanized Lebanese Pedagogy — World Conference on Women’s Studies (2021)\n\nToward an Improvement of Engineering Teaming Skills Through an In-House Professionalism Course — IEEE Transactions on Education (2020)\n\nFormation of Policies Guided by Multivariable Control Theory — Operations Research Perspectives (2020)\n\nThe Impact of Collaborative Research: A Case Study in a Developing Country — IEEE SMIT Conference (2020)\n\nOn the Impact of Multi-Authorship Scholarly Publications — IEEE ATMR Conference (2020)"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html",
    "href": "weekly-notes/week-02-notes.html",
    "title": "Week 2 Notes - Algorithmic Decision Making & Census Data",
    "section": "",
    "text": "Algorithms are a set of rules or instructions for solving a problem and completing a task.\nAlgorithms in a government are typically systems used to assist or replace human decision-makers:\n\nBased on predictions from models trained on historical data.\n\nUsing inputs (features or predictors) and outputs (outcome, dependent variable).\n\n\nLong history of government data collection:\n\nCensus data\n\nCivil registration system\n\nAdministrative records\n\n\nBudgetary constraints force governments to use algos:\n\nEfficiency: faster case processing\n\nConsistency: same rules applied to everyone\n\nObjectivity: removes human bias\n\nCost savings\n\n\nIssues with Algorithms:\n\nData cleaning decisions (might not be good)\n\nData coding or classification (misclassification, race for example)\n\nData collection (proxies for items)\n\nHow results are interpreted\n\nWhat variables you put in the model\n\nEX: Healthcare algo bias whereby black patients are discriminated against due to proxies for healthcare needs.\n\n\nCensus data is used for:\n\nUnderstanding community demos\n\nAllocate government resources\n\nTracking neighborhood change\n\nDesigning fair algos\n\nPut into the constitution by Madison\n\n\nCensuses are decennial and contain 9 basic demographic questions.\n\nACS is 3% of households, done annually, and has more detailed questions on income, education, employment, and housing costs.\n\nACS is aggregated to 5-year estimates (to have more reliable data):\n\nCounty-level ACS data is for state and regional planning\n\nCensus tract level\n\nBlock group level: local analysis, but has large margins of errors (MOEs)\n\n\nTo protect privacy the census applies mathematical noise to individual data while preserving overall patterns:\n\nMOEs might skew results slightly or cause biases\n\n\nTIGER is Topologically Integrated Geographic Encoding and Referencing system."
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-02-notes.html#key-concepts-learned",
    "title": "Week 2 Notes - Algorithmic Decision Making & Census Data",
    "section": "",
    "text": "Algorithms are a set of rules or instructions for solving a problem and completing a task.\nAlgorithms in a government are typically systems used to assist or replace human decision-makers:\n\nBased on predictions from models trained on historical data.\n\nUsing inputs (features or predictors) and outputs (outcome, dependent variable).\n\n\nLong history of government data collection:\n\nCensus data\n\nCivil registration system\n\nAdministrative records\n\n\nBudgetary constraints force governments to use algos:\n\nEfficiency: faster case processing\n\nConsistency: same rules applied to everyone\n\nObjectivity: removes human bias\n\nCost savings\n\n\nIssues with Algorithms:\n\nData cleaning decisions (might not be good)\n\nData coding or classification (misclassification, race for example)\n\nData collection (proxies for items)\n\nHow results are interpreted\n\nWhat variables you put in the model\n\nEX: Healthcare algo bias whereby black patients are discriminated against due to proxies for healthcare needs.\n\n\nCensus data is used for:\n\nUnderstanding community demos\n\nAllocate government resources\n\nTracking neighborhood change\n\nDesigning fair algos\n\nPut into the constitution by Madison\n\n\nCensuses are decennial and contain 9 basic demographic questions.\n\nACS is 3% of households, done annually, and has more detailed questions on income, education, employment, and housing costs.\n\nACS is aggregated to 5-year estimates (to have more reliable data):\n\nCounty-level ACS data is for state and regional planning\n\nCensus tract level\n\nBlock group level: local analysis, but has large margins of errors (MOEs)\n\n\nTo protect privacy the census applies mathematical noise to individual data while preserving overall patterns:\n\nMOEs might skew results slightly or cause biases\n\n\nTIGER is Topologically Integrated Geographic Encoding and Referencing system."
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#coding-techniques",
    "href": "weekly-notes/week-02-notes.html#coding-techniques",
    "title": "Week 2 Notes - Algorithmic Decision Making & Census Data",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nGEOID: geographic identifier\n\nNAM: Human-readable location\n\nTypical output is long, but you can force output = \"wide\"\n\nstr_remove(), str_extract(), str_replace()\n\nkable() for professional formatting"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#questions-challenges",
    "href": "weekly-notes/week-02-notes.html#questions-challenges",
    "title": "Week 2 Notes - Algorithmic Decision Making & Census Data",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\n\n\nMake my interpretations understandable for a policy audience."
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#connections-to-policy",
    "href": "weekly-notes/week-02-notes.html#connections-to-policy",
    "title": "Week 2 Notes - Algorithmic Decision Making & Census Data",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\n\nAlgorithmic decision making → understanding why your analysis matters for real policy decisions\n\nData subjectivity → why we emphasize transparent, reproducible methods in this class\n\nCensus data → the foundation for most urban planning and policy analysis\n\nR skills → the tools to do this work professionally and ethically"
  },
  {
    "objectID": "weekly-notes/week-02-notes.html#reflection",
    "href": "weekly-notes/week-02-notes.html#reflection",
    "title": "Week 2 Notes - Algorithmic Decision Making & Census Data",
    "section": "Reflection",
    "text": "Reflection\n\nThe data is clean and samples are sufficiently random and large to be representative.\n\nUndocumented workers are likely to be excluded and those who are nomadic or without a permanent address."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html",
    "href": "weekly-notes/week-04-notes.html",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "Setting up repositories on"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#key-concepts-learned",
    "href": "weekly-notes/week-04-notes.html#key-concepts-learned",
    "title": "Week 1 Notes - Course Introduction",
    "section": "",
    "text": "Setting up repositories on"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#coding-techniques",
    "href": "weekly-notes/week-04-notes.html#coding-techniques",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Coding Techniques",
    "text": "Coding Techniques\n\nBasics of tidyverse and its accompanying commands of filter(), select(), mutate(), and summarize()\nQuarto functions on how to bold, italics, both bold and italics, code list, and strikethrough"
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#questions-challenges",
    "href": "weekly-notes/week-04-notes.html#questions-challenges",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Questions & Challenges",
    "text": "Questions & Challenges\n\n\n\nEverything was clear. I would still like to mess around more with Quarto."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#connections-to-policy",
    "href": "weekly-notes/week-04-notes.html#connections-to-policy",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Connections to Policy",
    "text": "Connections to Policy\n\n\nThis was a building block week, so not much of direct application rather tracking and documentation baseline for setup."
  },
  {
    "objectID": "weekly-notes/week-04-notes.html#reflection",
    "href": "weekly-notes/week-04-notes.html#reflection",
    "title": "Week 1 Notes - Course Introduction",
    "section": "Reflection",
    "text": "Reflection\n\nLearning how to create a custom repository was both enjoyable and insightful.\nIt could also serve as a way to share supplementary analyses and to present code in a more public, graphical, and accessible format for non-coding audiences."
  }
]